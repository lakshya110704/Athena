method accepts a few options. Instead of applying these options on a
per-document basis, we may declare the options at the schema level and have
them applied to all of the schema's documents by default.
To have all virtuals show up in your console.log output, set the
toObject option to { getters: true }:
To see all available toObject options, read this.
By default, if you have an object with key 'type' in your schema, mongoose
will interpret it as a type declaration.
However, for applications like geoJSON,
the 'type' property is important. If you want to control which key mongoose
uses to find type declarations, set the 'typeKey' schema option.
By default, documents are automatically validated before they are saved to
the database. This is to prevent saving an invalid document. If you want to
handle validation manually, and be able to save objects which don't pass
validation, you can set validateBeforeSave to false.
The versionKey is a property set on each document when first created by
Mongoose. This keys value contains the internal
revision
of the document. The versionKey option is a string that represents the
path to use for versioning. The default is __v. If this conflicts with
your application you can configure as such:
Note that Mongoose's default versioning is not a full optimistic concurrency
solution. Mongoose's default versioning only operates on arrays as shown below.
If you need optimistic concurrency support for save(), you can set the optimisticConcurrency option
Document versioning can also be disabled by setting the versionKey to
false.
DO NOT disable versioning unless you know what you are doing.
Mongoose only updates the version key when you use save().
If you use update(), findOneAndUpdate(), etc. Mongoose will not
update the version key. As a workaround, you can use the below middleware.
Optimistic concurrency is a strategy to ensure
the document you're updating didn't change between when you loaded it using find() or findOne(), and when
you update it using save().
For example, suppose you have a House model that contains a list of photos, and a status that represents
whether this house shows up in searches. Suppose that a house that has status 'APPROVED' must have at least
two photos. You might implement the logic of approving a house document as shown below:
The markApproved() function looks right in isolation, but there might be a potential issue: what if another
function removes the house's photos between the findOne() call and the save() call? For example, the below
code will succeed:
If you set the optimisticConcurrency option on the House model's schema, the above script will throw an
error.
Sets a default collation
for every query and aggregation. Here's a beginner-friendly overview of collations.
If you set the timeseries option on a schema, Mongoose will create a timeseries collection for any model that you create from that schema.
skipVersioning allows excluding paths from versioning (i.e., the internal
revision will not be incremented even if these paths are updated). DO NOT
do this unless you know what you're doing. For subdocuments, include this
on the parent document using the fully qualified path.
The timestamps option tells Mongoose to assign createdAt and updatedAt fields
to your schema. The type assigned is Date.
By default, the names of the fields are createdAt and updatedAt. Customize
the field names by setting timestamps.createdAt and timestamps.updatedAt.
The way timestamps works under the hood is:
By default, Mongoose uses new Date() to get the current time.
If you want to overwrite the function
Mongoose uses to get the current time, you can set the
timestamps.currentTime option. Mongoose will call the
timestamps.currentTime function whenever it needs to get
the current time.
Mongoose supports defining global plugins, plugins that apply to all schemas.
Sometimes, you may only want to apply a given plugin to some schemas.
In that case, you can add pluginTags to a schema:
If you call plugin() with a tags option, Mongoose will only apply that plugin to schemas that have a matching entry in pluginTags.
By default, Mongoose will automatically select() any populated paths for
you, unless you explicitly exclude them.
To opt out of selecting populated fields by default, set selectPopulatedPaths
to false in your schema.
For legacy reasons, when there is a validation error in subpath of a
single nested schema, Mongoose will record that there was a validation error
in the single nested schema path as well. For example:
Set the storeSubdocValidationError to false on the child schema to make
Mongoose only reports the parent error.
Options like collation and capped affect the options Mongoose passes to MongoDB when creating a new collection.
Mongoose schemas support most MongoDB createCollection() options, but not all.
You can use the collectionOptions option to set any createCollection() options; Mongoose will use collectionOptions as the default values when calling createCollection() for your schema.
Similar to autoIndex, except for automatically creates any Atlas search indexes defined in your schema.
Unlike autoIndex, this option defaults to false.
Schemas have a loadClass() method
that you can use to create a Mongoose schema from an ES6 class:
Here's an example of using loadClass() to create a schema from an ES6 class:
Schemas are also pluggable which allows us to package up reusable features into
plugins that can be shared with the community or just between your projects.
Here's an alternative introduction to Mongoose schemas.
To get the most out of MongoDB, you need to learn the basics of MongoDB schema design.
SQL schema design (third normal form) was designed to minimize storage costs,
whereas MongoDB schema design is about making common queries as fast as possible.
The 6 Rules of Thumb for MongoDB Schema Design blog series
is an excellent resource for learning the basic rules for making your queries
fast.
Users looking to master MongoDB schema design in Node.js should look into
The Little MongoDB Schema Design Book
by Christian Kvalheim, the original author of the MongoDB Node.js driver.
This book shows you how to implement performant schemas for a laundry list
of use cases, including e-commerce, wikis, and appointment bookings.
Now that we've covered Schemas, let's take a look at SchemaTypes.
If you haven't yet done so, please take a minute to read the quickstart to get an idea of how Mongoose works.
If you are migrating from 7.x to 8.x please take a moment to read the migration guide.
Everything in Mongoose starts with a Schema. Each schema maps to a MongoDB
collection and defines the shape of the documents within that collection.
If you want to add additional keys later, use the
Schema#add method.
Each key in our code blogSchema defines a property in our documents which
will be cast to its associated SchemaType.
For example, we've defined a property title which will be cast to the
String SchemaType and property date
which will be cast to a Date SchemaType.
Notice above that if a property only requires a type, it can be specified using
a shorthand notation (contrast the title property above with the date
property).
Keys may also be assigned nested objects containing further key/type definitions
like the meta property above.  This will happen whenever a key's value is a POJO
that doesn't have a type property.
In these cases, Mongoose only creates actual schema paths for leaves
in the tree. (like meta.votes and meta.favs above),
and the branches do not have actual paths.  A side-effect of this is that meta
above cannot have its own validation.  If validation is needed up the tree, a path
needs to be created up the tree - see the Subdocuments section
for more information on how to do this.  Also read the Mixed
subsection of the SchemaTypes guide for some gotchas.
The permitted SchemaTypes are:
Read more about SchemaTypes here.
Schemas not only define the structure of your document and casting of
properties, they also define document instance methods,
static Model methods, compound indexes,
and document lifecycle hooks called middleware.
To use our schema definition, we need to convert our blogSchema into a
Model we can work with.
To do so, we pass it into mongoose.model(modelName, schema):
By default, Mongoose adds an _id property to your schemas.
When you create a new document with the automatically added _id property, Mongoose creates a new _id of type ObjectId to your document.
You can also overwrite Mongoose's default _id with your own _id.
Just be careful: Mongoose will refuse to save a top-level document that doesn't have an _id, so you're responsible for setting _id if you define your own _id path.
Mongoose also adds an _id property to subdocuments.
You can disable the _id property on your subdocuments as follows.
Mongoose does allow saving subdocuments without an _id property.
Alternatively, you can disable _id using the following syntax:
Instances of Models are documents. Documents have
many of their own built-in instance methods.
We may also define our own custom document instance methods.
Now all of our animal instances have a findSimilarTypes method available
to them.
You can also add static functions to your model. There are three equivalent
ways to add a static:
Do not declare statics using ES6 arrow functions (=>). Arrow functions explicitly prevent binding this, so the above examples will not work because of the value of this.
You can also add query helper functions, which are like instance methods
but for mongoose queries. Query helper methods let you extend mongoose's
chainable query builder API.
MongoDB supports secondary indexes.
With mongoose, we define these indexes within our Schema at the path level or the schema level.
Defining indexes at the schema level is necessary when creating
compound indexes.
See SchemaType#index() for other index options.
When your application starts up, Mongoose automatically calls createIndex for each defined index in your schema.
Mongoose will call createIndex for each index sequentially, and emit an 'index' event on the model when all the createIndex calls succeeded or when there was an error.
While nice for development, it is recommended this behavior be disabled in production since index creation can cause a significant performance impact.
Disable the behavior by setting the autoIndex option of your schema to false, or globally on the connection by setting the option autoIndex to false.
Mongoose will emit an index event on the model when indexes are done
building or an error occurred.
See also the Model#ensureIndexes method.
Virtuals are document properties that
you can get and set but that do not get persisted to MongoDB. The getters
are useful for formatting or combining fields, while setters are useful for
de-composing a single value into multiple values for storage.
Suppose you want to print out the person's full name. You could do it yourself:
But concatenating the first and
last name every time can get cumbersome.
And what if you want to do some extra processing on the name, like
removing diacritics? A
virtual property getter lets you
define a fullName property that won't get persisted to MongoDB.
Now, mongoose will call your getter function every time you access the
fullName property:
If you use toJSON() or toObject() Mongoose will not include virtuals by default.
Pass { virtuals: true } to toJSON() or toObject() to include virtuals.
The above caveat for toJSON() also includes the output of calling JSON.stringify() on a Mongoose document, because JSON.stringify() calls toJSON().
To include virtuals in JSON.stringify() output, you can either call toObject({ virtuals: true }) on the document before calling JSON.stringify(), or set the toJSON: { virtuals: true } option on your schema.
You can also add a custom setter to your virtual that will let you set both
first name and last name via the fullName virtual.
Virtual property setters are applied before other validation. So the example
above would still work even if the first and last name fields were
required.
Only non-virtual properties work as part of queries and for field selection.
Since virtuals are not stored in MongoDB, you can't query with them.
You can learn more about virtuals here.
Aliases are a particular type of virtual where the getter and setter
seamlessly get and set another property. This is handy for saving network
bandwidth, so you can convert a short property name stored in the database
into a longer name for code readability.
You can also declare aliases on nested paths. It is easier to use nested
schemas and subdocuments, but you can also declare
nested path aliases inline as long as you use the full nested path
nested.myProp as the alias.
Schemas have a few configurable options which can be passed to the
constructor or to the set method:
Valid options:
By default, Mongoose's init() function
creates all the indexes defined in your model's schema by calling
Model.createIndexes()
after you successfully connect to MongoDB. Creating indexes automatically is
great for development and test environments. But index builds can also create
significant load on your production database. If you want to manage indexes
carefully in production, you can set autoIndex to false.
The autoIndex option is set to true by default. You can change this
default by setting mongoose.set('autoIndex', false);
Before Mongoose builds indexes, it calls Model.createCollection() to create the underlying collection in MongoDB by default.
Calling createCollection() sets the collection's default collation based on the collation option and establishes the collection as
a capped collection if you set the capped schema option.
You can disable this behavior by setting autoCreate to false using mongoose.set('autoCreate', false).
Like autoIndex, autoCreate is helpful for development and test environments, but you may want to disable it for production to avoid unnecessary database calls.
Unfortunately, createCollection() cannot change an existing collection.
For example, if you add capped: { size: 1024 } to your schema and the existing collection is not capped, createCollection() will not overwrite the existing collection.
That is because the MongoDB server does not allow changing a collection's options without dropping the collection first.
By default, mongoose buffers commands when the connection goes down until
the driver manages to reconnect. To disable buffering, set bufferCommands
to false.
The schema bufferCommands option overrides the global bufferCommands option.
If bufferCommands is on, this option sets the maximum amount of time Mongoose buffering will wait before
throwing an error. If not specified, Mongoose will use 10000 (10 seconds).
Mongoose supports MongoDBs capped
collections. To specify the underlying MongoDB collection be capped, set
the capped option to the maximum size of the collection in
bytes.
The capped option may also be set to an object if you want to pass
additional options like max.
In this case you must explicitly pass the size option, which is required.
Mongoose by default produces a collection name by passing the model name to
the utils.toCollectionName method.
This method pluralizes the name. Set this option if you need a different name
for your collection.
When you define a discriminator, Mongoose adds a path to your
schema that stores which discriminator a document is an instance of. By default, Mongoose
adds an __t path, but you can set discriminatorKey to overwrite this default.
When excludeIndexes is true, Mongoose will not create indexes from the given subdocument schema.
This option only works when the schema is used in a subdocument path or document array path, Mongoose ignores this option if set on the top-level schema for a model.
Defaults to false.
Mongoose assigns each of your schemas an id virtual getter by default
which returns the document's _id field cast to a string, or in the case of
ObjectIds, its hexString. If you don't want an id getter added to your
schema, you may disable it by passing this option at schema construction time.
Mongoose assigns each of your schemas an _id field by default if one
is not passed into the Schema constructor.
The type assigned is an ObjectId
to coincide with MongoDB's default behavior. If you don't want an _id
added to your schema at all, you may disable it using this option.
You can only use this option on subdocuments. Mongoose can't
save a document without knowing its id, so you will get an error if
you try to save a document without an _id.
Mongoose will, by default, "minimize" schemas by removing empty objects.
This behavior can be overridden by setting minimize option to false. It
will then store empty objects.
To check whether an object is empty, you can use the $isEmpty() helper:
Allows setting query#read options at the
schema level, providing us a way to apply default
ReadPreferences
to all queries derived from a model.
The alias of each pref is also permitted so instead of having to type out
'secondaryPreferred' and getting the spelling wrong, we can simply pass 'sp'.
The read option also allows us to specify tag sets. These tell the
driver from which members
of the replica-set it should attempt to read. Read more about tag sets
here and
here.
NOTE: you may also specify the driver read preference strategy
option when connecting:
Allows setting write concern
at the schema level.
The shardKey option is used when we have a sharded MongoDB architecture.
Each sharded collection is given a shard key which must be present in all
insert/update operations. We just need to set this schema option to the same
shard key and we’ll be all set.
Note that Mongoose does not send the shardcollection command for you. You
must configure your shards yourself.
The strict option, (enabled by default), ensures that values passed to our
model constructor that were not specified in our schema do not get saved to
the db.
This also affects the use of doc.set() to set a property value.
This value can be overridden at the model instance level by passing a second
boolean argument:
The strict option may also be set to "throw" which will cause errors
to be produced instead of dropping the bad data.
NOTE: Any key/val set on the instance that does not exist in your schema
is always ignored, regardless of schema option.
Mongoose supports a separate strictQuery option to avoid strict mode for query filters.
This is because empty query filters cause Mongoose to return all documents in the model, which can cause issues.
The strict option does apply to updates.
The strictQuery option is just for query filters.
Mongoose has a separate strictQuery option to toggle strict mode for the filter parameter to queries.
In general, we do not recommend passing user-defined objects as query filters:
In Mongoose 7, strictQuery is false by default.
However, you can override this behavior globally:
Exactly the same as the toObject option but only applies when
the document's toJSON method is called.
To see all available toJSON/toObject options, read this.
Documents have a toObject method
which converts the mongoose document into a plain JavaScript object. This
method accepts a few options. Instead of applying these options on a
per-document basis, we may declare the options at the schema level and have
them applied to all of the schema's documents by default.
To have all virtuals show up in your console.log output, set the
toObject option to { getters: true }:
To see all available toObject options, read this.
By default, if you have an object with key 'type' in your schema, mongoose
will interpret it as a type declaration.
However, for applications like geoJSON,
the 'type' property is important. If you want to control which key mongoose
uses to find type declarations, set the 'typeKey' schema option.
By default, documents are automatically validated before they are saved to
the database. This is to prevent saving an invalid document. If you want to
handle validation manually, and be able to save objects which don't pass
validation, you can set validateBeforeSave to false.
The versionKey is a property set on each document when first created by
Mongoose. This keys value contains the internal
revision
of the document. The versionKey option is a string that represents the
path to use for versioning. The default is __v. If this conflicts with
your application you can configure as such:
Note that Mongoose's default versioning is not a full optimistic concurrency
solution. Mongoose's default versioning only operates on arrays as shown below.
If you need optimistic concurrency support for save(), you can set the optimisticConcurrency option
Document versioning can also be disabled by setting the versionKey to
false.
DO NOT disable versioning unless you know what you are doing.
Mongoose only updates the version key when you use save().
If you use update(), findOneAndUpdate(), etc. Mongoose will not
update the version key. As a workaround, you can use the below middleware.
Optimistic concurrency is a strategy to ensure
the document you're updating didn't change between when you loaded it using find() or findOne(), and when
you update it using save().
For example, suppose you have a House model that contains a list of photos, and a status that represents
whether this house shows up in searches. Suppose that a house that has status 'APPROVED' must have at least
two photos. You might implement the logic of approving a house document as shown below:
The markApproved() function looks right in isolation, but there might be a potential issue: what if another
function removes the house's photos between the findOne() call and the save() call? For example, the below
code will succeed:
If you set the optimisticConcurrency option on the House model's schema, the above script will throw an
error.
Sets a default collation
for every query and aggregation. Here's a beginner-friendly overview of collations.
If you set the timeseries option on a schema, Mongoose will create a timeseries collection for any model that you create from that schema.
skipVersioning allows excluding paths from versioning (i.e., the internal
revision will not be incremented even if these paths are updated). DO NOT
do this unless you know what you're doing. For subdocuments, include this
on the parent document using the fully qualified path.
The timestamps option tells Mongoose to assign createdAt and updatedAt fields
to your schema. The type assigned is Date.
By default, the names of the fields are createdAt and updatedAt. Customize
the field names by setting timestamps.createdAt and timestamps.updatedAt.
The way timestamps works under the hood is:
By default, Mongoose uses new Date() to get the current time.
If you want to overwrite the function
Mongoose uses to get the current time, you can set the
timestamps.currentTime option. Mongoose will call the
timestamps.currentTime function whenever it needs to get
the current time.
Mongoose supports defining global plugins, plugins that apply to all schemas.
Sometimes, you may only want to apply a given plugin to some schemas.
In that case, you can add pluginTags to a schema:
If you call plugin() with a tags option, Mongoose will only apply that plugin to schemas that have a matching entry in pluginTags.
By default, Mongoose will automatically select() any populated paths for
you, unless you explicitly exclude them.
To opt out of selecting populated fields by default, set selectPopulatedPaths
to false in your schema.
For legacy reasons, when there is a validation error in subpath of a
single nested schema, Mongoose will record that there was a validation error
in the single nested schema path as well. For example:
Set the storeSubdocValidationError to false on the child schema to make
Mongoose only reports the parent error.
Options like collation and capped affect the options Mongoose passes to MongoDB when creating a new collection.
Mongoose schemas support most MongoDB createCollection() options, but not all.
You can use the collectionOptions option to set any createCollection() options; Mongoose will use collectionOptions as the default values when calling createCollection() for your schema.
Similar to autoIndex, except for automatically creates any Atlas search indexes defined in your schema.
Unlike autoIndex, this option defaults to false.
Schemas have a loadClass() method
that you can use to create a Mongoose schema from an ES6 class:
Here's an example of using loadClass() to create a schema from an ES6 class:
Schemas are also pluggable which allows us to package up reusable features into
plugins that can be shared with the community or just between your projects.
Here's an alternative introduction to Mongoose schemas.
To get the most out of MongoDB, you need to learn the basics of MongoDB schema design.
SQL schema design (third normal form) was designed to minimize storage costs,
whereas MongoDB schema design is about making common queries as fast as possible.
The 6 Rules of Thumb for MongoDB Schema Design blog series
is an excellent resource for learning the basic rules for making your queries
fast.
Users looking to master MongoDB schema design in Node.js should look into
The Little MongoDB Schema Design Book
by Christian Kvalheim, the original author of the MongoDB Node.js driver.
This book shows you how to implement performant schemas for a laundry list
of use cases, including e-commerce, wikis, and appointment bookings.
Now that we've covered Schemas, let's take a look at SchemaTypes.
If you haven't yet done so, please take a minute to read the quickstart to get an idea of how Mongoose works.
If you are migrating from 7.x to 8.x please take a moment to read the migration guide.
Everything in Mongoose starts with a Schema. Each schema maps to a MongoDB
collection and defines the shape of the documents within that collection.
If you want to add additional keys later, use the
Schema#add method.
Each key in our code blogSchema defines a property in our documents which
will be cast to its associated SchemaType.
For example, we've defined a property title which will be cast to the
String SchemaType and property date
which will be cast to a Date SchemaType.
Notice above that if a property only requires a type, it can be specified using
a shorthand notation (contrast the title property above with the date
property).
Keys may also be assigned nested objects containing further key/type definitions
like the meta property above.  This will happen whenever a key's value is a POJO
that doesn't have a type property.
In these cases, Mongoose only creates actual schema paths for leaves
in the tree. (like meta.votes and meta.favs above),
and the branches do not have actual paths.  A side-effect of this is that meta
above cannot have its own validation.  If validation is needed up the tree, a path
needs to be created up the tree - see the Subdocuments section
for more information on how to do this.  Also read the Mixed
subsection of the SchemaTypes guide for some gotchas.
The permitted SchemaTypes are:
Read more about SchemaTypes here.
Schemas not only define the structure of your document and casting of
properties, they also define document instance methods,
static Model methods, compound indexes,
and document lifecycle hooks called middleware.
To use our schema definition, we need to convert our blogSchema into a
Model we can work with.
To do so, we pass it into mongoose.model(modelName, schema):
By default, Mongoose adds an _id property to your schemas.
When you create a new document with the automatically added _id property, Mongoose creates a new _id of type ObjectId to your document.
You can also overwrite Mongoose's default _id with your own _id.
Just be careful: Mongoose will refuse to save a top-level document that doesn't have an _id, so you're responsible for setting _id if you define your own _id path.
Mongoose also adds an _id property to subdocuments.
You can disable the _id property on your subdocuments as follows.
Mongoose does allow saving subdocuments without an _id property.
Alternatively, you can disable _id using the following syntax:
Instances of Models are documents. Documents have
many of their own built-in instance methods.
We may also define our own custom document instance methods.
Now all of our animal instances have a findSimilarTypes method available
to them.
You can also add static functions to your model. There are three equivalent
ways to add a static:
Do not declare statics using ES6 arrow functions (=>). Arrow functions explicitly prevent binding this, so the above examples will not work because of the value of this.
You can also add query helper functions, which are like instance methods
but for mongoose queries. Query helper methods let you extend mongoose's
chainable query builder API.
MongoDB supports secondary indexes.
With mongoose, we define these indexes within our Schema at the path level or the schema level.
Defining indexes at the schema level is necessary when creating
compound indexes.
See SchemaType#index() for other index options.
When your application starts up, Mongoose automatically calls createIndex for each defined index in your schema.
Mongoose will call createIndex for each index sequentially, and emit an 'index' event on the model when all the createIndex calls succeeded or when there was an error.
While nice for development, it is recommended this behavior be disabled in production since index creation can cause a significant performance impact.
Disable the behavior by setting the autoIndex option of your schema to false, or globally on the connection by setting the option autoIndex to false.
Mongoose will emit an index event on the model when indexes are done
building or an error occurred.
See also the Model#ensureIndexes method.
Virtuals are document properties that
you can get and set but that do not get persisted to MongoDB. The getters
are useful for formatting or combining fields, while setters are useful for
de-composing a single value into multiple values for storage.
Suppose you want to print out the person's full name. You could do it yourself:
But concatenating the first and
last name every time can get cumbersome.
And what if you want to do some extra processing on the name, like
removing diacritics? A
virtual property getter lets you
define a fullName property that won't get persisted to MongoDB.
Now, mongoose will call your getter function every time you access the
fullName property:
If you use toJSON() or toObject() Mongoose will not include virtuals by default.
Pass { virtuals: true } to toJSON() or toObject() to include virtuals.
The above caveat for toJSON() also includes the output of calling JSON.stringify() on a Mongoose document, because JSON.stringify() calls toJSON().
To include virtuals in JSON.stringify() output, you can either call toObject({ virtuals: true }) on the document before calling JSON.stringify(), or set the toJSON: { virtuals: true } option on your schema.
You can also add a custom setter to your virtual that will let you set both
first name and last name via the fullName virtual.
Virtual property setters are applied before other validation. So the example
above would still work even if the first and last name fields were
required.
Only non-virtual properties work as part of queries and for field selection.
Since virtuals are not stored in MongoDB, you can't query with them.
You can learn more about virtuals here.
Aliases are a particular type of virtual where the getter and setter
seamlessly get and set another property. This is handy for saving network
bandwidth, so you can convert a short property name stored in the database
into a longer name for code readability.
You can also declare aliases on nested paths. It is easier to use nested
schemas and subdocuments, but you can also declare
nested path aliases inline as long as you use the full nested path
nested.myProp as the alias.
Schemas have a few configurable options which can be passed to the
constructor or to the set method:
Valid options:
By default, Mongoose's init() function
creates all the indexes defined in your model's schema by calling
Model.createIndexes()
after you successfully connect to MongoDB. Creating indexes automatically is
great for development and test environments. But index builds can also create
significant load on your production database. If you want to manage indexes
carefully in production, you can set autoIndex to false.
The autoIndex option is set to true by default. You can change this
default by setting mongoose.set('autoIndex', false);
Before Mongoose builds indexes, it calls Model.createCollection() to create the underlying collection in MongoDB by default.
Calling createCollection() sets the collection's default collation based on the collation option and establishes the collection as
a capped collection if you set the capped schema option.
You can disable this behavior by setting autoCreate to false using mongoose.set('autoCreate', false).
Like autoIndex, autoCreate is helpful for development and test environments, but you may want to disable it for production to avoid unnecessary database calls.
Unfortunately, createCollection() cannot change an existing collection.
For example, if you add capped: { size: 1024 } to your schema and the existing collection is not capped, createCollection() will not overwrite the existing collection.
That is because the MongoDB server does not allow changing a collection's options without dropping the collection first.
By default, mongoose buffers commands when the connection goes down until
the driver manages to reconnect. To disable buffering, set bufferCommands
to false.
The schema bufferCommands option overrides the global bufferCommands option.
If bufferCommands is on, this option sets the maximum amount of time Mongoose buffering will wait before
throwing an error. If not specified, Mongoose will use 10000 (10 seconds).
Mongoose supports MongoDBs capped
collections. To specify the underlying MongoDB collection be capped, set
the capped option to the maximum size of the collection in
bytes.
The capped option may also be set to an object if you want to pass
additional options like max.
In this case you must explicitly pass the size option, which is required.
Mongoose by default produces a collection name by passing the model name to
the utils.toCollectionName method.
This method pluralizes the name. Set this option if you need a different name
for your collection.
When you define a discriminator, Mongoose adds a path to your
schema that stores which discriminator a document is an instance of. By default, Mongoose
adds an __t path, but you can set discriminatorKey to overwrite this default.
When excludeIndexes is true, Mongoose will not create indexes from the given subdocument schema.
This option only works when the schema is used in a subdocument path or document array path, Mongoose ignores this option if set on the top-level schema for a model.
Defaults to false.
Mongoose assigns each of your schemas an id virtual getter by default
which returns the document's _id field cast to a string, or in the case of
ObjectIds, its hexString. If you don't want an id getter added to your
schema, you may disable it by passing this option at schema construction time.
Mongoose assigns each of your schemas an _id field by default if one
is not passed into the Schema constructor.
The type assigned is an ObjectId
to coincide with MongoDB's default behavior. If you don't want an _id
added to your schema at all, you may disable it using this option.
You can only use this option on subdocuments. Mongoose can't
save a document without knowing its id, so you will get an error if
you try to save a document without an _id.
Mongoose will, by default, "minimize" schemas by removing empty objects.
This behavior can be overridden by setting minimize option to false. It
will then store empty objects.
To check whether an object is empty, you can use the $isEmpty() helper:
Allows setting query#read options at the
schema level, providing us a way to apply default
ReadPreferences
to all queries derived from a model.
The alias of each pref is also permitted so instead of having to type out
'secondaryPreferred' and getting the spelling wrong, we can simply pass 'sp'.
The read option also allows us to specify tag sets. These tell the
driver from which members
of the replica-set it should attempt to read. Read more about tag sets
here and
here.
NOTE: you may also specify the driver read preference strategy
option when connecting:
Allows setting write concern
at the schema level.
The shardKey option is used when we have a sharded MongoDB architecture.
Each sharded collection is given a shard key which must be present in all
insert/update operations. We just need to set this schema option to the same
shard key and we’ll be all set.
Note that Mongoose does not send the shardcollection command for you. You
must configure your shards yourself.
The strict option, (enabled by default), ensures that values passed to our
model constructor that were not specified in our schema do not get saved to
the db.
This also affects the use of doc.set() to set a property value.
This value can be overridden at the model instance level by passing a second
boolean argument:
The strict option may also be set to "throw" which will cause errors
to be produced instead of dropping the bad data.
NOTE: Any key/val set on the instance that does not exist in your schema
is always ignored, regardless of schema option.
Mongoose supports a separate strictQuery option to avoid strict mode for query filters.
This is because empty query filters cause Mongoose to return all documents in the model, which can cause issues.
The strict option does apply to updates.
The strictQuery option is just for query filters.
Mongoose has a separate strictQuery option to toggle strict mode for the filter parameter to queries.
In general, we do not recommend passing user-defined objects as query filters:
In Mongoose 7, strictQuery is false by default.
However, you can override this behavior globally:
Exactly the same as the toObject option but only applies when
the document's toJSON method is called.
To see all available toJSON/toObject options, read this.
Documents have a toObject method
which converts the mongoose document into a plain JavaScript object. This
method accepts a few options. Instead of applying these options on a
per-document basis, we may declare the options at the schema level and have
them applied to all of the schema's documents by default.
To have all virtuals show up in your console.log output, set the
toObject option to { getters: true }:
To see all available toObject options, read this.
By default, if you have an object with key 'type' in your schema, mongoose
will interpret it as a type declaration.
However, for applications like geoJSON,
the 'type' property is important. If you want to control which key mongoose
uses to find type declarations, set the 'typeKey' schema option.
By default, documents are automatically validated before they are saved to
the database. This is to prevent saving an invalid document. If you want to
handle validation manually, and be able to save objects which don't pass
validation, you can set validateBeforeSave to false.
The versionKey is a property set on each document when first created by
Mongoose. This keys value contains the internal
revision
of the document. The versionKey option is a string that represents the
path to use for versioning. The default is __v. If this conflicts with
your application you can configure as such:
Note that Mongoose's default versioning is not a full optimistic concurrency
solution. Mongoose's default versioning only operates on arrays as shown below.
If you need optimistic concurrency support for save(), you can set the optimisticConcurrency option
Document versioning can also be disabled by setting the versionKey to
false.
DO NOT disable versioning unless you know what you are doing.
Mongoose only updates the version key when you use save().
If you use update(), findOneAndUpdate(), etc. Mongoose will not
update the version key. As a workaround, you can use the below middleware.
Optimistic concurrency is a strategy to ensure
the document you're updating didn't change between when you loaded it using find() or findOne(), and when
you update it using save().
For example, suppose you have a House model that contains a list of photos, and a status that represents
whether this house shows up in searches. Suppose that a house that has status 'APPROVED' must have at least
two photos. You might implement the logic of approving a house document as shown below:
The markApproved() function looks right in isolation, but there might be a potential issue: what if another
function removes the house's photos between the findOne() call and the save() call? For example, the below
code will succeed:
If you set the optimisticConcurrency option on the House model's schema, the above script will throw an
error.
Sets a default collation
for every query and aggregation. Here's a beginner-friendly overview of collations.
If you set the timeseries option on a schema, Mongoose will create a timeseries collection for any model that you create from that schema.
skipVersioning allows excluding paths from versioning (i.e., the internal
revision will not be incremented even if these paths are updated). DO NOT
do this unless you know what you're doing. For subdocuments, include this
on the parent document using the fully qualified path.
The timestamps option tells Mongoose to assign createdAt and updatedAt fields
to your schema. The type assigned is Date.
By default, the names of the fields are createdAt and updatedAt. Customize
the field names by setting timestamps.createdAt and timestamps.updatedAt.
The way timestamps works under the hood is:
By default, Mongoose uses new Date() to get the current time.
If you want to overwrite the function
Mongoose uses to get the current time, you can set the
timestamps.currentTime option. Mongoose will call the
timestamps.currentTime function whenever it needs to get
the current time.
Mongoose supports defining global plugins, plugins that apply to all schemas.
Sometimes, you may only want to apply a given plugin to some schemas.
In that case, you can add pluginTags to a schema:
If you call plugin() with a tags option, Mongoose will only apply that plugin to schemas that have a matching entry in pluginTags.
By default, Mongoose will automatically select() any populated paths for
you, unless you explicitly exclude them.
To opt out of selecting populated fields by default, set selectPopulatedPaths
to false in your schema.
For legacy reasons, when there is a validation error in subpath of a
single nested schema, Mongoose will record that there was a validation error
in the single nested schema path as well. For example:
Set the storeSubdocValidationError to false on the child schema to make
Mongoose only reports the parent error.
Options like collation and capped affect the options Mongoose passes to MongoDB when creating a new collection.
Mongoose schemas support most MongoDB createCollection() options, but not all.
You can use the collectionOptions option to set any createCollection() options; Mongoose will use collectionOptions as the default values when calling createCollection() for your schema.
Similar to autoIndex, except for automatically creates any Atlas search indexes defined in your schema.
Unlike autoIndex, this option defaults to false.
Schemas have a loadClass() method
that you can use to create a Mongoose schema from an ES6 class:
Here's an example of using loadClass() to create a schema from an ES6 class:
Schemas are also pluggable which allows us to package up reusable features into
plugins that can be shared with the community or just between your projects.
Here's an alternative introduction to Mongoose schemas.
To get the most out of MongoDB, you need to learn the basics of MongoDB schema design.
SQL schema design (third normal form) was designed to minimize storage costs,
whereas MongoDB schema design is about making common queries as fast as possible.
The 6 Rules of Thumb for MongoDB Schema Design blog series
is an excellent resource for learning the basic rules for making your queries
fast.
Users looking to master MongoDB schema design in Node.js should look into
The Little MongoDB Schema Design Book
by Christian Kvalheim, the original author of the MongoDB Node.js driver.
This book shows you how to implement performant schemas for a laundry list
of use cases, including e-commerce, wikis, and appointment bookings.
Now that we've covered Schemas, let's take a look at SchemaTypes.
A Model is a class that's your primary tool for interacting with MongoDB.
An instance of a Model is called a Document.
In Mongoose, the term "Model" refers to subclasses of the mongoose.Model
class. You should not use the mongoose.Model class directly. The
mongoose.model() and
connection.model() functions
create subclasses of mongoose.Model as shown below.
Creates a Query and specifies a $where condition.
Sometimes you need to query for things in mongodb using a JavaScript expression. You can do so via find({ $where: javascript }), or you can use the mongoose shortcut method $where via a Query chain or from your mongoose Model.
Performs aggregations on the models collection.
If a callback is passed, the aggregate is executed and a Promise is returned. If a callback is not passed, the aggregate itself is returned.
This function triggers the following middleware.
Apply defaults to the given document or POJO.
takes an array of documents, gets the changes and inserts/updates documents in the database
 according to whether or not the document is new, or whether it has changes or not.
bulkSave uses bulkWrite under the hood, so it's mostly useful when dealing with many documents (10K+)
Sends multiple insertOne, updateOne, updateMany, replaceOne,
deleteOne, and/or deleteMany operations to the MongoDB server in one
command. This is faster than sending multiple independent operations (e.g.
if you use create()) because with bulkWrite() there is only one round
trip to MongoDB.
Mongoose will perform casting on all operations you provide.
This function does not trigger any middleware, neither save(), nor update().
If you need to trigger
save() middleware for every document use create() instead.
The supported operations are:
Cast the given POJO to the model's schema
Deletes all indexes that aren't defined in this model's schema. Used by
syncIndexes().
The returned promise resolves to a list of the dropped indexes' names as an array
Counts number of documents matching filter in a database collection.
If you want to count all documents in a large collection,
use the estimatedDocumentCount() function
instead. If you call countDocuments({}), MongoDB will always execute
a full collection scan and not use any indexes.
The countDocuments() function is similar to count(), but there are a
few operators that countDocuments() does not support.
Below are the operators that count() supports but countDocuments() does not,
and the suggested replacement:
Shortcut for saving one or more documents to the database.
MyModel.create(docs) does new MyModel(doc).save() for every doc in
docs.
This function triggers the following middleware.
Create the collection for this model. By default, if no indexes are specified,
mongoose will not create the collection for the model until any documents are
created. Use this method to create the collection explicitly.
Note 1: You may need to call this before starting a transaction
See https://www.mongodb.com/docs/manual/core/transactions/#transactions-and-operations
Note 2: You don't have to call this if your schema contains index or unique field.
In that case, just use Model.init()
Similar to ensureIndexes(), except for it uses the createIndex
function.
Create an Atlas search index.
This function only works when connected to MongoDB Atlas.
Connection instance the model uses.
Deletes all of the documents that match conditions from the collection.
It returns an object with the property deletedCount containing the number of documents deleted.
Behaves like remove(), but deletes all documents that match conditions
regardless of the single option.
This function triggers deleteMany query hooks. Read the
middleware docs to learn more.
Deletes the first document that matches conditions from the collection.
It returns an object with the property deletedCount indicating how many documents were deleted.
Behaves like remove(), but deletes at most one document regardless of the
single option.
This function triggers deleteOne query hooks. Read the
middleware docs to learn more.
Does a dry-run of Model.syncIndexes(), returning the indexes that syncIndexes() would drop and create if you were to run syncIndexes().
Adds a discriminator type.
Creates a Query for a distinct operation.
Delete an existing Atlas search index by name.
This function only works when connected to MongoDB Atlas.
Sends createIndex commands to mongo for each index declared in the schema.
The createIndex commands are sent in series.
After completion, an index event is emitted on this Model passing an error if one occurred.
NOTE: It is not recommended that you run this in production. Index creation may impact database performance depending on your load. Use with caution.
Estimates the number of documents in the MongoDB collection. Faster than
using countDocuments() for large collections because
estimatedDocumentCount() uses collection metadata rather than scanning
the entire collection.
Event emitter that reports any errors that occurred. Useful for global error
handling.
Returns a document with _id only if at least one document exists in the database that matches
the given filter, and null otherwise.
Under the hood, MyModel.exists({ answer: 42 }) is equivalent to
MyModel.findOne({ answer: 42 }).select({ _id: 1 }).lean()
This function triggers the following middleware.
Finds documents.
Mongoose casts the filter to match the model's schema before the command is sent.
See our query casting tutorial for
more information on how Mongoose casts filter.
Finds a single document by its _id field. findById(id) is almost*
equivalent to findOne({ _id: id }). If you want to query by a document's
_id, use findById() instead of findOne().
The id is cast based on the Schema before sending the command.
This function triggers the following middleware.
* Except for how it treats undefined. If you use findOne(), you'll see
that findOne(undefined) and findOne({ _id: undefined }) are equivalent
to findOne({}) and return arbitrary documents. However, mongoose
translates findById(undefined) into findOne({ _id: null }).
Issue a MongoDB findOneAndDelete() command by a document's _id field.
In other words, findByIdAndDelete(id) is a shorthand for
findOneAndDelete({ _id: id }).
This function triggers the following middleware.
Issues a mongodb findOneAndUpdate command by a document's _id field.
findByIdAndUpdate(id, ...) is equivalent to findOneAndUpdate({ _id: id }, ...).
Finds a matching document, updates it according to the update arg,
passing any options, and returns the found document (if any).
This function triggers the following middleware.
All top level update keys which are not atomic operation names are treated as set operations:
findOneAndX and findByIdAndX functions support limited validation. You can
enable validation by setting the runValidators option.
If you need full-fledged validation, use the traditional approach of first
retrieving the document.
Finds one document.
The conditions are cast to their respective SchemaTypes before the command is sent.
Note: conditions is optional, and if conditions is null or undefined,
mongoose will send an empty findOne command to MongoDB, which will return
an arbitrary document. If you're querying by _id, use findById() instead.
Issue a MongoDB findOneAndDelete() command.
Finds a matching document, removes it, and returns the found document (if any).
This function triggers the following middleware.
findOneAndX and findByIdAndX functions support limited validation. You can
enable validation by setting the runValidators option.
If you need full-fledged validation, use the traditional approach of first
retrieving the document.
Issue a MongoDB findOneAndReplace() command.
Finds a matching document, replaces it with the provided doc, and returns the document.
This function triggers the following query middleware.
Issues a mongodb findOneAndUpdate command.
Finds a matching document, updates it according to the update arg, passing any options, and returns the found document (if any) to the callback. The query executes if callback is passed else a Query object is returned.
All top level update keys which are not atomic operation names are treated as set operations:
findOneAndX and findByIdAndX functions support limited validation that
you can enable by setting the runValidators option.
If you need full-fledged validation, use the traditional approach of first
retrieving the document.
Shortcut for creating a new Document from existing raw data, pre-saved in the DB.
The document returned has no paths marked as modified initially.
This function is responsible for initializing the underlying connection in MongoDB based on schema options.
This function performs the following operations:
Mongoose calls this function automatically when a model is a created using
mongoose.model() or
connection.model(), so you
don't need to call init() to trigger index builds.
However, you may need to call init()  to get back a promise that will resolve when your indexes are finished.
Calling await Model.init() is helpful if you need to wait for indexes to build before continuing.
For example, if you want to wait for unique indexes to build before continuing with a test case.
Shortcut for validating an array of documents and inserting them into
MongoDB if they're all valid. This function is faster than .create()
because it only sends one operation to the server, rather than one for each
document.
Mongoose always validates each document before sending insertMany
to MongoDB. So if one document has a validation error, no documents will
be saved, unless you set
the ordered option to false.
This function does not trigger save middleware.
This function triggers the following middleware.
Helper for console.log. Given a model named 'MyModel', returns the string
'Model { MyModel }'.
Lists the indexes currently defined in MongoDB. This may or may not be
the same as the indexes defined in your schema depending on whether you
use the autoIndex option and if you
build indexes manually.
Populates document references.
Changed in Mongoose 6: the model you call populate() on should be the
"local field" model, not the "foreign field" model.
Returns the model instance used to create this document if no name specified.
If name specified, returns the model with the given name.
Additional properties to attach to the query when calling save() and
isNew is false.
Base Mongoose instance the model uses.
If this is a discriminator model, baseModelName is the name of
the base model.
The collection instance this model uses.
A Mongoose collection is a thin wrapper around a [MongoDB Node.js driver collection](MongoDB Node.js driver collection).
Using Model.collection means you bypass Mongoose middleware, validation, and casting.
This property is read-only. Modifying this property is a no-op.
Collection the model uses.
Connection the model uses.
Delete this document from the db.
Registered discriminators for this model.
Signal that we desire an increment of this documents version.
Returns the model instance used to create this document if no name specified.
If name specified, returns the model with the given name.
The name of the model
Saves this document by inserting a new document into the database if document.isNew is true,
or sends an updateOne operation with just the modified paths if isNew is false.
If save is successful, the returned promise will fulfill with the document
saved.
Replace the existing document with the given document (no atomic operators like $set).
This function triggers the following middleware.
Schema the model uses.
Requires MongoDB >= 3.6.0. Starts a MongoDB session
for benefits like causal consistency, retryable writes,
and transactions.
Calling MyModel.startSession() is equivalent to calling MyModel.db.startSession().
This function does not trigger any middleware.
Makes the indexes in MongoDB match the indexes defined in this model's
schema. This function will drop any indexes that are not defined in
the model's schema except the _id index, and build any indexes that
are in your schema but not in MongoDB.
See the introductory blog post
for more information.
You should be careful about running syncIndexes() on production applications under heavy load,
because index builds are expensive operations, and unexpected index drops can lead to degraded
performance. Before running syncIndexes(), you can use the diffIndexes() function
to check what indexes syncIndexes() will drop and create.
Translate any aliases fields/conditions so the final query or document object is pure
By default, translateAliases() overwrites raw fields with aliased fields.
So if n is an alias for name, { n: 'alias', name: 'raw' } will resolve to { name: 'alias' }.
However, you can set the errorOnDuplicates option to throw an error if there are potentially conflicting paths.
The translateAliases option for queries uses errorOnDuplicates.
Only translate arguments of object type anything else is returned raw
Same as updateOne(), except MongoDB will update all documents that match
filter (as opposed to just the first one) regardless of the value of
the multi option.
Note updateMany will not fire update middleware. Use pre('updateMany')
and post('updateMany') instead.
This function triggers the following middleware.
Update only the first document that matches filter.
This function triggers the following middleware.
Update an existing Atlas search index.
This function only works when connected to MongoDB Atlas.
Casts and validates the given object against this model's schema, passing the
given context to custom validators.
Requires a replica set running MongoDB >= 3.6.0. Watches the
underlying collection for changes using
MongoDB change streams.
This function does not trigger any middleware. In particular, it
does not trigger aggregate middleware.
The ChangeStream object is an event emitter that emits the following events:
Creates a Query, applies the passed conditions, and returns the Query.
For example, instead of writing:
we can instead write:
Since the Query class also supports where you can continue chaining
A Model is a class that's your primary tool for interacting with MongoDB.
An instance of a Model is called a Document.
In Mongoose, the term "Model" refers to subclasses of the mongoose.Model
class. You should not use the mongoose.Model class directly. The
mongoose.model() and
connection.model() functions
create subclasses of mongoose.Model as shown below.
Creates a Query and specifies a $where condition.
Sometimes you need to query for things in mongodb using a JavaScript expression. You can do so via find({ $where: javascript }), or you can use the mongoose shortcut method $where via a Query chain or from your mongoose Model.
Performs aggregations on the models collection.
If a callback is passed, the aggregate is executed and a Promise is returned. If a callback is not passed, the aggregate itself is returned.
This function triggers the following middleware.
Apply defaults to the given document or POJO.
takes an array of documents, gets the changes and inserts/updates documents in the database
 according to whether or not the document is new, or whether it has changes or not.
bulkSave uses bulkWrite under the hood, so it's mostly useful when dealing with many documents (10K+)
Sends multiple insertOne, updateOne, updateMany, replaceOne,
deleteOne, and/or deleteMany operations to the MongoDB server in one
command. This is faster than sending multiple independent operations (e.g.
if you use create()) because with bulkWrite() there is only one round
trip to MongoDB.
Mongoose will perform casting on all operations you provide.
This function does not trigger any middleware, neither save(), nor update().
If you need to trigger
save() middleware for every document use create() instead.
The supported operations are:
Cast the given POJO to the model's schema
Deletes all indexes that aren't defined in this model's schema. Used by
syncIndexes().
The returned promise resolves to a list of the dropped indexes' names as an array
Counts number of documents matching filter in a database collection.
If you want to count all documents in a large collection,
use the estimatedDocumentCount() function
instead. If you call countDocuments({}), MongoDB will always execute
a full collection scan and not use any indexes.
The countDocuments() function is similar to count(), but there are a
few operators that countDocuments() does not support.
Below are the operators that count() supports but countDocuments() does not,
and the suggested replacement:
Shortcut for saving one or more documents to the database.
MyModel.create(docs) does new MyModel(doc).save() for every doc in
docs.
This function triggers the following middleware.
Create the collection for this model. By default, if no indexes are specified,
mongoose will not create the collection for the model until any documents are
created. Use this method to create the collection explicitly.
Note 1: You may need to call this before starting a transaction
See https://www.mongodb.com/docs/manual/core/transactions/#transactions-and-operations
Note 2: You don't have to call this if your schema contains index or unique field.
In that case, just use Model.init()
Similar to ensureIndexes(), except for it uses the createIndex
function.
Create an Atlas search index.
This function only works when connected to MongoDB Atlas.
Connection instance the model uses.
Deletes all of the documents that match conditions from the collection.
It returns an object with the property deletedCount containing the number of documents deleted.
Behaves like remove(), but deletes all documents that match conditions
regardless of the single option.
This function triggers deleteMany query hooks. Read the
middleware docs to learn more.
Deletes the first document that matches conditions from the collection.
It returns an object with the property deletedCount indicating how many documents were deleted.
Behaves like remove(), but deletes at most one document regardless of the
single option.
This function triggers deleteOne query hooks. Read the
middleware docs to learn more.
Does a dry-run of Model.syncIndexes(), returning the indexes that syncIndexes() would drop and create if you were to run syncIndexes().
Adds a discriminator type.
Creates a Query for a distinct operation.
Delete an existing Atlas search index by name.
This function only works when connected to MongoDB Atlas.
Sends createIndex commands to mongo for each index declared in the schema.
The createIndex commands are sent in series.
After completion, an index event is emitted on this Model passing an error if one occurred.
NOTE: It is not recommended that you run this in production. Index creation may impact database performance depending on your load. Use with caution.
Estimates the number of documents in the MongoDB collection. Faster than
using countDocuments() for large collections because
estimatedDocumentCount() uses collection metadata rather than scanning
the entire collection.
Event emitter that reports any errors that occurred. Useful for global error
handling.
Returns a document with _id only if at least one document exists in the database that matches
the given filter, and null otherwise.
Under the hood, MyModel.exists({ answer: 42 }) is equivalent to
MyModel.findOne({ answer: 42 }).select({ _id: 1 }).lean()
This function triggers the following middleware.
Finds documents.
Mongoose casts the filter to match the model's schema before the command is sent.
See our query casting tutorial for
more information on how Mongoose casts filter.
Finds a single document by its _id field. findById(id) is almost*
equivalent to findOne({ _id: id }). If you want to query by a document's
_id, use findById() instead of findOne().
The id is cast based on the Schema before sending the command.
This function triggers the following middleware.
* Except for how it treats undefined. If you use findOne(), you'll see
that findOne(undefined) and findOne({ _id: undefined }) are equivalent
to findOne({}) and return arbitrary documents. However, mongoose
translates findById(undefined) into findOne({ _id: null }).
Issue a MongoDB findOneAndDelete() command by a document's _id field.
In other words, findByIdAndDelete(id) is a shorthand for
findOneAndDelete({ _id: id }).
This function triggers the following middleware.
Issues a mongodb findOneAndUpdate command by a document's _id field.
findByIdAndUpdate(id, ...) is equivalent to findOneAndUpdate({ _id: id }, ...).
Finds a matching document, updates it according to the update arg,
passing any options, and returns the found document (if any).
This function triggers the following middleware.
All top level update keys which are not atomic operation names are treated as set operations:
findOneAndX and findByIdAndX functions support limited validation. You can
enable validation by setting the runValidators option.
If you need full-fledged validation, use the traditional approach of first
retrieving the document.
Finds one document.
The conditions are cast to their respective SchemaTypes before the command is sent.
Note: conditions is optional, and if conditions is null or undefined,
mongoose will send an empty findOne command to MongoDB, which will return
an arbitrary document. If you're querying by _id, use findById() instead.
Issue a MongoDB findOneAndDelete() command.
Finds a matching document, removes it, and returns the found document (if any).
This function triggers the following middleware.
findOneAndX and findByIdAndX functions support limited validation. You can
enable validation by setting the runValidators option.
If you need full-fledged validation, use the traditional approach of first
retrieving the document.
Issue a MongoDB findOneAndReplace() command.
Finds a matching document, replaces it with the provided doc, and returns the document.
This function triggers the following query middleware.
Issues a mongodb findOneAndUpdate command.
Finds a matching document, updates it according to the update arg, passing any options, and returns the found document (if any) to the callback. The query executes if callback is passed else a Query object is returned.
All top level update keys which are not atomic operation names are treated as set operations:
findOneAndX and findByIdAndX functions support limited validation that
you can enable by setting the runValidators option.
If you need full-fledged validation, use the traditional approach of first
retrieving the document.
Shortcut for creating a new Document from existing raw data, pre-saved in the DB.
The document returned has no paths marked as modified initially.
This function is responsible for initializing the underlying connection in MongoDB based on schema options.
This function performs the following operations:
Mongoose calls this function automatically when a model is a created using
mongoose.model() or
connection.model(), so you
don't need to call init() to trigger index builds.
However, you may need to call init()  to get back a promise that will resolve when your indexes are finished.
Calling await Model.init() is helpful if you need to wait for indexes to build before continuing.
For example, if you want to wait for unique indexes to build before continuing with a test case.
Shortcut for validating an array of documents and inserting them into
MongoDB if they're all valid. This function is faster than .create()
because it only sends one operation to the server, rather than one for each
document.
Mongoose always validates each document before sending insertMany
to MongoDB. So if one document has a validation error, no documents will
be saved, unless you set
the ordered option to false.
This function does not trigger save middleware.
This function triggers the following middleware.
Helper for console.log. Given a model named 'MyModel', returns the string
'Model { MyModel }'.
Lists the indexes currently defined in MongoDB. This may or may not be
the same as the indexes defined in your schema depending on whether you
use the autoIndex option and if you
build indexes manually.
Populates document references.
Changed in Mongoose 6: the model you call populate() on should be the
"local field" model, not the "foreign field" model.
Returns the model instance used to create this document if no name specified.
If name specified, returns the model with the given name.
Additional properties to attach to the query when calling save() and
isNew is false.
Base Mongoose instance the model uses.
If this is a discriminator model, baseModelName is the name of
the base model.
The collection instance this model uses.
A Mongoose collection is a thin wrapper around a [MongoDB Node.js driver collection](MongoDB Node.js driver collection).
Using Model.collection means you bypass Mongoose middleware, validation, and casting.
This property is read-only. Modifying this property is a no-op.
Collection the model uses.
Connection the model uses.
Delete this document from the db.
Registered discriminators for this model.
Signal that we desire an increment of this documents version.
Returns the model instance used to create this document if no name specified.
If name specified, returns the model with the given name.
The name of the model
Saves this document by inserting a new document into the database if document.isNew is true,
or sends an updateOne operation with just the modified paths if isNew is false.
If save is successful, the returned promise will fulfill with the document
saved.
Replace the existing document with the given document (no atomic operators like $set).
This function triggers the following middleware.
Schema the model uses.
Requires MongoDB >= 3.6.0. Starts a MongoDB session
for benefits like causal consistency, retryable writes,
and transactions.
Calling MyModel.startSession() is equivalent to calling MyModel.db.startSession().
This function does not trigger any middleware.
Makes the indexes in MongoDB match the indexes defined in this model's
schema. This function will drop any indexes that are not defined in
the model's schema except the _id index, and build any indexes that
are in your schema but not in MongoDB.
See the introductory blog post
for more information.
You should be careful about running syncIndexes() on production applications under heavy load,
because index builds are expensive operations, and unexpected index drops can lead to degraded
performance. Before running syncIndexes(), you can use the diffIndexes() function
to check what indexes syncIndexes() will drop and create.
Translate any aliases fields/conditions so the final query or document object is pure
By default, translateAliases() overwrites raw fields with aliased fields.
So if n is an alias for name, { n: 'alias', name: 'raw' } will resolve to { name: 'alias' }.
However, you can set the errorOnDuplicates option to throw an error if there are potentially conflicting paths.
The translateAliases option for queries uses errorOnDuplicates.
Only translate arguments of object type anything else is returned raw
Same as updateOne(), except MongoDB will update all documents that match
filter (as opposed to just the first one) regardless of the value of
the multi option.
Note updateMany will not fire update middleware. Use pre('updateMany')
and post('updateMany') instead.
This function triggers the following middleware.
Update only the first document that matches filter.
This function triggers the following middleware.
Update an existing Atlas search index.
This function only works when connected to MongoDB Atlas.
Casts and validates the given object against this model's schema, passing the
given context to custom validators.
Requires a replica set running MongoDB >= 3.6.0. Watches the
underlying collection for changes using
MongoDB change streams.
This function does not trigger any middleware. In particular, it
does not trigger aggregate middleware.
The ChangeStream object is an event emitter that emits the following events:
Creates a Query, applies the passed conditions, and returns the Query.
For example, instead of writing:
we can instead write:
Since the Query class also supports where you can continue chaining
Mongoose constructor.
The exports object of the mongoose module is an instance of this class.
Most apps will only use this one instance.
The Mongoose Aggregate constructor
The Mongoose CastError constructor
The Mongoose Collection constructor
The Mongoose Connection constructor
Expose connection states for user-land
The Mongoose Date SchemaType.
The Mongoose Decimal128 SchemaType. Used for
declaring paths in your schema that should be
128-bit decimal floating points.
Do not use this to create a new Decimal128 instance, use mongoose.Types.Decimal128
instead.
The Mongoose Document constructor.
The Mongoose DocumentProvider constructor. Mongoose users should not have to
use this directly
The MongooseError constructor.
The Mongoose Mixed SchemaType. Used for
declaring paths in your schema that Mongoose's change tracking, casting,
and validation should ignore.
The Mongoose Model constructor.
The Mongoose constructor
The exports of the mongoose module is an instance of this class.
The Mongoose Number SchemaType. Used for
declaring paths in your schema that Mongoose should cast to numbers.
The Mongoose ObjectId SchemaType. Used for
declaring paths in your schema that should be
MongoDB ObjectIds.
Do not use this to create a new ObjectId instance, use mongoose.Types.ObjectId
instead.
The Mongoose Query constructor.
Expose connection states for user-land
The Mongoose Schema constructor
The Mongoose SchemaType constructor
The constructor used for schematype options
The various Mongoose SchemaTypes.
Alias of mongoose.Schema.Types for backwards compatibility.
The various Mongoose Types.
Using this exposed access to the ObjectId type, we can construct ids on demand.
The Mongoose VirtualType constructor
Opens the default mongoose connection.
The Mongoose module's default connection. Equivalent to mongoose.connections[0], see connections.
This is the connection used by default for every model created using mongoose.model.
To create a new connection, use createConnection().
An array containing all connections associated with this
Mongoose instance. By default, there is 1 connection. Calling
createConnection() adds a connection
to this array.
Creates a Connection instance.
Each connection instance maps to a single database. This method is helpful when managing multiple db connections.
Options passed take precedence over options included in connection strings.
Removes the model named name from the default connection, if it exists.
You can use this function to clean up any models you created in your tests to
prevent OverwriteModelErrors.
Equivalent to mongoose.connection.deleteModel(name).
Runs .close() on all connections in parallel.
Object with get() and set() containing the underlying driver this Mongoose instance
uses to communicate with the database. A driver is a Mongoose-specific interface that defines functions
like find().
Gets mongoose options
Returns true if the given value is a Mongoose ObjectId (using instanceof) or if the
given value is a 24 character hex string, which is the most commonly used string representation
of an ObjectId.
This function is similar to isValidObjectId(), but considerably more strict, because
isValidObjectId() will return true for any value that Mongoose can convert to an
ObjectId. That includes Mongoose documents, any string of length 12, and any number.
isObjectIdOrHexString() returns true only for ObjectId instances or 24 character hex
strings, and will return false for numbers, documents, and strings of length 12.
Returns true if Mongoose can cast the given value to an ObjectId, or
false otherwise.
Defines a model or retrieves it.
Models defined on the mongoose instance are available to all connection
created by the same mongoose instance.
If you call mongoose.model() with twice the same name but a different schema,
you will get an OverwriteModelError. If you call mongoose.model() with
the same name and same schema, you'll get the same schema back.
When no collection argument is passed, Mongoose uses the model name. If you don't like this behavior, either pass a collection name, use mongoose.pluralize(), or set your schemas collection name option.
Returns an array of model names created on this instance of Mongoose.
Does not include names of models created using connection.model().
The mquery query builder Mongoose uses.
Mongoose uses this function to get the current time when setting
timestamps. You may stub out this function
using a tool like Sinon for testing.
Use this function in post() middleware to replace the result
Declares a global plugin executed on all Schemas.
Equivalent to calling .plugin(fn) on each Schema you create.
Getter/setter around function for pluralizing collection names.
Sanitizes query filters against query selector injection attacks
by wrapping any nested objects that have a property whose name starts with $ in a $eq.
Sets mongoose options
key can be used a object to set multiple options at once.
If a error gets thrown for one option, other options will still be evaluated.
Currently supported options are:
Overwrites the current driver used by this Mongoose instance. A driver is a
Mongoose-specific interface that defines functions like find().
Use this function in pre() middleware to skip calling the wrapped function.
Requires MongoDB >= 3.6.0. Starts a MongoDB session
for benefits like causal consistency, retryable writes,
and transactions.
Calling mongoose.startSession() is equivalent to calling mongoose.connection.startSession().
Sessions are scoped to a connection, so calling mongoose.startSession()
starts a session on the default mongoose connection.
Syncs all the indexes for the models registered with this connection.
Tells sanitizeFilter() to skip the given object when filtering out potential query selector injection attacks.
Use this method when you have a known query selector that you want to use.
The Mongoose version
Most Popular Articles
EbooksThe 80/20 Guide to ES2015 Generators
The 80/20 Guide to ES2015 Generators
Collations are another great new feature in MongoDB 3.4. You can think of collations as a way to configure how MongoDB orders and compares strings. In this article, I'll demonstrate some basic uses of collations and show how to use them in Node.js with the MongoDB driver and mongoose.
At a previous company I was tasked with implementing a city search bar much like
Airbnb's:
The problem is how to make "San Jose" match "San José" with the acute accent over the 'e'. Before collations, your best bet would be to use a module like diacritics to remove all diacritics from the city. In practice you would have a displayName that would include diacritics for display, and a searchName with diacritics removed for searching.
With collations, searching with diacritics is easy. Let's say you insert 2 documents, one with "San Jose" as the California city is commonly spelled, and another with "San José".
If you use the new collation() function, you can make MongoDB ignore the differences in diacritics using the strength option. The collation arguments take experience to become comfortable with. For now, remember that strength: 1 means MongoDB will ignore case and diacritics.
Keep in mind that collations do not currently work with regular expression search, so db.cities.find({ name: /^San Jose/ }) will not match "San José".
Collations aren't just useful for matching, they also help with sorting. By default MongoDB sorts strings by their characters' ASCII order (modulo non-ASCII characters), so 'Alpha' comes before 'Zeta' comes before '_' comes before 'alpha'.
The caseLevel option, if set, sorts so that 'alpha' and 'Alpha' come before 'zeta' and 'Zeta'.
Another annoying issue with sorting strings is handling numbers. For example, let's say you insert a bunch of files named 'invoice_1', 'invoice_2', 'invoice_10', and 'invoice_100'. In conventional sort order, 'invoice_2' will come after 'invoice_10' and 'invoice_100'.
If you turn on the numericOrdering flag, MongoDB will sort numeric substrings based on their numeric value rather than by ASCII characters. In other words, the order will be 'invoice_1', 'invoice_2', 'invoice_10', 'invoice_100', which makes more sense in this case.
Version 2.2.10 of the MongoDB driver and Mongoose 4.8.0 include helpers for collations. Here's an example of using a collation with find() using the MongoDB driver:
And using the mongoose query builder's collation() helper function:
Collations are powerful, but far from the only great new feature in MongoDB 3.4. I previously wrote about the Decimal type, the $facet aggregation operator, and the $graphLookup aggregation operator. Check out those articles and learn how to take advantage of MongoDB 3.4 in Node.js!
Docs Home → MongoDB Manual
On this page
Capped collections are fixed-size
collections that support high-throughput operations that insert
and retrieve documents based on insertion order. Capped
collections work in a way similar to circular buffers: once a
collection fills its allocated space, it makes room for new documents
by overwriting the oldest documents in the collection.
See createCollection() or create
for more information on creating capped collections.
As an alternative to capped collections, consider MongoDB's
TTL (Time To Live) indexes. As
described in Expire Data from Collections by Setting TTL, these indexes allow you
to expire and remove data from normal collections based on the value
of a date-typed field and a TTL value for the index.
TTL indexes are not compatible with capped collections.
Capped collections guarantee preservation of the insertion order. As a
result, queries do not need an index to return documents in insertion
order. Without this indexing overhead, capped collections can support
higher insertion throughput.
To make room for new documents, capped collections automatically remove
the oldest documents in the collection without requiring scripts or
explicit remove operations.
Consider the following potential use cases for capped
collections:
Store log information generated by high-volume systems. Inserting
documents in a capped collection without an index is close to the
speed of writing log information directly to a file
system. Furthermore, the built-in first-in-first-out property
maintains the order of events, while managing storage use.
For example, the oplog
uses a capped collection.
Cache small amounts of data in a capped collections. Since caches
are read rather than write heavy, you would either need to ensure
that this collection always remains in the working set (i.e. in
RAM) or accept some write penalty for the required index or
indexes.
The oplog.rs collection that stores a log
of the operations in a replica set uses a capped collection.
Starting in MongoDB 4.0, unlike other capped collections, the oplog can
grow past its configured size limit to avoid deleting the majority
commit point.
MongoDB rounds the capped size of the oplog up to the nearest
integer multiple of 256, in bytes.
Capped collections have an _id field and an index on the _id
field by default.
Starting in MongoDB 5.0, you cannot use read concern
"snapshot" when reading from a
capped collection.
If you plan to update documents in a capped collection, create an index
so that these update operations do not require a collection scan.
You cannot shard a capped collection.
Use natural ordering to retrieve the most recently inserted elements
from the collection efficiently. This is similar to using the tail
command on a log file.
The aggregation pipeline stage $out
cannot write results to a capped collection.
You cannot write to capped collections in transactions.
Capped collections are not supported in Stable API V1.
You must create capped collections explicitly using the
db.createCollection() method, which is a
mongosh helper for the create command.
When creating a capped collection you must specify the maximum size of
the collection in bytes, which MongoDB will pre-allocate for the
collection. The size of the capped collection includes a small amount of
space for internal overhead.
The value that you provide for the size field
must be greater than 0 and less than or equal to
1024^5 (1 PB). MongoDB rounds the size of all capped
collections up to the nearest integer multiple of 256, in bytes.
Additionally, you may also specify a maximum number of documents for the
collection using the max field as in the following document:
The size field is always required, even when
you specify the max number of documents. MongoDB removes older
documents if a collection reaches the maximum size limit before it
reaches the maximum document count.
db.createCollection() and  create.
If you perform a find() on a capped collection
with no ordering specified, MongoDB guarantees that the ordering of
results is the same as the insertion order.
To retrieve documents in reverse insertion order, issue
find() along with the sort()
method with the $natural parameter set to -1, as shown
in the following example:
Use the isCapped() method to determine if a
collection is capped, as follows:
You can convert a non-capped collection to a capped collection with
the convertToCapped command:
The size parameter specifies the size of the capped collection in
bytes.
This holds a database exclusive lock for the duration of the operation.
Other operations which lock the same database will be blocked until the
operation completes. See What locks are taken by some common client operations? for
operations that lock the database.
New in version 6.0.
You can resize a capped collection using the collMod command's
cappedSize option to set the cappedSize in bytes. cappedSize must be
greater than 0 and less than or equal to 1024^5 (1 PB).
Before you can resize a capped collection, you must have already set
the featureCompatibilityVersion to at least version
"6.0".
For example, the following command sets the maximum size of the "log" capped
collection to 100000 bytes:
New in version 6.0.
To change the maximum number of documents in a capped collection, use the
collMod command's cappedMax option. If cappedMax is less
than or equal to 0, there is no maximum document limit. If
cappedMax is less than the current number of documents in the
collection, MongoDB removes the excess documents on the next insert operation.
For example, the following command sets the maximum number of documents in the
"log" capped collection to 500:
You can use a tailable cursor with capped collections. Similar to the
Unix tail -f command, the tailable cursor "tails" the end of a
capped collection. As new documents are inserted into the capped
collection, you can use the tailable cursor to continue retrieving
documents.
See Tailable Cursors for information on creating
a tailable cursor.
On this page
About
Support
Social
Docs Home → MongoDB Manual
On this page
Capped collections are fixed-size
collections that support high-throughput operations that insert
and retrieve documents based on insertion order. Capped
collections work in a way similar to circular buffers: once a
collection fills its allocated space, it makes room for new documents
by overwriting the oldest documents in the collection.
See createCollection() or create
for more information on creating capped collections.
As an alternative to capped collections, consider MongoDB's
TTL (Time To Live) indexes. As
described in Expire Data from Collections by Setting TTL, these indexes allow you
to expire and remove data from normal collections based on the value
of a date-typed field and a TTL value for the index.
TTL indexes are not compatible with capped collections.
Capped collections guarantee preservation of the insertion order. As a
result, queries do not need an index to return documents in insertion
order. Without this indexing overhead, capped collections can support
higher insertion throughput.
To make room for new documents, capped collections automatically remove
the oldest documents in the collection without requiring scripts or
explicit remove operations.
Consider the following potential use cases for capped
collections:
Store log information generated by high-volume systems. Inserting
documents in a capped collection without an index is close to the
speed of writing log information directly to a file
system. Furthermore, the built-in first-in-first-out property
maintains the order of events, while managing storage use.
For example, the oplog
uses a capped collection.
Cache small amounts of data in a capped collections. Since caches
are read rather than write heavy, you would either need to ensure
that this collection always remains in the working set (i.e. in
RAM) or accept some write penalty for the required index or
indexes.
The oplog.rs collection that stores a log
of the operations in a replica set uses a capped collection.
Starting in MongoDB 4.0, unlike other capped collections, the oplog can
grow past its configured size limit to avoid deleting the majority
commit point.
MongoDB rounds the capped size of the oplog up to the nearest
integer multiple of 256, in bytes.
Capped collections have an _id field and an index on the _id
field by default.
Starting in MongoDB 5.0, you cannot use read concern
"snapshot" when reading from a
capped collection.
If you plan to update documents in a capped collection, create an index
so that these update operations do not require a collection scan.
You cannot shard a capped collection.
Use natural ordering to retrieve the most recently inserted elements
from the collection efficiently. This is similar to using the tail
command on a log file.
The aggregation pipeline stage $out
cannot write results to a capped collection.
You cannot write to capped collections in transactions.
Capped collections are not supported in Stable API V1.
You must create capped collections explicitly using the
db.createCollection() method, which is a
mongosh helper for the create command.
When creating a capped collection you must specify the maximum size of
the collection in bytes, which MongoDB will pre-allocate for the
collection. The size of the capped collection includes a small amount of
space for internal overhead.
The value that you provide for the size field
must be greater than 0 and less than or equal to
1024^5 (1 PB). MongoDB rounds the size of all capped
collections up to the nearest integer multiple of 256, in bytes.
Additionally, you may also specify a maximum number of documents for the
collection using the max field as in the following document:
The size field is always required, even when
you specify the max number of documents. MongoDB removes older
documents if a collection reaches the maximum size limit before it
reaches the maximum document count.
db.createCollection() and  create.
If you perform a find() on a capped collection
with no ordering specified, MongoDB guarantees that the ordering of
results is the same as the insertion order.
To retrieve documents in reverse insertion order, issue
find() along with the sort()
method with the $natural parameter set to -1, as shown
in the following example:
Use the isCapped() method to determine if a
collection is capped, as follows:
You can convert a non-capped collection to a capped collection with
the convertToCapped command:
The size parameter specifies the size of the capped collection in
bytes.
This holds a database exclusive lock for the duration of the operation.
Other operations which lock the same database will be blocked until the
operation completes. See What locks are taken by some common client operations? for
operations that lock the database.
New in version 6.0.
You can resize a capped collection using the collMod command's
cappedSize option to set the cappedSize in bytes. cappedSize must be
greater than 0 and less than or equal to 1024^5 (1 PB).
Before you can resize a capped collection, you must have already set
the featureCompatibilityVersion to at least version
"6.0".
For example, the following command sets the maximum size of the "log" capped
collection to 100000 bytes:
New in version 6.0.
To change the maximum number of documents in a capped collection, use the
collMod command's cappedMax option. If cappedMax is less
than or equal to 0, there is no maximum document limit. If
cappedMax is less than the current number of documents in the
collection, MongoDB removes the excess documents on the next insert operation.
For example, the following command sets the maximum number of documents in the
"log" capped collection to 500:
You can use a tailable cursor with capped collections. Similar to the
Unix tail -f command, the tailable cursor "tails" the end of a
capped collection. As new documents are inserted into the capped
collection, you can use the tailable cursor to continue retrieving
documents.
See Tailable Cursors for information on creating
a tailable cursor.
On this page
About
Support
Social
Docs Home → MongoDB Manual
On this page
Capped collections are fixed-size
collections that support high-throughput operations that insert
and retrieve documents based on insertion order. Capped
collections work in a way similar to circular buffers: once a
collection fills its allocated space, it makes room for new documents
by overwriting the oldest documents in the collection.
See createCollection() or create
for more information on creating capped collections.
As an alternative to capped collections, consider MongoDB's
TTL (Time To Live) indexes. As
described in Expire Data from Collections by Setting TTL, these indexes allow you
to expire and remove data from normal collections based on the value
of a date-typed field and a TTL value for the index.
TTL indexes are not compatible with capped collections.
Capped collections guarantee preservation of the insertion order. As a
result, queries do not need an index to return documents in insertion
order. Without this indexing overhead, capped collections can support
higher insertion throughput.
To make room for new documents, capped collections automatically remove
the oldest documents in the collection without requiring scripts or
explicit remove operations.
Consider the following potential use cases for capped
collections:
Store log information generated by high-volume systems. Inserting
documents in a capped collection without an index is close to the
speed of writing log information directly to a file
system. Furthermore, the built-in first-in-first-out property
maintains the order of events, while managing storage use.
For example, the oplog
uses a capped collection.
Cache small amounts of data in a capped collections. Since caches
are read rather than write heavy, you would either need to ensure
that this collection always remains in the working set (i.e. in
RAM) or accept some write penalty for the required index or
indexes.
The oplog.rs collection that stores a log
of the operations in a replica set uses a capped collection.
Starting in MongoDB 4.0, unlike other capped collections, the oplog can
grow past its configured size limit to avoid deleting the majority
commit point.
MongoDB rounds the capped size of the oplog up to the nearest
integer multiple of 256, in bytes.
Capped collections have an _id field and an index on the _id
field by default.
Starting in MongoDB 5.0, you cannot use read concern
"snapshot" when reading from a
capped collection.
If you plan to update documents in a capped collection, create an index
so that these update operations do not require a collection scan.
You cannot shard a capped collection.
Use natural ordering to retrieve the most recently inserted elements
from the collection efficiently. This is similar to using the tail
command on a log file.
The aggregation pipeline stage $out
cannot write results to a capped collection.
You cannot write to capped collections in transactions.
Capped collections are not supported in Stable API V1.
You must create capped collections explicitly using the
db.createCollection() method, which is a
mongosh helper for the create command.
When creating a capped collection you must specify the maximum size of
the collection in bytes, which MongoDB will pre-allocate for the
collection. The size of the capped collection includes a small amount of
space for internal overhead.
The value that you provide for the size field
must be greater than 0 and less than or equal to
1024^5 (1 PB). MongoDB rounds the size of all capped
collections up to the nearest integer multiple of 256, in bytes.
Additionally, you may also specify a maximum number of documents for the
collection using the max field as in the following document:
The size field is always required, even when
you specify the max number of documents. MongoDB removes older
documents if a collection reaches the maximum size limit before it
reaches the maximum document count.
db.createCollection() and  create.
If you perform a find() on a capped collection
with no ordering specified, MongoDB guarantees that the ordering of
results is the same as the insertion order.
To retrieve documents in reverse insertion order, issue
find() along with the sort()
method with the $natural parameter set to -1, as shown
in the following example:
Use the isCapped() method to determine if a
collection is capped, as follows:
You can convert a non-capped collection to a capped collection with
the convertToCapped command:
The size parameter specifies the size of the capped collection in
bytes.
This holds a database exclusive lock for the duration of the operation.
Other operations which lock the same database will be blocked until the
operation completes. See What locks are taken by some common client operations? for
operations that lock the database.
New in version 6.0.
You can resize a capped collection using the collMod command's
cappedSize option to set the cappedSize in bytes. cappedSize must be
greater than 0 and less than or equal to 1024^5 (1 PB).
Before you can resize a capped collection, you must have already set
the featureCompatibilityVersion to at least version
"6.0".
For example, the following command sets the maximum size of the "log" capped
collection to 100000 bytes:
New in version 6.0.
To change the maximum number of documents in a capped collection, use the
collMod command's cappedMax option. If cappedMax is less
than or equal to 0, there is no maximum document limit. If
cappedMax is less than the current number of documents in the
collection, MongoDB removes the excess documents on the next insert operation.
For example, the following command sets the maximum number of documents in the
"log" capped collection to 500:
You can use a tailable cursor with capped collections. Similar to the
Unix tail -f command, the tailable cursor "tails" the end of a
capped collection. As new documents are inserted into the capped
collection, you can use the tailable cursor to continue retrieving
documents.
See Tailable Cursors for information on creating
a tailable cursor.
On this page
About
Support
Social
Schema constructor.
When nesting schemas, (children in the example above), always declare the child schema first before passing it into its parent.
The various built-in Mongoose Schema Types.
Using this exposed access to the Mixed SchemaType, we can use them in our schema.
The allowed index types
Adds key path / schema type pairs to this schema.
Add an alias for path. This means getting or setting the alias
is equivalent to getting or setting the path.
Array of child schemas (from document arrays and single nested subdocs)
and their corresponding compiled models. Each element of the array is
an object with 2 properties: schema and model.
This property is typically only useful for plugin authors and advanced users.
You do not need to interact with this property at all to use mongoose.
Remove all indexes from this schema.
clearIndexes only removes indexes from your schema object. Does not affect the indexes
in MongoDB.
Returns a deep copy of the schema
Inherit a Schema by applying a discriminator on an existing Schema.
Iterates the schemas paths similar to Array#forEach.
The callback is passed the pathname and the schemaType instance.
Gets a schema option.
Defines an index (most likely compound) for this schema.
Returns a list of indexes that this schema declares, via schema.index() or by index: true in a path's options.
Indexes are expressed as an array [spec, options].
Plugins can use the return value of this function to modify a schema's indexes.
For example, the below plugin makes every index unique by default.
Loads an ES6 class into a schema. Maps setters + getters, static methods,
and instance methods
to schema virtuals,
statics, and
methods.
Adds an instance method to documents constructed from Models compiled from this schema.
If a hash of name/fn pairs is passed as the only argument, each name/fn pair will be added as methods.
NOTE: Schema.method() adds instance methods to the Schema.methods object. You can also add instance methods directly to the Schema.methods object as seen in the guide
The original object passed to the schema constructor
Returns a new schema that has the paths from the original schema, minus the omitted ones.
This method is analagous to Lodash's omit() function for Mongoose schemas.
Gets/sets schema paths.
Sets a path (if arity 2)
Gets a path (if arity 1)
Returns the pathType of path for this schema.
Given a path, returns whether it is a real, virtual, nested, or ad-hoc/undefined path.
The paths defined on this schema. The keys are the top-level paths
in this schema, and the values are instances of the SchemaType class.
Returns a new schema that has the picked paths from this schema.
This method is analagous to Lodash's pick() function for Mongoose schemas.
Registers a plugin for this schema.
Or with Options:
Defines a post hook for the document
Defines a pre hook for the model.
Adds a method call to the queue.
Removes the given path (or [paths]).
Or as a Array:
Remove an index by name or index specification.
removeIndex only removes indexes from your schema object. Does not affect the indexes
in MongoDB.
Removes the given virtual or virtuals from the schema.
Returns an Array of path strings that are required by this schema.
Add an Atlas search index that Mongoose will create using Model.createSearchIndex().
This function only works when connected to MongoDB Atlas.
Sets a schema option.
Adds static "class" methods to Models compiled from this schema.
If a hash of name/fn pairs is passed as the only argument, each name/fn pair will be added as methods.
If a hash of name/fn pairs is passed as the only argument, each name/fn pair will be added as statics.
Creates a virtual type with the given name.
Returns the virtual type with the given name.
Object containing all virtuals defined on this schema.
The objects' keys are the virtual paths and values are instances of VirtualType.
This property is typically only useful for plugin authors and advanced users.
You do not need to interact with this property at all to use mongoose.
Reserved document keys.
Keys in this object are names that are warned in schema declarations
because they have the potential to break Mongoose/ Mongoose plugins functionality. If you create a schema
using new Schema() with one of these property names, Mongoose will log a warning.
NOTE: Use of these terms as method names is permitted, but play at your own risk, as they may be existing mongoose document methods you are stomping on.
Schema constructor.
When nesting schemas, (children in the example above), always declare the child schema first before passing it into its parent.
The various built-in Mongoose Schema Types.
Using this exposed access to the Mixed SchemaType, we can use them in our schema.
The allowed index types
Adds key path / schema type pairs to this schema.
Add an alias for path. This means getting or setting the alias
is equivalent to getting or setting the path.
Array of child schemas (from document arrays and single nested subdocs)
and their corresponding compiled models. Each element of the array is
an object with 2 properties: schema and model.
This property is typically only useful for plugin authors and advanced users.
You do not need to interact with this property at all to use mongoose.
Remove all indexes from this schema.
clearIndexes only removes indexes from your schema object. Does not affect the indexes
in MongoDB.
Returns a deep copy of the schema
Inherit a Schema by applying a discriminator on an existing Schema.
Iterates the schemas paths similar to Array#forEach.
The callback is passed the pathname and the schemaType instance.
Gets a schema option.
Defines an index (most likely compound) for this schema.
Returns a list of indexes that this schema declares, via schema.index() or by index: true in a path's options.
Indexes are expressed as an array [spec, options].
Plugins can use the return value of this function to modify a schema's indexes.
For example, the below plugin makes every index unique by default.
Loads an ES6 class into a schema. Maps setters + getters, static methods,
and instance methods
to schema virtuals,
statics, and
methods.
Adds an instance method to documents constructed from Models compiled from this schema.
If a hash of name/fn pairs is passed as the only argument, each name/fn pair will be added as methods.
NOTE: Schema.method() adds instance methods to the Schema.methods object. You can also add instance methods directly to the Schema.methods object as seen in the guide
The original object passed to the schema constructor
Returns a new schema that has the paths from the original schema, minus the omitted ones.
This method is analagous to Lodash's omit() function for Mongoose schemas.
Gets/sets schema paths.
Sets a path (if arity 2)
Gets a path (if arity 1)
Returns the pathType of path for this schema.
Given a path, returns whether it is a real, virtual, nested, or ad-hoc/undefined path.
The paths defined on this schema. The keys are the top-level paths
in this schema, and the values are instances of the SchemaType class.
Returns a new schema that has the picked paths from this schema.
This method is analagous to Lodash's pick() function for Mongoose schemas.
Registers a plugin for this schema.
Or with Options:
Defines a post hook for the document
Defines a pre hook for the model.
Adds a method call to the queue.
Removes the given path (or [paths]).
Or as a Array:
Remove an index by name or index specification.
removeIndex only removes indexes from your schema object. Does not affect the indexes
in MongoDB.
Removes the given virtual or virtuals from the schema.
Returns an Array of path strings that are required by this schema.
Add an Atlas search index that Mongoose will create using Model.createSearchIndex().
This function only works when connected to MongoDB Atlas.
Sets a schema option.
Adds static "class" methods to Models compiled from this schema.
If a hash of name/fn pairs is passed as the only argument, each name/fn pair will be added as methods.
If a hash of name/fn pairs is passed as the only argument, each name/fn pair will be added as statics.
Creates a virtual type with the given name.
Returns the virtual type with the given name.
Object containing all virtuals defined on this schema.
The objects' keys are the virtual paths and values are instances of VirtualType.
This property is typically only useful for plugin authors and advanced users.
You do not need to interact with this property at all to use mongoose.
Reserved document keys.
Keys in this object are names that are warned in schema declarations
because they have the potential to break Mongoose/ Mongoose plugins functionality. If you create a schema
using new Schema() with one of these property names, Mongoose will log a warning.
NOTE: Use of these terms as method names is permitted, but play at your own risk, as they may be existing mongoose document methods you are stomping on.
Query constructor used for building queries. You do not need
to instantiate a Query directly. Instead use Model functions like
Model.find().
Specifies a javascript function or expression to pass to MongoDBs query system.
Only use $where when you have a condition that cannot be met using other MongoDB operators like $lt.
Be sure to read about all of its caveats before using.
Specifies an $all query condition.
When called with one argument, the most recent path passed to where() is used.
Sets the allowDiskUse option,
which allows the MongoDB server to use more than 100 MB for this query's sort(). This option can
let you work around QueryExceededMemoryLimitNoDiskUseAllowed errors from the MongoDB server.
Note that this option requires MongoDB server >= 4.4. Setting this option is a no-op for MongoDB 4.2
and earlier.
Calling query.allowDiskUse(v) is equivalent to query.setOptions({ allowDiskUse: v })
Specifies arguments for a $and condition.
Specifies the batchSize option.
Cannot be used with distinct()
Specifies a $box condition
Casts this query to the schema of model
If obj is present, it is cast instead of this query.
Executes the query returning a Promise which will be
resolved with either the doc(s) or rejected with the error.
Like .then(), but only takes a rejection handler.
More about Promise catch() in JavaScript.
DEPRECATED Alias for circle
Deprecated. Use circle instead.
DEPRECATED Specifies a $centerSphere condition
Deprecated. Use circle instead.
Specifies a $center or $centerSphere condition.
Make a copy of this query so you can re-execute it.
Adds a collation to this op (MongoDB 3.4 and up)
Specifies the comment option.
Cannot be used with distinct()
Specifies this query as a countDocuments() query. Behaves like count(),
except it always does a full collection scan when passed an empty filter {}.
There are also minor differences in how countDocuments() handles
$where and a couple geospatial operators.
versus count().
This function triggers the following middleware.
The countDocuments() function is similar to count(), but there are a
few operators that countDocuments() does not support.
Below are the operators that count() supports but countDocuments() does not,
and the suggested replacement:
Returns a wrapper around a mongodb driver cursor.
A QueryCursor exposes a Streams3 interface, as well as a .next() function.
The .cursor() function triggers pre find hooks, but not post find hooks.
Declare and/or execute this query as a deleteMany() operation. Works like
remove, except it deletes every document that matches filter in the
collection, regardless of the value of single.
This function triggers deleteMany middleware.
This function calls the MongoDB driver's Collection#deleteMany() function.
The returned promise resolves to an
object that contains 3 properties:
Declare and/or execute this query as a deleteOne() operation. Works like
remove, except it deletes at most one document regardless of the single
option.
This function triggers deleteOne middleware.
This function calls the MongoDB driver's Collection#deleteOne() function.
The returned promise resolves to an
object that contains 3 properties:
Declares or executes a distinct() operation.
This function does not trigger any middleware.
Specifies an $elemMatch condition
Specifies the complementary comparison value for paths specified with where()
Gets/sets the error flag on this query. If this flag is not null or
undefined, the exec() promise will reject without executing.
Note that query casting runs after hooks, so cast errors will override
custom errors.
Specifies this query as a estimatedDocumentCount() query. Faster than
using countDocuments() for large collections because
estimatedDocumentCount() uses collection metadata rather than scanning
the entire collection.
estimatedDocumentCount() does not accept a filter. Model.find({ foo: bar }).estimatedDocumentCount()
is equivalent to Model.find().estimatedDocumentCount()
This function triggers the following middleware.
Executes the query
Specifies an $exists condition
Sets the explain option,
which makes this query return detailed execution stats instead of the actual
query result. This method is useful for determining what index your queries
use.
Calling query.explain(v) is equivalent to query.setOptions({ explain: v })
Executes the query returning a Promise which will be
resolved with .finally() chained.
More about Promise finally() in JavaScript.
Find all documents that match selector. The result will be an array of documents.
If there are too many documents in the result to fit in memory, use
Query.prototype.cursor()
Declares the query a findOne operation. When executed, the first found document is passed to the callback.
The result of the query is a single document, or null if no document was found.
This function triggers the following middleware.
Issues a MongoDB findOneAndDelete command.
Finds a matching document, removes it, and returns the found document (if any).
This function triggers the following middleware.
Issues a MongoDB findOneAndReplace command.
Finds a matching document, removes it, and returns the found document (if any).
This function triggers the following middleware.
Issues a mongodb findOneAndUpdate() command.
Finds a matching document, updates it according to the update arg, passing any options, and returns the found
document (if any).
This function triggers the following middleware.
Specifies a $geometry condition
The argument is assigned to the most recent path passed to where().
geometry() must come after either intersects() or within().
The object argument must contain type and coordinates properties.
For update operations, returns the value of a path in the update's $set.
Useful for writing getters/setters that can work with both update operations
and save().
Returns the current query filter (also known as conditions) as a POJO.
Gets query options.
Gets a list of paths to be populated by this query
Returns the current query filter. Equivalent to getFilter().
You should use getFilter() instead of getQuery() where possible. getQuery()
will likely be deprecated in a future release.
Returns the current update operations as a JSON object.
Specifies a $gt query condition.
When called with one argument, the most recent path passed to where() is used.
Specifies a $gte query condition.
When called with one argument, the most recent path passed to where() is used.
Sets query hints.
Cannot be used with distinct()
Specifies an $in query condition.
When called with one argument, the most recent path passed to where() is used.
Declares an intersects query for geometry().
MUST be used after where().
In Mongoose 3.7, intersects changed from a getter to a function. If you need the old syntax, use this.
Wrapper function to call isPathSelectedInclusive on a query.
Requests acknowledgement that this operation has been persisted to MongoDB's
on-disk journal.
This option is only valid for operations that write to the database:
Defaults to the schema's writeConcern.j option
Sets the lean option.
Documents returned from queries with the lean option enabled are plain
javascript objects, not Mongoose Documents. They have no
save method, getters/setters, virtuals, or other Mongoose features.
Lean is great for high-performance, read-only cases,
especially when combined
with cursors.
If you need virtuals, getters/setters, or defaults with lean(), you need
to use a plugin. See:
Specifies the maximum number of documents the query will return.
Cannot be used with distinct()
Specifies a $lt query condition.
When called with one argument, the most recent path passed to where() is used.
Specifies a $lte query condition.
When called with one argument, the most recent path passed to where() is used.
Specifies a maxDistance query condition.
When called with one argument, the most recent path passed to where() is used.
Sets the maxTimeMS
option. This will tell the MongoDB server to abort if the query or write op
has been running for more than ms milliseconds.
Calling query.maxTimeMS(v) is equivalent to query.setOptions({ maxTimeMS: v })
Merges another Query or conditions object into this one.
When a Query is passed, conditions, field selection and options are merged.
Specifies a $mod condition, filters documents for documents whose
path property is a number that is equal to remainder modulo divisor.
The model this query is associated with.
Getter/setter around the current mongoose-specific options for this query
Below are the current Mongoose-specific options.
Mongoose maintains a separate object for internal options because
Mongoose sends Query.prototype.options to the MongoDB server, and the
above options are not relevant for the MongoDB server.
Specifies a $ne query condition.
When called with one argument, the most recent path passed to where() is used.
Specifies a $near or $nearSphere condition
These operators return documents sorted by distance.
DEPRECATED Specifies a $nearSphere condition
Deprecated. Use query.near() instead with the spherical option set to true.
Specifies an $nin query condition.
When called with one argument, the most recent path passed to where() is used.
Specifies arguments for a $nor condition.
Specifies arguments for an $or condition.
Make this query throw an error if no documents match the given filter.
This is handy for integrating with async/await, because orFail() saves you
an extra if statement to check if no document was found.
Specifies a $polygon condition
Specifies paths which should be populated with other documents.
Paths are populated after the query executes and a response is received. A
separate query is then executed for each path specified for population. After
a response for each query has also been returned, the results are passed to
the callback.
Add post middleware to this query instance. Doesn't affect
other queries.
Add pre middleware to this query instance. Doesn't affect
other queries.
Get/set the current projection (AKA fields). Pass null to remove the
current projection.
Unlike projection(), the select() function modifies the current
projection in place. This function overwrites the existing projection.
Determines the MongoDB nodes from which to read.
Aliases
Read more about how to use read preferences here.
Sets the readConcern option for the query.
Aliases
Read more about how to use read concern here.
Specifies a $regex query condition.
When called with one argument, the most recent path passed to where() is used.
Declare and/or execute this query as a replaceOne() operation.
MongoDB will replace the existing document and will not accept any atomic operators ($set, etc.)
Note replaceOne will not fire update middleware. Use pre('replaceOne')
and post('replaceOne') instead.
This function triggers the following middleware.
Specifies which document fields to include or exclude (also known as the query "projection")
When using string syntax, prefixing a path with - will flag that path as excluded. When a path does not have the - prefix, it is included. Lastly, if a path is prefixed with +, it forces inclusion of the path, which is useful for paths excluded at the schema level.
A projection must be either inclusive or exclusive. In other words, you must
either list the fields to include (which excludes all others), or list the fields
to exclude (which implies all other fields are included). The _id field is the only exception because MongoDB includes it by default.
Determines if field selection has been made.
Determines if exclusive field selection has been made.
Determines if inclusive field selection has been made.
Sets the MongoDB session
associated with this query. Sessions are how you mark a query as part of a
transaction.
Calling session(null) removes the session from this query.
Adds a $set to this query's update without changing the operation.
This is useful for query middleware so you can add an update regardless
of whether you use updateOne(), updateMany(), findOneAndUpdate(), etc.
Sets query options. Some options only make sense for certain operations.
The following options are only for find():
The following options are only for write operations: updateOne(), updateMany(), replaceOne(), findOneAndUpdate(), and findByIdAndUpdate():
The following options are only for find(), findOne(), findById(), findOneAndUpdate(), findOneAndReplace(), findOneAndDelete(), and findByIdAndUpdate():
The following options are only for all operations except updateOne(), updateMany(), deleteOne(), and deleteMany():
The following options are for find(), findOne(), findOneAndUpdate(), findOneAndDelete(), updateOne(), and deleteOne():
The following options are for findOneAndUpdate() and findOneAndDelete()
The following options are for all operations:
Sets the query conditions to the provided JSON object.
Sets the current update operation to new value.
Specifies a $size query condition.
When called with one argument, the most recent path passed to where() is used.
Specifies the number of documents to skip.
Cannot be used with distinct()
Specifies a $slice projection for an array.
Note: If the absolute value of the number of elements to be sliced is greater than the number of elements in the array, all array elements will be returned.
Note: If the number of elements to skip is positive and greater than the number of elements in the array, an empty array will be returned.
Note: If the number of elements to skip is negative and its absolute value is greater than the number of elements in the array, the starting position is the start of the array.
Sets the sort order
If an object is passed, values allowed are asc, desc, ascending, descending, 1, and -1.
If a string is passed, it must be a space delimited list of path names. The
sort order of each path is ascending unless the path name is prefixed with -
which will be treated as descending.
Cannot be used with distinct()
Sets the tailable option (for use with capped collections).
Cannot be used with distinct()
Executes the query returning a Promise which will be
resolved with either the doc(s) or rejected with the error.
More about then() in JavaScript.
Converts this query to a customized, reusable query constructor with all arguments and options retained.
Runs a function fn and treats the return value of fn as the new value
for the query to resolve to.
Any functions you pass to transform() will run after any post hooks.
Declare and/or execute this query as an updateMany() operation.
MongoDB will update all documents that match filter (as opposed to just the first one).
Note updateMany will not fire update middleware. Use pre('updateMany')
and post('updateMany') instead.
This function triggers the following middleware.
Declare and/or execute this query as an updateOne() operation.
MongoDB will update only the first document that matches filter.
Note updateOne will not fire update middleware. Use pre('updateOne')
and post('updateOne') instead.
This function triggers the following middleware.
Sets the specified number of mongod servers, or tag set of mongod servers,
that must acknowledge this write before this write is considered successful.
This option is only valid for operations that write to the database:
Defaults to the schema's writeConcern.w option
Specifies a path for use with chaining.
Defines a $within or $geoWithin argument for geo-spatial queries.
MUST be used after where().
As of Mongoose 3.7, $geoWithin is always used for queries. To change this behavior, see Query.use$geoWithin.
In Mongoose 3.7, within changed from a getter to a function. If you need the old syntax, use this.
Sets the 3 write concern parameters for this query:
This option is only valid for operations that write to the database:
Defaults to the schema's writeConcern option
If w > 1, the maximum amount of time to
wait for this write to propagate through the replica set before this
operation fails. The default is 0, which means no timeout.
This option is only valid for operations that write to the database:
Defaults to the schema's writeConcern.wtimeout option
Returns an asyncIterator for use with for/await/of loops
This function only works for find() queries.
You do not need to call this function explicitly, the JavaScript runtime
will call it for you.
Node.js 10.x supports async iterators natively without any flags. You can
enable async iterators in Node.js 8.x using the --harmony_async_iteration flag.
Note: This function is not if Symbol.asyncIterator is undefined. If
Symbol.asyncIterator is undefined, that means your Node.js version does not
support async iterators.
Returns a string representation of this query.
More about toString() in JavaScript.
Flag to opt out of using $geoWithin.
MongoDB 2.4 deprecated the use of $within, replacing it with $geoWithin. Mongoose uses $geoWithin by default (which is 100% backward compatible with $within). If you are running an older version of MongoDB, set this flag to false so your within() queries continue to work.
Failed to fetch https://http://www.mongodb.com/docs/manual/applications/replication/#replica-set-read-preference: HTTPSConnectionPool(host='http', port=443): Max retries exceeded with url: //www.mongodb.com/docs/manual/applications/replication/ (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f20e97f38b0>: Failed to resolve 'http' ([Errno -2] Name or service not known)"))
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          

        The Official MongoDB Node.js Driver
      
The official MongoDB driver for Node.js.
Upgrading to version 6? Take a look at our upgrade guide here!
Think you’ve found a bug? Want to see a new feature in node-mongodb-native? Please open a
case in our issue management tool, JIRA:
Bug reports in JIRA for all driver projects (i.e. NODE, PYTHON, CSHARP, JAVA) and the
Core Server (i.e. SERVER) project are public.
For issues with, questions about, or feedback for the Node.js driver, please look into our support channels. Please do not email any of the driver developers directly with issues or questions - you're more likely to get an answer on the MongoDB Community Forums.
Change history can be found in HISTORY.md.
For server and runtime version compatibility matrices, please refer to the following links:
The following table describes add-on component version compatibility for the Node.js driver. Only packages with versions in these supported ranges are stable when used in combination.
We recommend using the latest version of typescript, however we currently ensure the driver's public types compile against typescript@4.1.6.
This is the lowest typescript version guaranteed to work with our driver: older versions may or may not work - use at your own risk.
Since typescript does not restrict breaking changes to major versions we consider this support best effort.
If you run into any unexpected compiler failures against our supported TypeScript versions please let us know by filing an issue on our JIRA.
The recommended way to get started using the Node.js 5.x driver is by using the npm (Node Package Manager) to install the dependency in your project.
After you've created your own project using npm init, you can run:
This will download the MongoDB driver and add a dependency entry in your package.json file.
If you are a Typescript user, you will need the Node.js type definitions to use the driver's definitions:
The MongoDB driver can optionally be enhanced by the following feature packages:
Maintained by MongoDB:
Some of these packages include native C++ extensions.
Consult the trouble shooting guide here if you run into compilation issues.
Third party:
This guide will show you how to set up a simple application using Node.js and MongoDB. Its scope is only how to set up the driver and perform the simple CRUD operations. For more in-depth coverage, see the official documentation.
First, create a directory where your application will live.
Enter the following command and answer the questions to create the initial structure for your new project:
Next, install the driver as a dependency.
For complete MongoDB installation instructions, see the manual.
You should see the mongod process start up and print some status information.
Create a new app.js file and add the following code to try out some basic CRUD
operations using the MongoDB driver.
Add code to connect to the server and the database myProject:
NOTE: Resolving DNS Connection issues
Node.js 18 changed the default DNS resolution ordering from always prioritizing ipv4 to the ordering
returned by the DNS provider. In some environments, this can result in localhost resolving to
an ipv6 address instead of ipv4 and a consequent failure to connect to the server.
This can be resolved by:
Run your app from the command line with:
The application should print Connected successfully to server to the console.
Add to app.js the following function which uses the insertMany
method to add three documents to the documents collection.
The insertMany command returns an object with information about the insert operations.
Add a query that returns all the documents.
This query returns all the documents in the documents collection.
If you add this below the insertMany example you'll see the document's you've inserted.
Add a query filter to find only documents which meet the query criteria.
Only the documents which match 'a' : 3 should be returned.
The following operation updates a document in the documents collection.
The method updates the first document where the field a is equal to 3 by adding a new field b to the document set to 1. updateResult contains information about whether there was a matching document to update or not.
Remove the document where the field a is equal to 3.
Indexes can improve your application's
performance. The following function creates an index on the a field in the
documents collection.
For more detailed information, see the indexing strategies page.
If you need to filter certain errors from our driver we have a helpful tree of errors described in etc/notes/errors.md.
It is our recommendation to use instanceof checks on errors and to avoid relying on parsing error.message and error.name strings in your code.
We guarantee instanceof checks will pass according to semver guidelines, but errors may be sub-classed or their messages may change at any time, even patch releases, as we see fit to increase the helpfulness of the errors.
Any new errors we add to the driver will directly extend an existing error class and no existing error will be moved to a different parent class outside of a major release.
This means instanceof will always be able to accurately capture the errors that our driver throws.
If you need to test with a change from the latest main branch our mongodb npm package has nightly versions released under the nightly tag.
Nightly versions are published regardless of testing outcome.
This means there could be sematic breakages or partially implemented features.
The nightly build is not suitable for production use.
Apache 2.0
© 2012-present MongoDB Contributors 
© 2009-2012 Christian Amor Kvalheim

        The Official MongoDB Node.js Driver
      
Failed to fetch https://http://www.mongodb.com/docs/manual/applications/replication/#tag-sets: HTTPSConnectionPool(host='http', port=443): Max retries exceeded with url: //www.mongodb.com/docs/manual/applications/replication/ (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f20e97f3700>: Failed to resolve 'http' ([Errno -2] Name or service not known)"))
Docs Home → MongoDB Manual
On this page
Read preference describes how MongoDB clients route read operations to
the members of a replica set.
By default, an application directs its read operations to the
primary member in a replica set (i.e. read preference
mode "primary"). But, clients can specify a read preference to send
read operations to secondaries.
Read preference consists of the read preference mode and optionally, a tag set list, the maxStalenessSeconds option, and the
hedged read option. Hedged read option is available for MongoDB 4.4+ sharded
clusters for reads that use non-primary read preference.
The following table lists a brief summary of the read preference modes:
Starting in version 4.4, non-primary read preference modes
support hedged read on sharded
clusters.
Default mode. All operations read from the current replica set
primary.
Distributed transactions that contain
read operations must use read preference primary. All
operations in a given transaction must route to the same member.
In most situations, operations read from the primary but
if it is unavailable, operations read from secondary
members.
Starting in version 4.4, primaryPreferred supports
hedged reads on sharded clusters.
All operations read from the secondary members of the
replica set.
Starting in version 4.4, secondary supports
hedged reads on sharded clusters.
Operations typically read data from secondary members of the
replica set. If the replica set has only one single primary
member and no other members, operations read data from the primary
member.
Starting in version 4.4, secondaryPreferred supports
hedged reads on sharded clusters.
Operations read from a random eligible replica set
member, irrespective of whether that member is a primary
or secondary, based on a specified latency threshold.
The operation considers the following when calculating latency:
The localThresholdMS connection string option
The maxStalenessSeconds read preference
option
Any specified tag set lists
Starting in version 4.4, nearest supports
hedged reads on sharded clusters
and enables the hedged read option by default.
For detailed description of the read preference modes, see
Read Preference Modes.
Read Preference Tag Set Lists
Read Preference maxStalenessSeconds
Hedged Reads
All read preference modes except primary may return
stale data because secondaries replicate
operations from the primary in an asynchronous process.
[1] Ensure that your application can
tolerate stale data if you choose to use a non-primary
mode.
Read preference does not affect the visibility of data;
i.e. clients can see the results of writes before they are
acknowledged or have propagated to a majority of replica set
members. For details, see
Read Isolation, Consistency, and Recency.
Read preference does not affect
causal consistency. The
causal consistency guarantees provided by
causally consistent sessions for read operations with
"majority" read concern and write      operations with
"majority" write concern hold across all members
of the MongoDB deployment.
All read operations use only the current replica set
primary. [1] This is the default read
mode. If the primary is unavailable, read operations produce an
error or throw an exception.
The primary read preference mode is not compatible with
read preference modes that use tag set lists or maxStalenessSeconds.
If you specify tag set lists or a maxStalenessSeconds value
with primary, the driver will produce an error.
Distributed transactions that contain
read operations must use read preference primary. All
operations in a given transaction must route to the same member.
In most situations, operations read from the primary member
of the set. However, if the primary is unavailable, as is the case
during failover situations, operations read from secondary
members that satisfy the read preference's maxStalenessSeconds and
tag set lists.
When the primaryPreferred read preference includes a maxStalenessSeconds value and there is no primary from which to read,
the client estimates how stale each
secondary is by comparing the secondary's last write
to that of the secondary with the most recent write. The client then directs the read operation to a
secondary whose estimated lag is less than or equal to maxStalenessSeconds.
When the read preference includes a tag set list (an array of tag
sets) and there is no primary from which to read,
the client attempts to find secondary members with matching tags
(trying the tag sets in order until a match is found). If
matching secondaries are found, the client selects a
random secondary from the nearest group of matching
secondaries. If no secondaries have matching tags, the read operation produces an error.
When the read preference includes a maxStalenessSeconds value
and a tag set list, the client filters by staleness first and
then by the specified tags.
Read operations using the primaryPreferred mode may return stale data. Use the
maxStalenessSeconds option to avoid reading from secondaries
that the client estimates are overly stale.
Starting in version 4.4, primaryPreferred supports
hedged reads on sharded clusters.
Operations read only from the secondary members of the set.
If no secondaries are available, then this read operation produces an
error or exception.
Most replica sets have at least one secondary, but there are
situations where there may be no available secondary. For example, a
replica set with a primary, a secondary, and an
arbiter may not have any secondaries if a member is in
recovering state or unavailable.
When the secondary read preference includes a maxStalenessSeconds value,
the client estimates how stale each
secondary is by comparing the secondary's last write
to that of the primary. The client then directs the read operation to a
secondary whose estimated lag is less than or equal to maxStalenessSeconds.
If there is no primary, the client uses the secondary with the most
recent write for the comparison.
When the read preference includes a tag set list (an array of tag
sets),
the client attempts to find secondary members with matching tags
(trying the tag sets in order until a match is found). If
matching secondaries are found, the client selects a
random secondary from the nearest group of matching
secondaries. If no secondaries have matching tags, the read operation produces an error.
When the read preference includes a maxStalenessSeconds value
and a tag set list, the client filters by staleness first and
then by the specified tags.
Read operations using the secondary mode may return stale data. Use the
maxStalenessSeconds option to avoid reading from secondaries
that the client estimates are overly stale.
Starting in version 4.4, secondary supports
hedged reads on sharded clusters.
Operations typically read data from secondary members of the
replica set. If the replica set has only one single primary
member and no other members, operations read data from the primary
member.
When the secondaryPreferred read preference includes a maxStalenessSeconds value,
the client estimates how stale each
secondary is by comparing the secondary's last write
to that of the primary. The client then directs the read operation to a
secondary whose estimated lag is less than or equal to maxStalenessSeconds.
If there is no primary, the client uses the secondary with the most
recent write for the comparison. If there are no secondaries with
estimated lag less than or equal to maxStalenessSeconds, the
client directs the read operation to the replica set's primary.
When the read preference includes a tag set list (an array of tag
sets),
the client attempts to find secondary members with matching tags
(trying the tag sets in order until a match is found). If
matching secondaries are found, the client selects a
random secondary from the nearest group of matching
secondaries. If no secondaries have matching tags, the client ignores tags and reads from the primary.
When the read preference includes a maxStalenessSeconds value
and a tag set list, the client filters by staleness first and
then by the specified tags.
Read operations using the secondaryPreferred mode may return stale data. Use the
maxStalenessSeconds option to avoid reading from secondaries
that the client estimates are overly stale.
Starting in version 4.4, secondaryPreferred supports
hedged reads on sharded clusters.
The driver reads from a member whose network latency falls within
the acceptable latency window. Reads in the nearest mode
do not consider whether a member is a primary or
secondary when routing read operations: primaries and
secondaries are treated equivalently.
Set this mode to minimize the effect of network latency
on read operations without preference for current or stale data.
When the read preference includes a maxStalenessSeconds value, the client estimates
how stale each secondary is by comparing the secondary's last write
to that of the primary, if available, or to the secondary with the
most recent write if there is no primary. The client will then
filter out any secondary whose estimated lag is greater than
maxStalenessSeconds and randomly direct the read to a remaining
member (primary or secondary) whose network latency falls within the
acceptable latency window.
If you specify a tag set list, the client attempts to
find a replica set member that matches the specified tag set lists and
directs reads to an arbitrary member from among the nearest
group.
When the read preference includes a maxStalenessSeconds value
and a tag set list, the client filters by staleness first and
then by the specified tags. From the remaining mongod instances, the client then
randomly directs the read to an instance that falls within the
acceptable latency window. The read preference member
selection
documentation describes the process in detail.
Read operations using the nearest mode may return stale data. Use the
maxStalenessSeconds option to avoid reading from secondaries
that the client estimates are overly stale.
Starting in version 4.4, read preference nearest, by
default, specifies the use of hedged reads for reads on a sharded cluster.
To learn about use cases for specific read preference settings,
see Read Preference Use Cases.
When using a MongoDB driver, you can specify the read preference using
the driver's read preference API. See the driver API
documentation. You can also set the read preference (except
for the hedged read option) when connecting to the replica set or
sharded cluster. For an example, see
connection string.
For a given read preference, the MongoDB drivers use the same
member selection logic.
When using mongosh, see
cursor.readPref() and Mongo.setReadPref().
Distributed transactions that contain
read operations must use read preference primary. All
operations in a given transaction must route to the same member.
For aggregation pipeline
operations that include the $out or $merge
stages, the pipeline runs on the primary member regardless of
read preference setting.
For mapReduce operations, only "inline"
mapReduce operations that do not write data support read
preference. Otherwise, mapReduce operations run on the
primary member.
On this page
About
Support
Social
Docs Home → MongoDB Manual
On this page
Read preference describes how MongoDB clients route read operations to
the members of a replica set.
By default, an application directs its read operations to the
primary member in a replica set (i.e. read preference
mode "primary"). But, clients can specify a read preference to send
read operations to secondaries.
Read preference consists of the read preference mode and optionally, a tag set list, the maxStalenessSeconds option, and the
hedged read option. Hedged read option is available for MongoDB 4.4+ sharded
clusters for reads that use non-primary read preference.
The following table lists a brief summary of the read preference modes:
Starting in version 4.4, non-primary read preference modes
support hedged read on sharded
clusters.
Default mode. All operations read from the current replica set
primary.
Distributed transactions that contain
read operations must use read preference primary. All
operations in a given transaction must route to the same member.
In most situations, operations read from the primary but
if it is unavailable, operations read from secondary
members.
Starting in version 4.4, primaryPreferred supports
hedged reads on sharded clusters.
All operations read from the secondary members of the
replica set.
Starting in version 4.4, secondary supports
hedged reads on sharded clusters.
Operations typically read data from secondary members of the
replica set. If the replica set has only one single primary
member and no other members, operations read data from the primary
member.
Starting in version 4.4, secondaryPreferred supports
hedged reads on sharded clusters.
Operations read from a random eligible replica set
member, irrespective of whether that member is a primary
or secondary, based on a specified latency threshold.
The operation considers the following when calculating latency:
The localThresholdMS connection string option
The maxStalenessSeconds read preference
option
Any specified tag set lists
Starting in version 4.4, nearest supports
hedged reads on sharded clusters
and enables the hedged read option by default.
For detailed description of the read preference modes, see
Read Preference Modes.
Read Preference Tag Set Lists
Read Preference maxStalenessSeconds
Hedged Reads
All read preference modes except primary may return
stale data because secondaries replicate
operations from the primary in an asynchronous process.
[1] Ensure that your application can
tolerate stale data if you choose to use a non-primary
mode.
Read preference does not affect the visibility of data;
i.e. clients can see the results of writes before they are
acknowledged or have propagated to a majority of replica set
members. For details, see
Read Isolation, Consistency, and Recency.
Read preference does not affect
causal consistency. The
causal consistency guarantees provided by
causally consistent sessions for read operations with
"majority" read concern and write      operations with
"majority" write concern hold across all members
of the MongoDB deployment.
All read operations use only the current replica set
primary. [1] This is the default read
mode. If the primary is unavailable, read operations produce an
error or throw an exception.
The primary read preference mode is not compatible with
read preference modes that use tag set lists or maxStalenessSeconds.
If you specify tag set lists or a maxStalenessSeconds value
with primary, the driver will produce an error.
Distributed transactions that contain
read operations must use read preference primary. All
operations in a given transaction must route to the same member.
In most situations, operations read from the primary member
of the set. However, if the primary is unavailable, as is the case
during failover situations, operations read from secondary
members that satisfy the read preference's maxStalenessSeconds and
tag set lists.
When the primaryPreferred read preference includes a maxStalenessSeconds value and there is no primary from which to read,
the client estimates how stale each
secondary is by comparing the secondary's last write
to that of the secondary with the most recent write. The client then directs the read operation to a
secondary whose estimated lag is less than or equal to maxStalenessSeconds.
When the read preference includes a tag set list (an array of tag
sets) and there is no primary from which to read,
the client attempts to find secondary members with matching tags
(trying the tag sets in order until a match is found). If
matching secondaries are found, the client selects a
random secondary from the nearest group of matching
secondaries. If no secondaries have matching tags, the read operation produces an error.
When the read preference includes a maxStalenessSeconds value
and a tag set list, the client filters by staleness first and
then by the specified tags.
Read operations using the primaryPreferred mode may return stale data. Use the
maxStalenessSeconds option to avoid reading from secondaries
that the client estimates are overly stale.
Starting in version 4.4, primaryPreferred supports
hedged reads on sharded clusters.
Operations read only from the secondary members of the set.
If no secondaries are available, then this read operation produces an
error or exception.
Most replica sets have at least one secondary, but there are
situations where there may be no available secondary. For example, a
replica set with a primary, a secondary, and an
arbiter may not have any secondaries if a member is in
recovering state or unavailable.
When the secondary read preference includes a maxStalenessSeconds value,
the client estimates how stale each
secondary is by comparing the secondary's last write
to that of the primary. The client then directs the read operation to a
secondary whose estimated lag is less than or equal to maxStalenessSeconds.
If there is no primary, the client uses the secondary with the most
recent write for the comparison.
When the read preference includes a tag set list (an array of tag
sets),
the client attempts to find secondary members with matching tags
(trying the tag sets in order until a match is found). If
matching secondaries are found, the client selects a
random secondary from the nearest group of matching
secondaries. If no secondaries have matching tags, the read operation produces an error.
When the read preference includes a maxStalenessSeconds value
and a tag set list, the client filters by staleness first and
then by the specified tags.
Read operations using the secondary mode may return stale data. Use the
maxStalenessSeconds option to avoid reading from secondaries
that the client estimates are overly stale.
Starting in version 4.4, secondary supports
hedged reads on sharded clusters.
Operations typically read data from secondary members of the
replica set. If the replica set has only one single primary
member and no other members, operations read data from the primary
member.
When the secondaryPreferred read preference includes a maxStalenessSeconds value,
the client estimates how stale each
secondary is by comparing the secondary's last write
to that of the primary. The client then directs the read operation to a
secondary whose estimated lag is less than or equal to maxStalenessSeconds.
If there is no primary, the client uses the secondary with the most
recent write for the comparison. If there are no secondaries with
estimated lag less than or equal to maxStalenessSeconds, the
client directs the read operation to the replica set's primary.
When the read preference includes a tag set list (an array of tag
sets),
the client attempts to find secondary members with matching tags
(trying the tag sets in order until a match is found). If
matching secondaries are found, the client selects a
random secondary from the nearest group of matching
secondaries. If no secondaries have matching tags, the client ignores tags and reads from the primary.
When the read preference includes a maxStalenessSeconds value
and a tag set list, the client filters by staleness first and
then by the specified tags.
Read operations using the secondaryPreferred mode may return stale data. Use the
maxStalenessSeconds option to avoid reading from secondaries
that the client estimates are overly stale.
Starting in version 4.4, secondaryPreferred supports
hedged reads on sharded clusters.
The driver reads from a member whose network latency falls within
the acceptable latency window. Reads in the nearest mode
do not consider whether a member is a primary or
secondary when routing read operations: primaries and
secondaries are treated equivalently.
Set this mode to minimize the effect of network latency
on read operations without preference for current or stale data.
When the read preference includes a maxStalenessSeconds value, the client estimates
how stale each secondary is by comparing the secondary's last write
to that of the primary, if available, or to the secondary with the
most recent write if there is no primary. The client will then
filter out any secondary whose estimated lag is greater than
maxStalenessSeconds and randomly direct the read to a remaining
member (primary or secondary) whose network latency falls within the
acceptable latency window.
If you specify a tag set list, the client attempts to
find a replica set member that matches the specified tag set lists and
directs reads to an arbitrary member from among the nearest
group.
When the read preference includes a maxStalenessSeconds value
and a tag set list, the client filters by staleness first and
then by the specified tags. From the remaining mongod instances, the client then
randomly directs the read to an instance that falls within the
acceptable latency window. The read preference member
selection
documentation describes the process in detail.
Read operations using the nearest mode may return stale data. Use the
maxStalenessSeconds option to avoid reading from secondaries
that the client estimates are overly stale.
Starting in version 4.4, read preference nearest, by
default, specifies the use of hedged reads for reads on a sharded cluster.
To learn about use cases for specific read preference settings,
see Read Preference Use Cases.
When using a MongoDB driver, you can specify the read preference using
the driver's read preference API. See the driver API
documentation. You can also set the read preference (except
for the hedged read option) when connecting to the replica set or
sharded cluster. For an example, see
connection string.
For a given read preference, the MongoDB drivers use the same
member selection logic.
When using mongosh, see
cursor.readPref() and Mongo.setReadPref().
Distributed transactions that contain
read operations must use read preference primary. All
operations in a given transaction must route to the same member.
For aggregation pipeline
operations that include the $out or $merge
stages, the pipeline runs on the primary member regardless of
read preference setting.
For mapReduce operations, only "inline"
mapReduce operations that do not write data support read
preference. Otherwise, mapReduce operations run on the
primary member.
On this page
About
Support
Social
Docs Home → MongoDB Manual
On this page
Write concern describes the level of acknowledgment requested from
MongoDB for write operations to a standalone mongod,
replica sets, or sharded clusters. In sharded clusters, mongos
instances will pass the write concern on to the shards.
For multi-document transactions, you set
the write concern at the transaction level, not at the individual
operation level. Do not explicitly set the write concern for
individual write operations in a transaction.
If you specify a "majority" write concern for a
multi-document transaction and the
transaction fails to replicate to the calculated majority of replica set members, then the
transaction may not immediately roll back on replica set members.
The replica set will be eventually consistent. A transaction is always applied or rolled back on all
replica set members.
Starting in MongoDB 4.4, replica sets and sharded clusters support
setting a global default write concern. Operations which do not
specify an explicit write concern inherit the global default
write concern settings. See setDefaultRWConcern for
more information.
To learn more about setting the write concern for deployments
hosted in MongoDB Atlas, see
Build a Resilient Application with MongoDB Atlas
Write concern can include the following fields:
the w option to request acknowledgment that the write
operation has propagated to a specified number of mongod
instances or to mongod instances with specified tags.
the j option to request acknowledgment that the write
operation has been written to the on-disk journal, and
the wtimeout option to specify a time limit to
prevent write operations from blocking indefinitely.
The w option requests acknowledgment that the write operation has
propagated to a specified number of mongod instances or
to mongod instances with specified tags. If the write
concern is missing the w field, MongoDB sets the w option to the
default write concern.
If you use the setDefaultRWConcern to set the default
write concern, you must specify a w field value.
Using the w option, the following w: <value> write concerns are
available:
Requests acknowledgment that write operations have been durably committed
to the calculated majority of
the data-bearing voting members (i.e. primary and secondaries
with members[n].votes greater than 0).
{ w: "majority" } is the default write concern for most MongoDB
deployments. See Implicit Default Write Concern.
For example, consider a replica set with 3 voting members,
Primary-Secondary-Secondary (P-S-S). For this replica set,
calculated majority is two, and
the write must propagate to the primary and one secondary to
acknowledge the write concern to the client.
Hidden,
delayed,
and priority 0
members with members[n].votes greater than 0
can acknowledge "majority" write operations.
Delayed secondaries can return write acknowledgment no earlier
than the configured secondaryDelaySecs.
After the write operation returns with a w:
"majority" acknowledgment to the client, the
client can read the result of that write with a
"majority" readConcern.
If you specify a "majority" write concern for a
multi-document transaction and the
transaction fails to replicate to the calculated majority of replica set members, then the
transaction may not immediately roll back on replica set members.
The replica set will be eventually consistent. A transaction is always applied or rolled back on all
replica set members.
See Acknowledgment Behavior for when mongod instances
acknowledge the write.
Requests acknowledgment that the write operation has propagated
to the specified number of mongod instances. For
example:
Requests acknowledgment that the write operation has
propagated to the standalone mongod or the primary
in a replica set. Data can be rolled back
if the primary steps down before the write operations have
replicated to any of the secondaries.
In MongoDB 4.4 and later, if write operations use
{ w: 1 } write concern, the rollback
directory may exclude writes submitted after an oplog hole
if the primary restarts before the write operation completes.
Requests no acknowledgment of the write operation. However, w:
0 may return information about socket exceptions and
networking errors to the application. Data can be
rolled back if the primary steps down
before the write operations have replicated to any of the
secondaries.
If you specify w: 0 but include j: true, the
j: true prevails to request acknowledgment from
the standalone mongod or the primary of a replica
set.
w greater than 1 requires acknowledgment from the primary
and as many data-bearing secondaries as needed to meet the
specified write concern. The secondaries do not need to be
voting members to meet the write concern threshold.
For example, consider a 3-member replica set with a primary and 2
secondaries. Specifying w: 2 would require acknowledgment from the
primary and one of the secondaries. Specifying w: 3 would require
acknowledgment from the primary and both secondaries.
Hidden,
delayed,
and priority 0
members can acknowledge
w: <number> write operations.
Delayed secondaries can return write acknowledgment no earlier
than the configured secondaryDelaySecs.
See Acknowledgment Behavior for when mongod instances
acknowledge the write.
Requests acknowledgment that the write operations have
propagated to tagged members that
satisfy the custom write concern defined in
settings.getLastErrorModes. For an example, see
Custom Multi-Datacenter Write Concerns.
Data can be rolled back if the custom
write concern only requires acknowledgment from the primary and
the primary steps down before the write operations have
replicated to any of the secondaries.
See Acknowledgment Behavior for when mongod
instances acknowledge the write.
Default MongoDB Read Concerns/Write Concerns
Replica Set Protocol Version
The j option requests acknowledgment from MongoDB that
the write operation has been written to the on-disk journal.
If j: true, requests acknowledgment that the
mongod instances, as specified in the w:
<value>, have written to the on-disk journal. j:
true does not by itself guarantee that the write will not be
rolled back due to replica set primary failover.
Changed in version 3.2: With j: true, MongoDB returns only after the
requested number of members, including the primary, have written to the
journal. Previously j: true write concern in a
replica set only requires the primary to write to the journal,
regardless of the w: <value> write concern.
Specifying a write concern that includes j: true to a
mongod instance that is running without journaling
produces an error.
If journaling is enabled, w:
"majority" may imply j: true. The
writeConcernMajorityJournalDefault replica set
configuration setting determines the behavior. See
Acknowledgment Behavior for details.
A write concern that includes or implies j: true causes an
immediate journal synchronization. See Journaling Process.
This option specifies a time limit, in milliseconds, for the write
concern. wtimeout is only applicable for w values greater than
1.
wtimeout causes write operations to return with an error
after the specified limit, even if the required write concern will
eventually succeed. When these write operations return,
MongoDB does not undo successful data modifications performed
before the write concern exceeded the wtimeout time limit.
If you do not specify the wtimeout option and the level of write
concern is unachievable, the write operation will block indefinitely.
Specifying a wtimeout value of 0 is equivalent to a write
concern without the wtimeout option.
Starting in MongoDB 5.0, the implicit default
write concern is
w: majority. However, special
considerations are made for deployments containing
arbiters:
The voting majority of a replica set is 1 plus half the number of
voting members, rounded down. If the number of data-bearing voting
members is not greater than the voting majority, the default write
concern is { w: 1 }.
In all other scenarios, the default write concern is { w:
"majority" }.
Specifically, MongoDB uses the following formula to determine the
default write concern:
For example, consider the following deployments and their respective
default write concerns:
In the first example:
There are 2 non-arbiters and 1 arbiter for a total of 3 voting
nodes.
The majority of voting nodes (1 plus half of 3, rounded
down) is 2.
The number of non-arbiters (2) is equal to
the majority of voting nodes (2), resulting in an implicit write
concern of { w: 1 }.
In the second example:
There are 4 non-arbiters and 1 arbiter for a total of 5
voting nodes.
The majority of voting nodes (1 plus half of 5, rounded
down) is 3.
The number of non-arbiters (4) is greater than the majority
of voting nodes (3), resulting in an implicit write concern of
{ w: "majority" }.
The w option and the j option determine
when mongod instances acknowledge write operations.
A standalone mongod acknowledges a write operation either
after applying the write in memory or after writing to the on-disk
journal. The following table lists the acknowledgment behavior for a
standalone and the relevant write concerns:
With writeConcernMajorityJournalDefault set to false,
MongoDB does not wait for w: "majority"
writes to be written to the on-disk journal before acknowledging the
writes. As such, "majority" write operations could
possibly roll back in the event of a transient loss (e.g. crash and
restart) of a majority of nodes in a given replica set.
The value specified to w determines the number
of replica set members that must acknowledge the write before returning
success. For each eligible replica set member, the j
option determines whether the member acknowledges writes after applying
the write operation in memory or after writing to the on-disk journal.
Any data-bearing voting member of the replica set can contribute
to write acknowledgment of "majority" write
operations.
The following table lists when the member can acknowledge
the write based on the j value:
Acknowledgment depends on the value of
writeConcernMajorityJournalDefault:
If true, acknowledgment requires writing operation to
on-disk journal (j: true).
writeConcernMajorityJournalDefault defaults to
true
If false, acknowledgment requires writing operation in
memory (j: false).
For behavior details, see w: "majority" Behavior.
Any data-bearing member of the replica set can contribute
to write acknowledgment of w: <number> write
operations.
The following table lists when the member can acknowledge
the write based on the j value:
Hidden,
delayed,
and priority 0
members can acknowledge
w: <number> write operations.
Delayed secondaries can return write acknowledgment no earlier
than the configured secondaryDelaySecs.
With causally consistent client sessions, the
client sessions only guarantee causal consistency if:
the associated read operations use "majority" read
concern, and
the associated write operations use "majority"
write concern.
For details, see Causal Consistency.
With writeConcernMajorityJournalDefault set to false,
MongoDB does not wait for w: "majority"
writes to be written to the on-disk journal before acknowledging the
writes. As such, "majority" write operations could
possibly roll back in the event of a transient loss (e.g. crash and
restart) of a majority of nodes in a given replica set.
Hidden,
delayed,
and priority 0
members with members[n].votes greater than 0
can acknowledge "majority" write operations.
Delayed secondaries can return write acknowledgment no earlier
than the configured secondaryDelaySecs.
Starting in MongoDB 5.0, replica set members in the
STARTUP2 state do not participate in write majorities.
The local database does not support
write concerns. MongoDB silently ignores any configured write
concern for an operation on a collection in the local database.
Starting in version 4.2.1, the rs.status() returns the
writeMajorityCount field which contains
the calculated majority number.
The majority for write concern "majority" is calculated
as the smaller of the following values:
the majority of all voting members (including arbiters) vs.
the number of all data-bearing voting members.
In cases where the calculated majority number is equal to the number
of all data-bearing voting members (such as with a 3-member
Primary-Secondary-Arbiter deployment), write concern
"majority" may time out or never be acknowledged if
a data bearing voting member is down or unreachable. If possible,
use a data-bearing voting member instead of an arbiter.
For example, consider:
A replica set with 3 voting members, Primary-Secondary-Secondary
(P-S-S):
The majority of all voting members is 2.
The number of all data-bearing voting members is 3.
A replica set with 3 voting members, Primary-Secondary-Arbiter (P-S-A)
The majority of all voting members is 2.
The number of all data-bearing voting members is 2.
Avoid using a "majority" write concern with a
(P-S-A) or other topologies that require all data-bearing voting
members to be available to acknowledge the writes. Customers who
want the durability guarantees of using a
"majority" write concern should instead deploy a
topology that does not require all data bearing voting members to be
available (e.g. P-S-S).
Avoid deploying more than one arbiter in a replica set. See Concerns with Multiple Arbiters.
To add an arbiter to an existing replica set:
Typically, if there are two or fewer data-bearing members in the
replica set, you might need to first set the cluster wide write
concern for the replica set.
See cluster wide write concern for more information on why you
might need to set the cluster wide write concern.
You do not need to change the cluster wide write concern before starting
a new replica set with an arbiter.
Default write concern formula
Starting in version 4.4, MongoDB tracks write concern provenance,
which indicates the source of a particular write concern. You may see
provenance shown in the
getLastError metrics, write
concern error objects, and MongoDB logs.
The following table shows the possible write concern provenance
values and their significance:
There are important differences between commit quorums and write concerns:
Index builds use commit quorums.
Write operations use write concerns.
Each data-bearing node in a cluster is a voting member.
The commit quorum specifies how many data-bearing voting members, or
which voting members, including the primary, must be prepared to commit
a simultaneous index build.
before the primary will execute the commit.
The write concern is the level of acknowledgment that the write has
propagated to the specified number of instances.
The commit quorum specifies how many nodes must be ready to finish
the index build before the primary commits the index build. In contrast,
when the primary has committed the index build, the write concern
specifies how many nodes must finish the index build before the
command returns.
On this page
About
Support
Social
Docs Home → MongoDB Manual
On this page
Sharding is a method for distributing data across multiple
machines. MongoDB uses sharding to support deployments with very large data
sets and high throughput operations.
Database systems with large data sets or high throughput applications can
challenge the capacity of a single server. For example, high query rates can
exhaust the CPU capacity of the server. Working set sizes larger than the
system's RAM stress the I/O capacity of disk drives.
There are two methods for addressing system growth: vertical and horizontal
scaling.
Vertical Scaling involves increasing the capacity of a single server, such
as using a more powerful CPU, adding more RAM, or increasing the amount of
storage space. Limitations in available technology may restrict a single
machine from being sufficiently powerful for a given workload. Additionally,
Cloud-based providers have hard ceilings based on available hardware
configurations. As a result, there is a practical maximum for vertical scaling.
Horizontal Scaling involves dividing the system dataset and load over
multiple servers, adding additional servers to increase capacity as required.
While the overall speed or capacity of a single machine may not be high, each
machine handles a subset of the overall workload, potentially providing better
efficiency than a single high-speed high-capacity server. Expanding the
capacity of the deployment only requires adding additional servers as needed,
which can be a lower overall cost than high-end hardware for a single machine.
The trade off is increased complexity in infrastructure and maintenance for
the deployment.
MongoDB supports horizontal scaling through sharding.
You can shard collections in the UI for deployments hosted in MongoDB Atlas.
A MongoDB sharded cluster consists of the following components:
shard: Each shard contains a
subset of the sharded data. Each shard can be deployed as a replica set.
mongos: The mongos acts as a
query router, providing an interface between client applications and
the sharded cluster. Starting in MongoDB 4.4, mongos can support
hedged reads to minimize latencies.
config servers: Config
servers store metadata and configuration settings for the cluster.
The following graphic describes the interaction of components within a
sharded cluster:
MongoDB shards data at the collection level, distributing the
collection data across the shards in the cluster.
MongoDB uses the shard key to
distribute the collection's documents across shards. The shard key
consists of a field or multiple fields in the documents.
Starting in version 4.4, documents in sharded collections can be
missing the shard key fields. Missing shard key fields are treated as
having null values when distributing the documents across shards but
not when routing queries. For more information, see
Missing Shard Key Fields.
In version 4.2 and earlier, shard key fields must exist in every
document for a sharded collection.
You select the shard key when sharding a collection.
Starting in MongoDB 5.0, you can reshard a collection by changing a collection's shard key.
Starting in MongoDB 4.4, you can refine a shard key by adding a suffix field or fields to the existing
shard key.
In MongoDB 4.2 and earlier, the choice of shard key cannot
be changed after sharding.
A document's shard key value determines its distribution across the
shards.
Starting in MongoDB 4.2, you can update a document's shard key value
unless your shard key field is the immutable _id field. See
Change a Document's Shard Key Value for more information.
In MongoDB 4.0 and earlier, a document's shard key field value is
immutable.
To shard a populated collection, the collection must have an
index that starts with the shard key. When sharding an empty
collection, MongoDB creates the supporting index if the collection does
not already have an appropriate index for the specified shard key. See
Shard Key Indexes.
The choice of shard key affects the performance, efficiency, and scalability
of a sharded cluster. A cluster with the best possible hardware and
infrastructure can be bottlenecked by the choice of shard key. The choice of
shard key and its backing index can also affect the sharding strategy that your cluster can use.
Choose a Shard Key
MongoDB partitions sharded data into chunks. Each
chunk has an inclusive lower and exclusive upper range based on the
shard key.
In an attempt to achieve an even distribution of data across all
shards in the cluster, a balancer runs in the
background to migrate ranges across the shards.
Range Migration
MongoDB distributes the read and write workload across the
shards in the sharded cluster, allowing each shard to
process a subset of cluster operations. Both read and write workloads can be
scaled horizontally across the cluster by adding more shards.
For queries that include the shard key or the prefix of a compound shard key, mongos can target the query at a
specific shard or set of shards. These targeted
operations are generally more efficient than
broadcasting to every shard in the cluster.
Starting in MongoDB 4.4, mongos can support hedged
reads to minimize latencies.
Sharding distributes data across the shards in the
cluster, allowing each shard to contain a subset of the total cluster data. As
the data set grows, additional shards increase the storage capacity of the
cluster.
The deployment of config servers and shards as replica sets provide
increased availability.
Even if one or more shard replica sets become completely unavailable,
the sharded cluster can continue to perform partial reads and writes.
That is, while data on the unavailable shard(s) cannot be accessed,
reads or writes directed at the available shards can still succeed.
Sharded cluster infrastructure requirements and complexity require
careful planning, execution, and maintenance.
Once a collection has been sharded, MongoDB provides no method to
unshard a sharded collection.
While you can reshard your collection
later, it is important to carefully consider your shard key choice to
avoid scalability and perfomance issues.
Choose a Shard Key
To understand the operational requirements and restrictions for sharding
your collection, see Operational Restrictions in Sharded Clusters.
If queries do not include the shard key or the prefix of a
compound shard key, mongos performs
a broadcast operation, querying
all shards in the sharded cluster. These scatter/gather queries can
be long running operations.
Starting in MongoDB 5.1, when starting, restarting or adding a
shard server with sh.addShard() the
Cluster Wide Write Concern (CWWC)
must be set.
If the CWWC is not set and the shard is configured
such that the default write concern is
{ w : 1 } the shard server will fail to start or be added
and returns an error.
See default write concern calculations for
details on how the default write concern is calculated.
If you have an active support contract with MongoDB, consider contacting
your account representative for assistance with sharded cluster
planning and deployment.
A database can have a mixture of sharded and unsharded collections. Sharded
collections are partitioned and distributed across the
shards in the cluster. Unsharded collections are stored on a
primary shard. Each database has its own primary shard.
You must connect to a mongos router to interact with any collection in
the sharded cluster. This includes sharded and unsharded
collections. Clients should never connect to a single shard in order to
perform read or write operations.
You can connect to a mongos the same way you connect to a
mongod using the mongosh or a
MongoDB driver.
MongoDB supports two sharding strategies for distributing data
across sharded clusters.
Hashed Sharding involves computing a hash of the shard key field's
value. Each chunk is then assigned a range based on the
hashed shard key values.
MongoDB automatically computes the hashes when resolving queries using
hashed indexes.  Applications do not need to compute hashes.
While a range of shard keys may be "close", their hashed values are unlikely
to be on the same chunk. Data distribution based on hashed values
facilitates more even data distribution, especially in data sets where the
shard key changes monotonically.
However, hashed distribution means that range-based queries on the shard key
are less likely to target a single shard, resulting in more cluster wide
broadcast operations
See Hashed Sharding for more information.
Ranged sharding involves dividing data into ranges based on the
shard key values. Each chunk is then assigned a range based on the
shard key values.
A range of shard keys whose values are "close" are more likely to reside on
the same chunk. This allows for targeted
operations as a mongos can route the
operations to only the shards that contain the required data.
The efficiency of ranged sharding depends on the shard key chosen. Poorly
considered shard keys can result in uneven distribution of data, which can
negate some benefits of sharding or can cause performance bottlenecks. See
shard key selection for range-based sharding.
See Ranged Sharding for more information.
Zones can help improve the locality of data for sharded clusters that
span multiple data centers.
In sharded clusters, you can create zones of sharded data
based on the shard key. You can associate each zone with one or
more shards in the cluster. A shard can associate with any number of
zones. In a balanced cluster, MongoDB migrates chunks
covered by a zone only to those shards associated with the zone.
Each zone covers one or more ranges of shard key values. Each range a
zone covers is always inclusive of its lower boundary and exclusive of its
upper boundary.
You must use fields contained in the shard key when defining a new
range for a zone to cover. If using a compound shard
key, the range must include the prefix of the shard key. See shard keys
in zones for more information.
The possible use of zones in the future should be taken into
consideration when choosing a shard key.
Starting in MongoDB 4.0.3, setting up zones and zone ranges before
you shard an empty or a non-existing collection allows for a faster
setup of zoned sharding.
See zones for more information.
Use the shardCollection command with the collation :
{ locale : "simple" } option to shard a collection which has a
default collation. Successful
sharding requires that:
The collection must have an index whose prefix is the shard key
The index must have the collation { locale: "simple" }
When creating new collections with a collation, ensure these conditions
are met prior to sharding the collection.
Queries on the sharded collection continue to use the default
collation configured for the collection. To use the shard key
index's simple collation, specify {locale : "simple"}
in the query's collation document.
See shardCollection for more information about sharding
and collation.
Starting in MongoDB 3.6, change streams are
available for replica sets and sharded clusters. Change streams allow
applications to access real-time data changes without the complexity
and risk of tailing the oplog. Applications can use change streams to
subscribe to all data changes on a collection or collections.
Starting in MongoDB 4.2, with the introduction of distributed
transactions, multi-document transactions are
available on sharded clusters.
Until a transaction commits, the data changes made in the
transaction are not visible outside the transaction.
However, when a transaction writes to multiple shards, not all
outside read operations need to wait for the result of the committed
transaction to be visible across the shards. For example, if a
transaction is committed and write 1 is visible on shard A but write
2 is not yet visible on shard B, an outside read at read concern
"local" can read the results of write 1 without
seeing write 2.
For more information on how sharding works with
aggregations, read the sharding
chapter in the Practical MongoDB Aggregations
e-book.
Transactions
Production Considerations
Production Considerations (Sharded Clusters)
On this page
About
Support
Social
Most Popular Articles
EbooksThe 80/20 Guide to ES2015 Generators
The 80/20 Guide to ES2015 Generators
In JavaScript, the JSON.stringify() function
looks for functions named toJSON in the object being serialized. If
an object has a toJSON function, JSON.stringify() calls toJSON()
and serializes the return value from toJSON() instead.
For example, the below script prints the same thing as JSON.stringify({ answer: 42 }).
The toJSON() function is useful for making sure ES6 classes get
serialized correctly. For example, suppose you have a custom JavaScript
error class.
By default, JavaScript isn't great with serializing errors. The below
script prints {"status":404}, no error message or stack trace.
However, if you add a toJSON() method to your HTTPError class,
you can configure how JavaScript serializes instances of HTTPError.
You can even get fancy and make toJSON() serialize the stack trace
if the NODE_ENV is development.
The neat thing about toJSON() is that JavaScript handles recursion for you,
so it will still correctly serialize deeply nested HTTPError instances
and HTTPError instances in arrays.
Many libraries and frameworks use JSON.stringify() under the hood.
For example, Express' res.json() function and
Axios POST requests convert objects to
JSON using JSON.stringify(). So custom toJSON() functions work with those modules as well.
Many Node.js libraries and frameworks use toJSON() to ensure JSON.stringify()
can serialize complex objects into something meaningful. For example, Moment.js objects have a nice simple toJSON()
function that looks like this:
You can try it yourself by running:
Node.js buffers also
have a toJSON() function.
Mongoose documents also have a toJSON() function that ensures the internal state of Mongoose documents doesn't end up in JSON.stringify() output.
The toJSON() function is an important tool when building classes in JavaScript.
It is how you control how JavaScript serializes your class into JSON. The
toJSON() function can help you solve numerous problems, like making sure
dates or Node.js buffers get serialized in the right format for your app.
Give it a shot next time you write an ES6 class.
Throws an error if a given path is not populated
Returns a copy of this document with a deep clone of _doc and $__.
Hash containing current validation $errors.
Get all subdocs (by bfs)
Gets all populated documents associated with this document.
Don't run validation on this path or persist changes to this path.
Increments the numeric value at path by the given val.
When you call save() on this document, Mongoose will send a
$inc
as opposed to a $set.
Alias for .init
Checks if a path is set to its default.
Getter/setter, determines whether the document was removed or not.
Returns true if the given path is nullish or only contains empty objects.
Useful for determining whether this subdoc will get stripped out by the
minimize option.
Alias of .isModified
Boolean flag specifying if the document is new. If you create a document
using new, this document will be considered "new". $isNew is how
Mongoose determines whether save() should use insertOne() to create
a new document or updateOne() to update an existing document.
On the other hand, if you load an existing document from the database
using findOne() or another query operation,
$isNew will be false.
Mongoose sets $isNew to false immediately after save() succeeds.
That means Mongoose sets $isNew to false before post('save') hooks run.
In post('save') hooks, $isNew will be false if save() succeeded.
For subdocuments, $isNew is true if either the parent has $isNew set,
or if you create a new subdocument.
Empty object that you can use for storing properties on the document. This
is handy for passing data to middleware without conflicting with Mongoose
internals.
Marks a path as valid, removing existing validation errors.
A string containing the current operation that Mongoose is executing
on this document. May be null, 'save', 'validate', or 'remove'.
Alias for parent(). If this document is a subdocument or populated
document, returns the document's parent. Returns undefined otherwise.
Alias of .populated.
Getter/setter around the session associated with this document. Used to
automatically set session if you save() a doc that you got from a
query with an associated session.
If this is a top-level document, setting the session propagates to all child
docs.
Alias for set(), used internally to avoid conflicts
Getter/setter around whether this document will apply timestamps by
default when using save() and bulkSave().
Alias of .validate
Set this property to add additional query filters when Mongoose saves this document and isNew is false.
Takes a populated field and returns it to its unpopulated state.
If the path was not provided, then all populated fields are returned to their unpopulated state.
Returns the list of paths that have been directly modified. A direct
modified path is a path that you explicitly set, whether via doc.foo = 'bar',
Object.assign(doc, { foo: 'bar' }), or doc.set('foo', 'bar').
A path a may be in modifiedPaths() but not in directModifiedPaths()
because a child of a was directly modified.
Returns true if this document is equal to another document.
Documents are considered equal when they have matching _ids, unless neither
document has an _id, in which case this function falls back to using
deepEqual().
Hash containing current validation errors.
Returns the value of a path.
Returns the changes that happened to the document
in the format that will be sent to MongoDB.
Modifying the object that getChanges() returns does not affect the document's
change tracking state. Even if you delete user.getChanges().$set, Mongoose
will still send a $set to the server.
The string version of this documents _id.
This getter exists on all documents by default. The getter can be disabled by setting the id option of its Schema to false at construction time.
Initializes the document without setters or marking anything modified.
Called internally after a document is returned from mongodb. Normally,
you do not need to call this function on your own.
This function triggers init middleware.
Note that init hooks are synchronous.
Helper for console.log
Marks a path as invalid, causing validation to fail.
The errorMsg argument will become the message of the ValidationError.
The value argument (if passed) will be available through the ValidationError.value property.
Returns true if path was directly set and modified, else false.
Checks if path was explicitly selected. If no projection, always returns
true.
Checks if path is in the init state, that is, it was set by Document#init() and not modified since.
Returns true if any of the given paths is modified, else false. If no arguments, returns true if any path
in this document is modified.
If path is given, checks if a path or any full path containing path as part of its path chain has been modified.
Legacy alias for $isNew.
Checks if path was selected in the source query which initialized this document.
Marks the path as having pending changes to write to the db.
Very helpful when using Mixed types.
Returns the list of paths that have been modified.
Overwrite all values in this document with the values of obj, except
for immutable properties. Behaves similarly to set(), except for it
unsets all properties that aren't in obj.
If this document is a subdocument or populated document, returns the document's
parent. Returns the original document if there is no parent.
Populates paths on an existing document.
Gets _id(s) used during population of the given path.
If the path was not populated, returns undefined.
Sends a replaceOne command with this document _id as the query selector.
Saves this document by inserting a new document into the database if document.isNew is true,
or sends an updateOne operation only with the modifications to the database, it does not replace the whole document in the latter case.
If save is successful, the returned promise will fulfill with the document
saved.
The document's schema.
Sets the value of a path, or many paths.
Alias for .$set.
The return value of this method is used in calls to JSON.stringify(doc).
This method accepts the same options as Document#toObject. To apply the options to every document of your schema by default, set your schemas toJSON option to the same argument.
There is one difference between toJSON() and toObject() options.
When you call toJSON(), the flattenMaps option defaults to true, because JSON.stringify() doesn't convert maps to objects by default.
When you call toObject(), the flattenMaps option is false by default.
See schema options for more information on setting toJSON option defaults.
Converts this document into a plain-old JavaScript object (POJO).
Buffers are converted to instances of mongodb.Binary for proper storage.
Example of only applying path getters
Example of only applying virtual getters
Example of applying both path and virtual getters
To apply these options to every document of your schema by default, set your schemas toObject option to the same argument.
We may need to perform a transformation of the resulting object based on some criteria, say to remove some sensitive information or return a custom object. In this case we set the optional transform function.
Transform functions receive three arguments
With transformations we can do a lot more than remove properties. We can even return completely new customized objects:
Note: if a transform function returns undefined, the return value will be ignored.
Transformations may also be applied inline, overridding any transform set in the schema options.
Any transform function specified in toObject options also propagates to any subdocuments.
If you want to skip transformations, use transform: false:
If you pass a transform in toObject() options, Mongoose will apply the transform
to subdocuments in addition to the top-level document.
Similarly, transform: false skips transforms for all subdocuments.
Note that this behavior is different for transforms defined in the schema:
if you define a transform in schema.options.toObject.transform, that transform
will not apply to subdocuments.
Transforms, like all of these options, are also available for toJSON. See this guide to JSON.stringify() to learn why toJSON() and toObject() are separate functions.
See schema options for some more details.
During save, no custom options are applied to the document before being sent to the database.
Helper for console.log
Clears the modified state on the specified path.
Sends an updateOne command with this document _id as the query selector.
Executes registered validation rules for this document.
This method is called pre save and if a validation rule is violated, save is aborted and the error is thrown.
Executes registered validation rules (skipping asynchronous validators) for this document.
This method is useful if you need synchronous validation.
Failed to fetch https://http://www.mongodb.com/docs/manual/reference/geojson/: HTTPSConnectionPool(host='http', port=443): Max retries exceeded with url: //www.mongodb.com/docs/manual/reference/geojson/ (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f20e92dc9a0>: Failed to resolve 'http' ([Errno -2] Name or service not known)"))
Failed to fetch https://http://aaronheckmann.blogspot.com/2012/06/mongoose-v3-part-1-versioning.html: HTTPSConnectionPool(host='http', port=443): Max retries exceeded with url: //aaronheckmann.blogspot.com/2012/06/mongoose-v3-part-1-versioning.html (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f20e9bdf100>: Failed to resolve 'http' ([Errno -2] Name or service not known)"))
Optimistic concurrency control (OCC), also known as optimistic locking, is a non-locking concurrency control method applied to transactional systems such as relational database management systems and software transactional memory. OCC assumes that multiple transactions can frequently complete without interfering with each other. While running, transactions use data resources without acquiring locks on those resources. Before committing, each transaction verifies that no other transaction has modified the data it has read. If the check reveals conflicting modifications, the committing transaction rolls back and can be restarted.[1] Optimistic concurrency control was first proposed in 1979 by H. T. Kung and John T. Robinson.[2]

OCC is generally used in environments with low data contention. When conflicts are rare, transactions can complete without the expense of managing locks and without having transactions wait for other transactions' locks to clear, leading to higher throughput than other concurrency control methods. However, if contention for data resources is frequent, the cost of repeatedly restarting transactions hurts performance significantly, in which case other concurrency control methods may be better suited. However, locking-based ("pessimistic") methods also can deliver poor performance because locking can drastically limit effective concurrency even when deadlocks are avoided.

Optimistic concurrency control transactions involve these phases:[2]

The stateless nature of HTTP makes locking infeasible for web user interfaces. It is common for a user to start editing a record, then leave without following a "cancel" or "logout" link. If locking is used, other users who attempt to edit the same record must wait until the first user's lock times out.

HTTP does provide a form of built-in OCC. The response to an initial GET request can include an ETag for subsequent PUT requests to use in the If-Match header. Any PUT requests with an out-of-date ETag in the If-Match header can then be rejected.[3]

Some database management systems offer OCC natively, without requiring special application code. For others, the application can implement an OCC layer outside of the database, and avoid waiting or silently overwriting records. In such cases, the form may include a hidden field with the record's original content, a timestamp, a sequence number, or an opaque token. On submit, this is compared against the database. If it differs, the conflict resolution algorithm is invoked.

Throws an error if a given path is not populated
Returns a copy of this document with a deep clone of _doc and $__.
Hash containing current validation $errors.
Get all subdocs (by bfs)
Gets all populated documents associated with this document.
Don't run validation on this path or persist changes to this path.
Increments the numeric value at path by the given val.
When you call save() on this document, Mongoose will send a
$inc
as opposed to a $set.
Alias for .init
Checks if a path is set to its default.
Getter/setter, determines whether the document was removed or not.
Returns true if the given path is nullish or only contains empty objects.
Useful for determining whether this subdoc will get stripped out by the
minimize option.
Alias of .isModified
Boolean flag specifying if the document is new. If you create a document
using new, this document will be considered "new". $isNew is how
Mongoose determines whether save() should use insertOne() to create
a new document or updateOne() to update an existing document.
On the other hand, if you load an existing document from the database
using findOne() or another query operation,
$isNew will be false.
Mongoose sets $isNew to false immediately after save() succeeds.
That means Mongoose sets $isNew to false before post('save') hooks run.
In post('save') hooks, $isNew will be false if save() succeeded.
For subdocuments, $isNew is true if either the parent has $isNew set,
or if you create a new subdocument.
Empty object that you can use for storing properties on the document. This
is handy for passing data to middleware without conflicting with Mongoose
internals.
Marks a path as valid, removing existing validation errors.
A string containing the current operation that Mongoose is executing
on this document. May be null, 'save', 'validate', or 'remove'.
Alias for parent(). If this document is a subdocument or populated
document, returns the document's parent. Returns undefined otherwise.
Alias of .populated.
Getter/setter around the session associated with this document. Used to
automatically set session if you save() a doc that you got from a
query with an associated session.
If this is a top-level document, setting the session propagates to all child
docs.
Alias for set(), used internally to avoid conflicts
Getter/setter around whether this document will apply timestamps by
default when using save() and bulkSave().
Alias of .validate
Set this property to add additional query filters when Mongoose saves this document and isNew is false.
Takes a populated field and returns it to its unpopulated state.
If the path was not provided, then all populated fields are returned to their unpopulated state.
Returns the list of paths that have been directly modified. A direct
modified path is a path that you explicitly set, whether via doc.foo = 'bar',
Object.assign(doc, { foo: 'bar' }), or doc.set('foo', 'bar').
A path a may be in modifiedPaths() but not in directModifiedPaths()
because a child of a was directly modified.
Returns true if this document is equal to another document.
Documents are considered equal when they have matching _ids, unless neither
document has an _id, in which case this function falls back to using
deepEqual().
Hash containing current validation errors.
Returns the value of a path.
Returns the changes that happened to the document
in the format that will be sent to MongoDB.
Modifying the object that getChanges() returns does not affect the document's
change tracking state. Even if you delete user.getChanges().$set, Mongoose
will still send a $set to the server.
The string version of this documents _id.
This getter exists on all documents by default. The getter can be disabled by setting the id option of its Schema to false at construction time.
Initializes the document without setters or marking anything modified.
Called internally after a document is returned from mongodb. Normally,
you do not need to call this function on your own.
This function triggers init middleware.
Note that init hooks are synchronous.
Helper for console.log
Marks a path as invalid, causing validation to fail.
The errorMsg argument will become the message of the ValidationError.
The value argument (if passed) will be available through the ValidationError.value property.
Returns true if path was directly set and modified, else false.
Checks if path was explicitly selected. If no projection, always returns
true.
Checks if path is in the init state, that is, it was set by Document#init() and not modified since.
Returns true if any of the given paths is modified, else false. If no arguments, returns true if any path
in this document is modified.
If path is given, checks if a path or any full path containing path as part of its path chain has been modified.
Legacy alias for $isNew.
Checks if path was selected in the source query which initialized this document.
Marks the path as having pending changes to write to the db.
Very helpful when using Mixed types.
Returns the list of paths that have been modified.
Overwrite all values in this document with the values of obj, except
for immutable properties. Behaves similarly to set(), except for it
unsets all properties that aren't in obj.
If this document is a subdocument or populated document, returns the document's
parent. Returns the original document if there is no parent.
Populates paths on an existing document.
Gets _id(s) used during population of the given path.
If the path was not populated, returns undefined.
Sends a replaceOne command with this document _id as the query selector.
Saves this document by inserting a new document into the database if document.isNew is true,
or sends an updateOne operation only with the modifications to the database, it does not replace the whole document in the latter case.
If save is successful, the returned promise will fulfill with the document
saved.
The document's schema.
Sets the value of a path, or many paths.
Alias for .$set.
The return value of this method is used in calls to JSON.stringify(doc).
This method accepts the same options as Document#toObject. To apply the options to every document of your schema by default, set your schemas toJSON option to the same argument.
There is one difference between toJSON() and toObject() options.
When you call toJSON(), the flattenMaps option defaults to true, because JSON.stringify() doesn't convert maps to objects by default.
When you call toObject(), the flattenMaps option is false by default.
See schema options for more information on setting toJSON option defaults.
Converts this document into a plain-old JavaScript object (POJO).
Buffers are converted to instances of mongodb.Binary for proper storage.
Example of only applying path getters
Example of only applying virtual getters
Example of applying both path and virtual getters
To apply these options to every document of your schema by default, set your schemas toObject option to the same argument.
We may need to perform a transformation of the resulting object based on some criteria, say to remove some sensitive information or return a custom object. In this case we set the optional transform function.
Transform functions receive three arguments
With transformations we can do a lot more than remove properties. We can even return completely new customized objects:
Note: if a transform function returns undefined, the return value will be ignored.
Transformations may also be applied inline, overridding any transform set in the schema options.
Any transform function specified in toObject options also propagates to any subdocuments.
If you want to skip transformations, use transform: false:
If you pass a transform in toObject() options, Mongoose will apply the transform
to subdocuments in addition to the top-level document.
Similarly, transform: false skips transforms for all subdocuments.
Note that this behavior is different for transforms defined in the schema:
if you define a transform in schema.options.toObject.transform, that transform
will not apply to subdocuments.
Transforms, like all of these options, are also available for toJSON. See this guide to JSON.stringify() to learn why toJSON() and toObject() are separate functions.
See schema options for some more details.
During save, no custom options are applied to the document before being sent to the database.
Helper for console.log
Clears the modified state on the specified path.
Sends an updateOne command with this document _id as the query selector.
Executes registered validation rules for this document.
This method is called pre save and if a validation rule is violated, save is aborted and the error is thrown.
Executes registered validation rules (skipping asynchronous validators) for this document.
This method is useful if you need synchronous validation.
Docs Home → MongoDB Manual
On this page
Collation allows users to specify language-specific rules for string
comparison, such as rules for lettercase and accent marks.
You can specify collation for a collection or a view, an index, or
specific operations that support collation.
To specify collation when you query documents in the MongoDB Atlas UI, see
Specify Collation.
A collation document has the following fields:
When specifying collation, the locale field is mandatory; all
other collation fields are optional. For descriptions of the fields,
see Collation Document.
Default collation parameter values vary depending on which
locale you specify. For a complete list of default collation
parameters and the locales they are associated with, see
Collation Default Parameters.
The ICU locale. See Supported Languages and Locales for a
list of supported locales.
To specify simple binary comparison, specify locale value of
"simple".
Optional. The level of comparison to perform.
Corresponds to ICU Comparison Levels.
Possible values are:
Tertiary level of comparison. Collation performs comparisons
up to tertiary differences, such as case and letter variants.
That is, collation performs comparisons of base characters
(primary differences), diacritics (secondary differences), and
case and variants (tertiary differences). Differences between
base characters takes precedence over secondary differences,
which takes precedence over tertiary differences.
This is the default level.
See ICU Collation: Comparison Levels
for details.
Optional. Flag that determines whether to include case comparison
at strength level 1 or 2.
If true, include case comparison:
When used with strength:1, collation compares base characters
and case.
When used with strength:2, collation compares base characters,
diacritics (and possible other secondary differences) and case.
If false, do not include case comparison at level 1 or
2. The default is false.
For more information, see ICU Collation: Case Level.
Optional. A field that determines sort order of case differences during
tertiary level comparisons.
Possible values are:
Optional. Flag that determines whether to compare numeric strings as numbers
or as strings.
If true, compare as numbers. For example,
"10" is greater than "2".
If false, compare as strings. For example,
"10" is less than "2".
Default is false.
See numericOrdering Restrictions.
Optional. Field that determines whether collation should consider whitespace
and punctuation as base characters for purposes of comparison.
Possible values are:
See ICU Collation: Comparison Levels
for more information.
Default is "non-ignorable".
Optional. Field that determines up to which characters are considered
ignorable when alternate: "shifted". Has no effect if
alternate: "non-ignorable"
Possible values are:
Optional. Flag that determines whether strings with diacritics sort from back
of the string, such as with some French dictionary ordering.
If true, compare from back to front.
If false, compare from front to back.
The default value is false.
Optional. Flag that determines whether to check if text require normalization
and to perform normalization. Generally, majority of text does not
require this normalization processing.
If true, check if fully normalized and perform normalization to
compare text.
If false, does not check.
The default value is false.
See
http://userguide.icu-project.org/collation/concepts#TOC-Normalization for details.
You can specify collation for the following operations:
You cannot specify multiple collations for an operation. For
example, you cannot specify different collations per field, or if
performing a find with a sort, you cannot use one collation for the
find and another for the sort.
Some collation locales have variants, which employ special
language-specific rules. To specify a locale variant, use the following
syntax:
For example, to use the unihan variant of the Chinese collation:
For a complete list of all collation locales and their variants, see
Collation Locales.
You can specify a default collation
for a view at creation time. If no collation is specified, the
view's default collation is the "simple" binary comparison
collator. That is, the view does not inherit the collection's
default collation.
String comparisons on the view use the view's default collation.
An operation that attempts to change or override a view's default
collation will fail with an error.
If creating a view from another view, you cannot specify a
collation that differs from the source view's collation.
If performing an aggregation that involves multiple views, such as
with $lookup or $graphLookup, the views must
have the same collation.
To use an index for string comparisons, an operation must also
specify the same collation. That is, an index with a collation
cannot support an operation that performs string comparisons on the
indexed fields if the operation specifies a different collation.
Because indexes that are configured with collation use ICU
collation keys to achieve sort order, collation-aware index keys
may be larger than index keys for indexes without collation.
For example, the collection myColl has an index on a string
field category with the collation locale "fr".
The following query operation, which specifies the same collation as
the index, can use the index:
However, the following query operation, which by default uses the
"simple" binary collator, cannot use the index:
For a compound index where the index prefix keys are not strings,
arrays, and embedded documents, an operation that specifies a
different collation can still use the index to support comparisons
on the index prefix keys.
For example, the collection myColl has a compound index on the
numeric fields score and price and the string field
category; the index is created with the  collation locale
"fr" for string comparisons:
The following operations, which use "simple" binary collation
for string comparisons, can use the index:
The following operation, which uses "simple" binary collation
for string comparisons on the indexed category field, can use
the index to fulfill only the score: 5 portion of the query:
Matches against document keys, including embedded document keys,
use simple binary comparison. This means that a query for a key
like "foo.bár" will not match the key "foo.bar", regardless of the value you
set for the strength parameter.
The following indexes only support simple binary comparison and do
not support collation:
Text indexes
2d indexes
To create a text or 2d index on a collection that has a
non-simple collation, you must explicitly specify {collation:
{locale: "simple"} } when creating the index.
When specifying the numericOrdering as true the following
restrictions apply:
Only contiguous non-negative integer substrings of digits are
considered in the comparisons.
numericOrdering does not support:
+
-
decimal separators, like decimal points and decimal commas
exponents
Only Unicode code points in the Number or Decimal Digit (Nd) category
are treated as digits.
If a digit length exceeds 254 characters, the excess characters are
treated as a separate number.
Consider a collection with the following string number and decimal
values:
The following find query uses a
collation document containing the numericOrdering parameter:
The operation returns the following results:
numericOrdering: true sorts the string values in ascending
order as if they were numeric values.
The two negative values -2.1 and -10 are not sorted in the
expected sort order because they have unsupported - characters.
The value 2.2 is sorted before the value 2.10, due to the fact
that the numericOrdering parameter does not support decimal
values.
As a result, 2.2 and 2.10 are sorted in lexicographic order.
On this page
About
Support
Social
Failed to fetch https://http://thecodebarbarian.com/a-nodejs-perspective-on-mongodb-34-collations: HTTPSConnectionPool(host='http', port=443): Max retries exceeded with url: //thecodebarbarian.com/a-nodejs-perspective-on-mongodb-34-collations (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f20e97d07f0>: Failed to resolve 'http' ([Errno -2] Name or service not known)"))
Docs Home → MongoDB Manual
On this page
Time series data is a sequence of data points in which insights are
gained by analyzing changes over time.
Time series data is generally composed of these components:
Time when the data point was recorded.
Metadata (sometimes referred to as source), which is a label or tag
that uniquely identifies a series and rarely changes.
Measurements (sometimes referred to as metrics or values), which
are the data points tracked at increments in time. Generally these are
key-value pairs that change over time.
This table shows examples of time series data:
For efficient time series data storage, MongoDB provides time series
collections.
New in version 5.0.
Time series collections efficiently store time series data. In time
series collections, writes are organized so that data from the same
source is stored alongside other data points from a similar point in
time.
You can create time series collections in the UI for deployments hosted in MongoDB Atlas.
Compared to normal collections, storing time series data in time series
collections improves query efficiency and reduces the disk usage for
time series data and secondary indexes.
MongoDB 6.3 and later automatically creates a compound index on the time and metadata
fields for new time series collections.
Time series collections use an underlying columnar storage format and
store data in time-order. This format provides the following benefits:
Reduced complexity for working with time series data
Improved query efficiency
Reduced disk usage
Reduced I/O for read operations
Increased WiredTiger cache usage
Time series collections behave like typical collections. You insert
and query data as usual.
MongoDB treats time series collections as writable non-materialized
views backed by an internal collection. When
you insert data, the internal collection automatically organizes time
series data into an optimized storage format.
Starting in MongoDB 6.3: if you create a new time series collection,
MongoDB also generates a compound index
on the metaField and timeField fields. To
improve query performance, queries on time series collections use the
new compound index. The compound index also uses the optimized storage
format.
To improve query performance, you can manually add secondary
indexes on measurement fields or
any field in your time series collection.
You must drop time series collections before downgrading:
MongoDB 6.0 or later to MongoDB 5.0.7 or earlier.
MongoDB 5.3 to MongoDB 5.0.5 or earlier.
Do not attempt to create a time series collection or view with the
name system.profile. MongoDB 6.3 and later versions return an
IllegalOperation error if you attempt to do so. Earlier MongoDB
versions crash.
To get started with time series collections, see
Create and Query a Time Series Collection.
On this page
About
Support
Social
Docs Home → MongoDB Manual
On this page
If an update operation with upsert: true
results in an insert of a document, then $setOnInsert
assigns the specified values to the fields in the document. If the
update operation does not result in an insert,
$setOnInsert does nothing.
You can specify the upsert option for:
db.collection.updateOne()
db.collection.updateMany()
db.collection.findAndModify()
To specify a <field> in an embedded document or in an array, use
dot notation.
Starting in MongoDB 5.0, update operators process document fields with
string-based names in lexicographic order. Fields with numeric names are
processed in numeric order. See Update Operators Behavior for details.
Starting in MongoDB 5.0, mongod no longer raises an
error when you use an update operator like $setOnInsert
with an empty operand expression ( { } ). An empty update results
in no changes and no oplog entry is created (meaning that the
operation is a no-op).
The products collection contains no documents.
Insert a new document using db.collection.updateOne() the upsert:
true parameter.
MongoDB uses <query> to create a new document with _id: 1.
$setOnInsert updates the document as specified.
The products collection contains the newly-inserted document:
When the upsert parameter is true
db.collection.updateOne():
creates a new document
applies the $set operation
applies the $setOnInsert operation
If db.collection.updateOne() matches an existing document,
MongoDB only applies the $set operation.
db.collection.updateOne()
db.collection.findAndModify()
On this page
About
Support
Social
If you haven't yet done so, please take a minute to read the quickstart to get an idea of how Mongoose works.
If you are migrating from 7.x to 8.x please take a moment to read the migration guide.
Everything in Mongoose starts with a Schema. Each schema maps to a MongoDB
collection and defines the shape of the documents within that collection.
If you want to add additional keys later, use the
Schema#add method.
Each key in our code blogSchema defines a property in our documents which
will be cast to its associated SchemaType.
For example, we've defined a property title which will be cast to the
String SchemaType and property date
which will be cast to a Date SchemaType.
Notice above that if a property only requires a type, it can be specified using
a shorthand notation (contrast the title property above with the date
property).
Keys may also be assigned nested objects containing further key/type definitions
like the meta property above.  This will happen whenever a key's value is a POJO
that doesn't have a type property.
In these cases, Mongoose only creates actual schema paths for leaves
in the tree. (like meta.votes and meta.favs above),
and the branches do not have actual paths.  A side-effect of this is that meta
above cannot have its own validation.  If validation is needed up the tree, a path
needs to be created up the tree - see the Subdocuments section
for more information on how to do this.  Also read the Mixed
subsection of the SchemaTypes guide for some gotchas.
The permitted SchemaTypes are:
Read more about SchemaTypes here.
Schemas not only define the structure of your document and casting of
properties, they also define document instance methods,
static Model methods, compound indexes,
and document lifecycle hooks called middleware.
To use our schema definition, we need to convert our blogSchema into a
Model we can work with.
To do so, we pass it into mongoose.model(modelName, schema):
By default, Mongoose adds an _id property to your schemas.
When you create a new document with the automatically added _id property, Mongoose creates a new _id of type ObjectId to your document.
You can also overwrite Mongoose's default _id with your own _id.
Just be careful: Mongoose will refuse to save a top-level document that doesn't have an _id, so you're responsible for setting _id if you define your own _id path.
Mongoose also adds an _id property to subdocuments.
You can disable the _id property on your subdocuments as follows.
Mongoose does allow saving subdocuments without an _id property.
Alternatively, you can disable _id using the following syntax:
Instances of Models are documents. Documents have
many of their own built-in instance methods.
We may also define our own custom document instance methods.
Now all of our animal instances have a findSimilarTypes method available
to them.
You can also add static functions to your model. There are three equivalent
ways to add a static:
Do not declare statics using ES6 arrow functions (=>). Arrow functions explicitly prevent binding this, so the above examples will not work because of the value of this.
You can also add query helper functions, which are like instance methods
but for mongoose queries. Query helper methods let you extend mongoose's
chainable query builder API.
MongoDB supports secondary indexes.
With mongoose, we define these indexes within our Schema at the path level or the schema level.
Defining indexes at the schema level is necessary when creating
compound indexes.
See SchemaType#index() for other index options.
When your application starts up, Mongoose automatically calls createIndex for each defined index in your schema.
Mongoose will call createIndex for each index sequentially, and emit an 'index' event on the model when all the createIndex calls succeeded or when there was an error.
While nice for development, it is recommended this behavior be disabled in production since index creation can cause a significant performance impact.
Disable the behavior by setting the autoIndex option of your schema to false, or globally on the connection by setting the option autoIndex to false.
Mongoose will emit an index event on the model when indexes are done
building or an error occurred.
See also the Model#ensureIndexes method.
Virtuals are document properties that
you can get and set but that do not get persisted to MongoDB. The getters
are useful for formatting or combining fields, while setters are useful for
de-composing a single value into multiple values for storage.
Suppose you want to print out the person's full name. You could do it yourself:
But concatenating the first and
last name every time can get cumbersome.
And what if you want to do some extra processing on the name, like
removing diacritics? A
virtual property getter lets you
define a fullName property that won't get persisted to MongoDB.
Now, mongoose will call your getter function every time you access the
fullName property:
If you use toJSON() or toObject() Mongoose will not include virtuals by default.
Pass { virtuals: true } to toJSON() or toObject() to include virtuals.
The above caveat for toJSON() also includes the output of calling JSON.stringify() on a Mongoose document, because JSON.stringify() calls toJSON().
To include virtuals in JSON.stringify() output, you can either call toObject({ virtuals: true }) on the document before calling JSON.stringify(), or set the toJSON: { virtuals: true } option on your schema.
You can also add a custom setter to your virtual that will let you set both
first name and last name via the fullName virtual.
Virtual property setters are applied before other validation. So the example
above would still work even if the first and last name fields were
required.
Only non-virtual properties work as part of queries and for field selection.
Since virtuals are not stored in MongoDB, you can't query with them.
You can learn more about virtuals here.
Aliases are a particular type of virtual where the getter and setter
seamlessly get and set another property. This is handy for saving network
bandwidth, so you can convert a short property name stored in the database
into a longer name for code readability.
You can also declare aliases on nested paths. It is easier to use nested
schemas and subdocuments, but you can also declare
nested path aliases inline as long as you use the full nested path
nested.myProp as the alias.
Schemas have a few configurable options which can be passed to the
constructor or to the set method:
Valid options:
By default, Mongoose's init() function
creates all the indexes defined in your model's schema by calling
Model.createIndexes()
after you successfully connect to MongoDB. Creating indexes automatically is
great for development and test environments. But index builds can also create
significant load on your production database. If you want to manage indexes
carefully in production, you can set autoIndex to false.
The autoIndex option is set to true by default. You can change this
default by setting mongoose.set('autoIndex', false);
Before Mongoose builds indexes, it calls Model.createCollection() to create the underlying collection in MongoDB by default.
Calling createCollection() sets the collection's default collation based on the collation option and establishes the collection as
a capped collection if you set the capped schema option.
You can disable this behavior by setting autoCreate to false using mongoose.set('autoCreate', false).
Like autoIndex, autoCreate is helpful for development and test environments, but you may want to disable it for production to avoid unnecessary database calls.
Unfortunately, createCollection() cannot change an existing collection.
For example, if you add capped: { size: 1024 } to your schema and the existing collection is not capped, createCollection() will not overwrite the existing collection.
That is because the MongoDB server does not allow changing a collection's options without dropping the collection first.
By default, mongoose buffers commands when the connection goes down until
the driver manages to reconnect. To disable buffering, set bufferCommands
to false.
The schema bufferCommands option overrides the global bufferCommands option.
If bufferCommands is on, this option sets the maximum amount of time Mongoose buffering will wait before
throwing an error. If not specified, Mongoose will use 10000 (10 seconds).
Mongoose supports MongoDBs capped
collections. To specify the underlying MongoDB collection be capped, set
the capped option to the maximum size of the collection in
bytes.
The capped option may also be set to an object if you want to pass
additional options like max.
In this case you must explicitly pass the size option, which is required.
Mongoose by default produces a collection name by passing the model name to
the utils.toCollectionName method.
This method pluralizes the name. Set this option if you need a different name
for your collection.
When you define a discriminator, Mongoose adds a path to your
schema that stores which discriminator a document is an instance of. By default, Mongoose
adds an __t path, but you can set discriminatorKey to overwrite this default.
When excludeIndexes is true, Mongoose will not create indexes from the given subdocument schema.
This option only works when the schema is used in a subdocument path or document array path, Mongoose ignores this option if set on the top-level schema for a model.
Defaults to false.
Mongoose assigns each of your schemas an id virtual getter by default
which returns the document's _id field cast to a string, or in the case of
ObjectIds, its hexString. If you don't want an id getter added to your
schema, you may disable it by passing this option at schema construction time.
Mongoose assigns each of your schemas an _id field by default if one
is not passed into the Schema constructor.
The type assigned is an ObjectId
to coincide with MongoDB's default behavior. If you don't want an _id
added to your schema at all, you may disable it using this option.
You can only use this option on subdocuments. Mongoose can't
save a document without knowing its id, so you will get an error if
you try to save a document without an _id.
Mongoose will, by default, "minimize" schemas by removing empty objects.
This behavior can be overridden by setting minimize option to false. It
will then store empty objects.
To check whether an object is empty, you can use the $isEmpty() helper:
Allows setting query#read options at the
schema level, providing us a way to apply default
ReadPreferences
to all queries derived from a model.
The alias of each pref is also permitted so instead of having to type out
'secondaryPreferred' and getting the spelling wrong, we can simply pass 'sp'.
The read option also allows us to specify tag sets. These tell the
driver from which members
of the replica-set it should attempt to read. Read more about tag sets
here and
here.
NOTE: you may also specify the driver read preference strategy
option when connecting:
Allows setting write concern
at the schema level.
The shardKey option is used when we have a sharded MongoDB architecture.
Each sharded collection is given a shard key which must be present in all
insert/update operations. We just need to set this schema option to the same
shard key and we’ll be all set.
Note that Mongoose does not send the shardcollection command for you. You
must configure your shards yourself.
The strict option, (enabled by default), ensures that values passed to our
model constructor that were not specified in our schema do not get saved to
the db.
This also affects the use of doc.set() to set a property value.
This value can be overridden at the model instance level by passing a second
boolean argument:
The strict option may also be set to "throw" which will cause errors
to be produced instead of dropping the bad data.
NOTE: Any key/val set on the instance that does not exist in your schema
is always ignored, regardless of schema option.
Mongoose supports a separate strictQuery option to avoid strict mode for query filters.
This is because empty query filters cause Mongoose to return all documents in the model, which can cause issues.
The strict option does apply to updates.
The strictQuery option is just for query filters.
Mongoose has a separate strictQuery option to toggle strict mode for the filter parameter to queries.
In general, we do not recommend passing user-defined objects as query filters:
In Mongoose 7, strictQuery is false by default.
However, you can override this behavior globally:
Exactly the same as the toObject option but only applies when
the document's toJSON method is called.
To see all available toJSON/toObject options, read this.
Documents have a toObject method
which converts the mongoose document into a plain JavaScript object. This
method accepts a few options. Instead of applying these options on a
per-document basis, we may declare the options at the schema level and have
them applied to all of the schema's documents by default.
To have all virtuals show up in your console.log output, set the
toObject option to { getters: true }:
To see all available toObject options, read this.
By default, if you have an object with key 'type' in your schema, mongoose
will interpret it as a type declaration.
However, for applications like geoJSON,
the 'type' property is important. If you want to control which key mongoose
uses to find type declarations, set the 'typeKey' schema option.
By default, documents are automatically validated before they are saved to
the database. This is to prevent saving an invalid document. If you want to
handle validation manually, and be able to save objects which don't pass
validation, you can set validateBeforeSave to false.
The versionKey is a property set on each document when first created by
Mongoose. This keys value contains the internal
revision
of the document. The versionKey option is a string that represents the
path to use for versioning. The default is __v. If this conflicts with
your application you can configure as such:
Note that Mongoose's default versioning is not a full optimistic concurrency
solution. Mongoose's default versioning only operates on arrays as shown below.
If you need optimistic concurrency support for save(), you can set the optimisticConcurrency option
Document versioning can also be disabled by setting the versionKey to
false.
DO NOT disable versioning unless you know what you are doing.
Mongoose only updates the version key when you use save().
If you use update(), findOneAndUpdate(), etc. Mongoose will not
update the version key. As a workaround, you can use the below middleware.
Optimistic concurrency is a strategy to ensure
the document you're updating didn't change between when you loaded it using find() or findOne(), and when
you update it using save().
For example, suppose you have a House model that contains a list of photos, and a status that represents
whether this house shows up in searches. Suppose that a house that has status 'APPROVED' must have at least
two photos. You might implement the logic of approving a house document as shown below:
The markApproved() function looks right in isolation, but there might be a potential issue: what if another
function removes the house's photos between the findOne() call and the save() call? For example, the below
code will succeed:
If you set the optimisticConcurrency option on the House model's schema, the above script will throw an
error.
Sets a default collation
for every query and aggregation. Here's a beginner-friendly overview of collations.
If you set the timeseries option on a schema, Mongoose will create a timeseries collection for any model that you create from that schema.
skipVersioning allows excluding paths from versioning (i.e., the internal
revision will not be incremented even if these paths are updated). DO NOT
do this unless you know what you're doing. For subdocuments, include this
on the parent document using the fully qualified path.
The timestamps option tells Mongoose to assign createdAt and updatedAt fields
to your schema. The type assigned is Date.
By default, the names of the fields are createdAt and updatedAt. Customize
the field names by setting timestamps.createdAt and timestamps.updatedAt.
The way timestamps works under the hood is:
By default, Mongoose uses new Date() to get the current time.
If you want to overwrite the function
Mongoose uses to get the current time, you can set the
timestamps.currentTime option. Mongoose will call the
timestamps.currentTime function whenever it needs to get
the current time.
Mongoose supports defining global plugins, plugins that apply to all schemas.
Sometimes, you may only want to apply a given plugin to some schemas.
In that case, you can add pluginTags to a schema:
If you call plugin() with a tags option, Mongoose will only apply that plugin to schemas that have a matching entry in pluginTags.
By default, Mongoose will automatically select() any populated paths for
you, unless you explicitly exclude them.
To opt out of selecting populated fields by default, set selectPopulatedPaths
to false in your schema.
For legacy reasons, when there is a validation error in subpath of a
single nested schema, Mongoose will record that there was a validation error
in the single nested schema path as well. For example:
Set the storeSubdocValidationError to false on the child schema to make
Mongoose only reports the parent error.
Options like collation and capped affect the options Mongoose passes to MongoDB when creating a new collection.
Mongoose schemas support most MongoDB createCollection() options, but not all.
You can use the collectionOptions option to set any createCollection() options; Mongoose will use collectionOptions as the default values when calling createCollection() for your schema.
Similar to autoIndex, except for automatically creates any Atlas search indexes defined in your schema.
Unlike autoIndex, this option defaults to false.
Schemas have a loadClass() method
that you can use to create a Mongoose schema from an ES6 class:
Here's an example of using loadClass() to create a schema from an ES6 class:
Schemas are also pluggable which allows us to package up reusable features into
plugins that can be shared with the community or just between your projects.
Here's an alternative introduction to Mongoose schemas.
To get the most out of MongoDB, you need to learn the basics of MongoDB schema design.
SQL schema design (third normal form) was designed to minimize storage costs,
whereas MongoDB schema design is about making common queries as fast as possible.
The 6 Rules of Thumb for MongoDB Schema Design blog series
is an excellent resource for learning the basic rules for making your queries
fast.
Users looking to master MongoDB schema design in Node.js should look into
The Little MongoDB Schema Design Book
by Christian Kvalheim, the original author of the MongoDB Node.js driver.
This book shows you how to implement performant schemas for a laundry list
of use cases, including e-commerce, wikis, and appointment bookings.
Now that we've covered Schemas, let's take a look at SchemaTypes.
Docs Home → MongoDB Manual
On this page
Creates a new collection. For views,
see db.createView().
Because MongoDB creates a collection implicitly when the collection
is first referenced in a command, this method is used primarily for
creating new collections that use specific options. For example, you
use db.createCollection() to create a:
Capped collection.
Clustered collection.
New collection that uses document validation.
db.createCollection() is a wrapper around the database
command create.
You can use db.createCollection() for deployments hosted in the following
environments:
MongoDB Atlas: The fully
managed service for MongoDB deployments in the cloud
MongoDB Enterprise: The
subscription-based, self-managed version of MongoDB
MongoDB Community: The
source-available, free-to-use, and self-managed version of MongoDB
The db.createCollection() method has the following
prototype form:
The db.createCollection() method has the following parameters:
Optional. Configuration options for creating a:
Capped collection
Clustered collection
View
The options document contains the following fields:
Optional. The name of the field which contains metadata in
each time series document. The metadata in the specified field
should be data that is used to label a unique series of
documents. The metadata should rarely, if ever, change.
The name of the specified field may not be _id or the same
as the timeseries.timeField. The field can be of any type
except array.
Optional, do not use if setting bucketRoundingSeconds and
bucketMaxSpanSeconds. Possible values are seconds
(default), minutes, and hours.
Set granularity to the value that most closely matches
the time between consecutive incoming timestamps. This
improves performance by optimizing how MongoDB internally
stores data in the collection.
For more information on granularity and bucket intervals, see
Set Granularity for Time Series Data.
Optional, used with bucketRoundingSeconds as an
alternative to granularity. Sets the maximum time between
timestamps in the same bucket. Possible values are 1-31536000.
If you set bucketMaxSpanSeconds, you must set
bucketRoundingSeconds to the same value.
To downgrade below MongoDB 6.3, you must either modify the
collection to use the corresponding granularity value, or
drop the collection. For details, see collMod.
Optional, used with bucketMaxSpanSeconds as an alternative
to granularity. Sets the number of seconds to round down
by when MongoDB sets the minimum timestamp for a new bucket.
Must be equal to bucketMaxSpanSeconds.
For example, setting both parameters to 1800 rounds new
buckets down to the nearest 30 minutes. If a document with a
time of 2023-03-27T18:24:35Z does not fit an
existing bucket, MongoDB creates a new bucket with a minimum
time of 2023-03-27T18:00:00Z and a maximum time of
2023-03-27T18:30:00Z.
Optional. Specifies the seconds after which documents in a
time series collection or clustered collection
expire. MongoDB deletes expired documents automatically.
For clustered collections, the documents are deleted
automatically based on the clustered index key _id and
the values must be date types. See TTL Indexes.
Starting in MongoDB 5.3, you can create a collection with a
clustered index. Collections
created with a clustered index are called clustered collections.
See Clustered Collections.
clusteredIndex has the following syntax:
New in version 5.3.
Optional.
Starting in MongoDB 6.0, you can use change stream events to output the version of a document before and
after changes (the document pre- and post-images):
The pre-image is the document before it was replaced, updated, or
deleted. There is no pre-image for an inserted document.
The post-image is the document after it was inserted, replaced, or
updated. There is no post-image for a deleted document.
Enable changeStreamPreAndPostImages for a collection using
db.createCollection(), create, or
collMod.
changeStreamPreAndPostImages has the following syntax:
For complete examples with the change stream output, see
Change Streams with Document Pre- and Post-Images.
For a db.createCollection() example on this page,
see
Create a Collection with Change Stream Pre- and Post-Images for Documents.
New in version 6.0.
Optional. Available for the WiredTiger storage engine only.
Allows users to specify configuration to the storage engine on a
per-collection basis when creating a collection. The value of the
storageEngine option should take the following form:
Storage engine configuration specified when creating collections
are validated and logged to the oplog during replication
to support replica sets with members that use different storage
engines.
Starting in MongoDB 7.2 (and 7.0.5), you can't specify wiredTiger storage
engine encryption options when you create a collection with
db.createCollection(). To configure encryption for
the WiredTiger storage engine, see Encryption at Rest.
Specify Storage Engine Options
Optional. Allows users to specify validation rules or
expressions for the collection.
The validator option takes a document that specifies the
validation rules or expressions. You can specify the expressions
using the same operators as the query operators with the exception of $near,
$nearSphere, $text, and $where.
To learn how to create a collection with schema validation,
see Specify JSON Schema Validation.
Optional. Determines how strictly MongoDB applies the
validation rules to existing documents during an update.
To see an example that uses validationLevel, see
Specify Validation Level for Existing Documents.
Optional. Determines whether to error on invalid documents
or just warn about the violations but allow invalid
documents to be inserted.
Validation of documents only applies to those documents as
determined by the validationLevel.
To see an example that uses validationAction, see
Choose How to Handle Invalid Documents.
Optional. Allows users to specify a default configuration for
indexes when creating a collection.
The indexOptionDefaults option accepts a storageEngine
document, which should take the following form:
Storage engine configuration specified when creating indexes are
validated and logged to the oplog during replication to
support replica sets with members that use different storage
engines.
Specifies the default collation for the
collection.
Collation allows users to specify
language-specific rules for string comparison, such as rules for
lettercase and accent marks.
The collation option has the following syntax:
When specifying collation, the locale field is mandatory; all
other collation fields are optional. For descriptions of the fields,
see Collation Document.
If you specify a collation at the collection level:
Indexes on that collection will be created with that collation unless
the index creation operation explicitly specify a different collation.
Operations on that collection use the collection's default
collation unless they explicitly specify a different collation.
You cannot specify multiple collations for an operation. For
example, you cannot specify different collations per field, or if
performing a find with a sort, you cannot use one collation for the
find and another for the sort.
If no collation is specified for the collection or for the
operations, MongoDB uses the simple binary comparison used in prior
versions for string comparisons.
For a collection, you can only specify the collation during the
collection creation. Once set, you cannot modify the collection's
default collation.
For an example, see Specify Collation.
Optional. A document that expresses the write concern for the operation. Omit to use the default write
concern.
When issued on a sharded cluster, mongos converts the
write concern of the
create command and its helper
db.createCollection() to "majority".
If the deployment enforces
authentication/authorization,
db.createCollection() requires the following privileges:
createCollection on the database, or
insert on the collection to create
convertToCapped for the collection
createCollection on the database
createCollection on the database.
However, if the user has the createCollection on
the database and find on the view to create,
the user must also have the following additional permissions:
find on the source collection or view.
find on any other collections or views
referenced in the pipeline, if any.
A user with the readWrite built in role on the database
has the required privileges to run the listed operations. Either
create a user with the required role
or grant the role to an existing user.
db.createCollection() obtains an exclusive lock on the
specified collection or view for the duration of the operation. All
subsequent operations on the collection must wait until
db.createCollection() releases the lock. db.createCollection() typically holds
this lock for a short time.
Creating a view requires obtaining an additional exclusive lock
on the system.views collection in the database. This lock blocks
creation or modification of views in the database until the command
completes.
You can create collections and indexes inside a distributed
transaction if the
transaction is not a cross-shard write transaction.
To use db.createCollection() in a transaction, the transaction must use read
concern "local". If you specify a read concern level
other than "local", the transaction fails.
Create Collections and Indexes in a Transaction
Capped collections
have maximum size or document counts that prevent them from growing
beyond maximum thresholds. All capped collections must specify a maximum
size and may also specify a maximum document count. MongoDB removes
older documents if a collection reaches the maximum size limit before it
reaches the maximum document count. Consider the following example:
This command creates a collection named log with a maximum size of 5
megabytes and a maximum of 5000 documents.
See Capped Collections for more
information about capped collections.
To create a time series collection that captures weather data
for the past 24 hours, issue this command:
Alternately, to create the same collection but limit each bucket to
timestamp values within the same hour, issue this command:
The following db.createCollection() example adds a
clustered collection named stocks:
In the example, clusteredIndex specifies:
"key": { _id: 1 }, which sets the clustered index key to the
_id field.
"unique": true, which indicates the clustered index key value must
be unique.
"name": "stocks clustered key", which sets the clustered index name.
Starting in MongoDB 6.0, you can use change stream events to output the version of a document before and
after changes (the document pre- and post-images):
The pre-image is the document before it was replaced, updated, or
deleted. There is no pre-image for an inserted document.
The post-image is the document after it was inserted, replaced, or
updated. There is no post-image for a deleted document.
Enable changeStreamPreAndPostImages for a collection using
db.createCollection(), create, or
collMod.
The following example creates a collection that has
changeStreamPreAndPostImages enabled:
Pre- and post-images are not available for a change stream event if the images were:
Not enabled on the collection at the time of a document update or
delete operation.
Removed after the pre- and post-image retention time set in
expireAfterSeconds.
The following example sets expireAfterSeconds to 100
seconds:
The following example returns the current changeStreamOptions
settings, including expireAfterSeconds:
Setting expireAfterSeconds to off uses the default retention
policy: pre- and post-images are retained until the corresponding
change stream events are removed from the oplog.
If a change stream event is removed from the oplog, then the
corresponding pre- and post-images are also deleted regardless of
the expireAfterSeconds pre- and post-image retention time.
Additional considerations:
Enabling pre- and post-images consumes storage space and adds
processing time. Only enable pre- and post-images if you need them.
Limit the change stream event size to less than 16 megabytes. To limit
the event size, you can:
Limit the document size to 8 megabytes. You can request pre- and
post-images simultaneously in the change stream output if other change stream event fields like
updateDescription are not large.
Request only post-images in the change stream output for documents
up to 16 megabytes if other change stream event fields like
updateDescription are not large.
Request only pre-images in the change stream output for documents up
to 16 megabytes if:
document updates affect only a small fraction of the document
structure or content, and
do not cause a replace change event. A replace event
always includes the post-image.
To request a pre-image, you set fullDocumentBeforeChange to
required or whenAvailable in db.collection.watch().
To request a post-image, you set fullDocument using the same
method.
Pre-images are written to the config.system.preimages collection.
The config.system.preimages collection may become large. To
limit the collection size, you can set expireAfterSeconds
time for the pre-images as shown earlier.
Pre-images are removed asynchronously by a background process.
Starting in MongoDB 6.0, if you are using document pre- and post-images
for change streams, you must disable
changeStreamPreAndPostImages for each collection using
the collMod command before you can downgrade to an earlier
MongoDB version.
For change stream events and output, see
Change Events.
To watch a collection for changes, see
db.collection.watch().
For complete examples with the change stream output, see
Change Streams with Document Pre- and Post-Images.
Collation allows users to specify
language-specific rules for string comparison, such as rules for
lettercase and accent marks.
You can specify collation at the collection or
view level. For example, the following
operation creates a collection, specifying a collation for the
collection (See Collation Document for descriptions of
the collation fields):
This collation will be used by indexes and operations that support
collation unless they explicitly specify a different collation. For
example, insert the following documents into myColl:
The following operation uses the collection's collation:
The operation returns documents in the following order:
The same operation on a collection that uses simple binary collation
(i.e. no specific collation set) returns documents in the following order:
Create a View with Default Collation
You can specify collection-specific storage engine configuration
options when you create a collection with
db.createCollection(). Consider the following operation:
This operation creates a new collection named users with a
specific configuration string that MongoDB will pass to the
wiredTiger storage engine. See the WiredTiger documentation of
collection level options
for specific wiredTiger options.
Starting in MongoDB 7.2 (and 7.0.5), you can't specify wiredTiger storage
engine encryption options when you create a collection with
db.createCollection(). To configure encryption for
the WiredTiger storage engine, see Encryption at Rest.
On this page
About
Support
Social
Docs Home → MongoDB Atlas
On this page
Atlas Search index is a data structure that categorizes data in an easily
searchable format. It is a mapping between terms and the documents that
contain those terms. Atlas Search indexes enable faster retrieval of documents
using certain identifiers. You must configure an Atlas Search index to query
data in your Atlas cluster using Atlas Search.
You can create an Atlas Search index on a single field or on multiple fields.
We recommend that you index the fields that you regularly use to sort
or filter your data in order to quickly retrieve the documents that
contain the relevant data at query-time.
You can create an Atlas Search index for all collections except time
series collections on your Atlas
cluster through the Atlas UI, API, Atlas CLI, and
Terraform.
If you use the $out
aggregation stage to modify a collection with an Atlas Search index,
you must delete and re-create the search index. If possible,
consider using $merge
instead of $out.
To create an Atlas Search index, you must have an Atlas cluster with:
MongoDB version 4.2 or higher
Collection to create the Atlas Search index for
The following table shows the modes of access each role supports.
You cannot create more than:
3 indexes on M0 clusters.
5 indexes on M2 clusters.
10 indexes on M5 clusters.
There are no limits to the number of indexes you can create on M10+ clusters.
When you create a new Atlas Search index, choose a configuration method.
You can specify the type of index that you want to create. For
$search queries, if you don't explicitly specify
type, the type defaults to search.
You can use either the default index definition or specify a custom
definition for the index. The default index definition is dynamic
mapping of fields in the documents and will
work with any collection. If you wish to create a custom index
definition for static mapping, you
can specify which fields to index with which analyzer and as which data
type.
The index name defaults to default. You can leave
the default name in place or choose one of your own.
If you name your index default, you don't need to specify
an index parameter when using the $search pipeline stage. Otherwise, you must specify
the index name using the index parameter.
Index names must be unique within their namespace.
Follow along with this video tutorial walk-through that demonstrates
how to create Atlas Search indexes of various complexity.
Duration: 15 Minutes
To create an Atlas Search index from the Atlas UI:
Click Database in the top-left corner of Atlas to navigate to
the Database Deployments page for your project.
Click Create Search Index.
For a guided experience, select the Atlas Search Visual Editor.
To edit the raw index definition, select the Atlas Search
JSON Editor.
In the Index Name field, enter a name for the index.
If you name your index default, you don't need to specify
an index parameter when using the $search pipeline stage. Otherwise, you must specify
the index name using the index parameter.
In the Database and Collection section, find the
database or collection, and select the collection name.
If you use the Visual Editor, click Next.
If you are satisfied with the default configuration, skip to step 10.
If you wish to refine your Atlas Search index, proceed to the next step.
If you use the Visual Editor and your index definition
contains static mappings, you can
save an index definition as a draft. You can't save the
default index definition as a
draft. You can save only a custom index definition as a draft.
Click Cancel.
Click Save Draft or Delete Draft.
You can't create a new index when you have a pending index draft.
To learn more about creating an index using an index draft, see
Resume or Delete an Atlas Search Index Draft.
A modal window appears to let you know your index is building. Click
the Close button.
The newly created index appears on the Atlas Search tab. While
the index is building, the Status field reads
Build in Progress. When the index is finished building,
the Status field reads Active.
Larger collections take longer to index. You will receive an email
notification when your index is finished building.
To create an Atlas Search index, send a POST request to the
fts/indexes endpoint. To learn more about the
syntax and parameters for this endpoint, see
Create One.
Atlas doesn't create the index if the collection doesn't exist,
but it still returns a 200 status.
You can also use Atlas Search with local Atlas deployments that you create
with the Atlas CLI. To learn more, see
Create a Local Atlas Deployment.
To create a search index for a cluster using the
Atlas CLI, run the following command:
To learn more about the command syntax and parameters, see the
Atlas CLI documentation for atlas clusters search indexes create.
Install the Atlas CLI
Connect to the Atlas CLI
To create a search index for the specified deployment using the
Atlas CLI, run the following command:
To learn more about the command syntax and parameters, see the
Atlas CLI documentation for atlas deployments search indexes create.
Install the Atlas CLI
Connect to the Atlas CLI
You can create an Atlas Search index programmatically by using
mongosh or a supported MongoDB Driver
in your preferred language.
You can't use the mongosh command or driver helper methods to
create Atlas Search indexes on M0, M2, or M5 Atlas
clusters. To create Atlas Search indexes using mongosh or the
driver, upgrade to a dedicated cluster tier.
You must have at least the
readWriteAnyDatabase role or readWrite access to
the database that contains the indexes. To learn more,
see Built-in Roles or Specific Privileges.
➤ Use the Select your language drop-down menu to set the
language of the example in this section.
When you create the Atlas Search index, the Atlas Search Indexes tab in the
right-side panel of the Atlas UI displays information about Atlas Search
indexes for the selected namespace. The Status column shows
the current state of the index on the primary node of the cluster.
Click the View status details link below the status to view
the state of the index on all the nodes of the cluster.
When the Status column reads Active, the index
is ready to use. In other states, queries against the index may return
incomplete results.
Atlas is building the index or re-building the
index after an edit. When the index is in this state:
For a new index, Atlas Search doesn't serve queries until the
index build is complete.
For an existing index, you can continue to use the old index
for existing and new queries until the index rebuild
is complete.
While Atlas builds the index and after the build completes, the
Documents column shows the percentage and number of
documents indexed. The column also shows the total number of documents
in the collection.
If you shard a collection that already has an Atlas Search index, you might
experience a brief period of query downtime when the collection
begins to appear on a shard. Also, if you add a shard for an already
sharded collection that contains an Atlas Search index, your search queries
against that collection will fail until the initial sync process
completes on the added shards. To learn more, see initial sync process.
On this page
About
Support
Social
Schema constructor.
When nesting schemas, (children in the example above), always declare the child schema first before passing it into its parent.
The various built-in Mongoose Schema Types.
Using this exposed access to the Mixed SchemaType, we can use them in our schema.
The allowed index types
Adds key path / schema type pairs to this schema.
Add an alias for path. This means getting or setting the alias
is equivalent to getting or setting the path.
Array of child schemas (from document arrays and single nested subdocs)
and their corresponding compiled models. Each element of the array is
an object with 2 properties: schema and model.
This property is typically only useful for plugin authors and advanced users.
You do not need to interact with this property at all to use mongoose.
Remove all indexes from this schema.
clearIndexes only removes indexes from your schema object. Does not affect the indexes
in MongoDB.
Returns a deep copy of the schema
Inherit a Schema by applying a discriminator on an existing Schema.
Iterates the schemas paths similar to Array#forEach.
The callback is passed the pathname and the schemaType instance.
Gets a schema option.
Defines an index (most likely compound) for this schema.
Returns a list of indexes that this schema declares, via schema.index() or by index: true in a path's options.
Indexes are expressed as an array [spec, options].
Plugins can use the return value of this function to modify a schema's indexes.
For example, the below plugin makes every index unique by default.
Loads an ES6 class into a schema. Maps setters + getters, static methods,
and instance methods
to schema virtuals,
statics, and
methods.
Adds an instance method to documents constructed from Models compiled from this schema.
If a hash of name/fn pairs is passed as the only argument, each name/fn pair will be added as methods.
NOTE: Schema.method() adds instance methods to the Schema.methods object. You can also add instance methods directly to the Schema.methods object as seen in the guide
The original object passed to the schema constructor
Returns a new schema that has the paths from the original schema, minus the omitted ones.
This method is analagous to Lodash's omit() function for Mongoose schemas.
Gets/sets schema paths.
Sets a path (if arity 2)
Gets a path (if arity 1)
Returns the pathType of path for this schema.
Given a path, returns whether it is a real, virtual, nested, or ad-hoc/undefined path.
The paths defined on this schema. The keys are the top-level paths
in this schema, and the values are instances of the SchemaType class.
Returns a new schema that has the picked paths from this schema.
This method is analagous to Lodash's pick() function for Mongoose schemas.
Registers a plugin for this schema.
Or with Options:
Defines a post hook for the document
Defines a pre hook for the model.
Adds a method call to the queue.
Removes the given path (or [paths]).
Or as a Array:
Remove an index by name or index specification.
removeIndex only removes indexes from your schema object. Does not affect the indexes
in MongoDB.
Removes the given virtual or virtuals from the schema.
Returns an Array of path strings that are required by this schema.
Add an Atlas search index that Mongoose will create using Model.createSearchIndex().
This function only works when connected to MongoDB Atlas.
Sets a schema option.
Adds static "class" methods to Models compiled from this schema.
If a hash of name/fn pairs is passed as the only argument, each name/fn pair will be added as methods.
If a hash of name/fn pairs is passed as the only argument, each name/fn pair will be added as statics.
Creates a virtual type with the given name.
Returns the virtual type with the given name.
Object containing all virtuals defined on this schema.
The objects' keys are the virtual paths and values are instances of VirtualType.
This property is typically only useful for plugin authors and advanced users.
You do not need to interact with this property at all to use mongoose.
Reserved document keys.
Keys in this object are names that are warned in schema declarations
because they have the potential to break Mongoose/ Mongoose plugins functionality. If you create a schema
using new Schema() with one of these property names, Mongoose will log a warning.
NOTE: Use of these terms as method names is permitted, but play at your own risk, as they may be existing mongoose document methods you are stomping on.
Most Popular Articles
EbooksThe 80/20 Guide to ES2015 Generators
The 80/20 Guide to ES2015 Generators
JavaScript introduced the class keyword in 2015 with the release of ES6. React made classes an indispensable language feature when they introduced support for using extends React.Component instead of React.createClass() in 2015, and removed support for React.createClass() entirely in 2017 in favor of classes. Today, classes are a fundamental part of JavaScript, and many new JavaScript devs don't remember a time before classes. In this article, I'll provide an overview of how classes work in JavaScript: how to declare classes, what features JavaScript provides with classes, and how inheritance works.
Here's how you define a basic class MyClass, and create an instance of MyClass.
You must instantiate a class with new. Calling MyClass() without new
throws an error:
A class is technically a function, although the ECMAScript spec explicitly disallows calling a class without new. In fact, the typeof operator identifies MyClass as a function.
To check whether an object is an instance of a class, you should use the instanceof
operator. You can also check whether the constructor property is equal to MyClass.
Like functions, classes in JavaScript are variables like any other. You can
assign a class to a variable, overwrite that class, and pass a class as a
parameter to a function. Like functions, you can also declare classes with or
without explicit names.
Unlike functions, classes are never hoisted.
In the below example the function Foo() prints successfully, because JavaScript
looks ahead and 'hoists' Foo() to the top of the function. But trying to print the
class Bar throws a reference error, because JavaScript does not hoist class definitions.
ES6 classes support numerous object-oriented programming constructs, like static functions, instance methods, and getters and setters.
Static functions are functions defined on the class itself. You call ClassName.staticName(), and, within the static function, this refers to
the class.
Instance methods are functions on instances of the class. When you create a new
object using new MyClass(), you can call obj.myMethod(). Within myMethod(),
this refers to obj.
Getters and setters let you define functions that run when you access or assign a property on
an instance of the class. For example, getters and setters can let you convert values to numbers when you set the property.
In the below example, instances of MyClass have a
special property num that the class tries to convert to a number. The num
property has a getter function that JavaScript executes when you access obj.num,
and a setter function that JavaScript executes when you assign to obj.num using
= or Object.assign(). The setter function converts num to a number, and
throws an error if it could not convert the given value to a number.
Inheritance is one of the four core concepts of object-oriented programming. Besides syntactic sugar, the big advantage of using
ES6 classes over pre-ES6 functions as class definitions
is cleaner inheritance.
JavaScript class inheritance is still prototype-based under the hood, but extends abstracts away prototypes. Without having to write prototype, class
inheritance in JavaScript looks a lot like inheritance in more "proper"
object-oriented languages like Java. Here's a basic example of inheritance using
ES6 classes:
The class A extends B syntax means the child class A has the same members
(including statics, methods, getters, and setters) as the
base class B, but can also override B's members. Here's how you would do
the same thing using pre-ES6 prototype-based inheritance.
Like in Java, you call the parent class's constructor using super, and you
need to call super() in the constructor before accessing this.
Node.js has a native inherits() function that many developers used for inheritance before ES6. The key difference
between inherits() and extends is that Node.js inherits() does not
inherit statics.
Classes are a fundamental part of JavaScript, and ES6 classes give you syntax
and inheritance that closely mimic those of OOP languages like Java. JavaScript
still uses prototype-based inheritance under the hood, which comes with several corner cases, but extends behaves as you would expect with the exception of
static properties. On the bright side, JavaScript classes are just variables,
which means you can assign classes to variables and pass classes as parameters
to functions without any special syntax.
In object-oriented programming, a class
is a template for creating objects. JavaScript's class keyword is how you declare a new class in JavaScript.
A method is a function defined in your
class that JavaScript adds to every instance of that class. For example, suppose
you wanted to compute the area of a Rectangle. You can define an area() method
as shown below.
In a method, the this keyword refers to the class instance the method is attached to. In the above example,
this refers to obj.
A static is a
a function that is defined on the class itself. In JavaScript, a class is just
another variable, so you can call static functions on a class.
An alternative way to define the area of a Rectangle is using getters. Using a getter,
you can make area a dynamically computed property of a Rectangle, rather
than a method.
You can also define a custom setter, which gets called when you set a property.
For example, suppose you want to be absolutely certain that height and width
are numbers. You can define a custom setter that throws an exception whenever
someone tries the set height to a non-numeric value.
When a class extends another class, that means the subclass has all the same statics, methods, getters, and setters as the parent class by default. But then the subclass can define additional
statics, methods, getters, and setters. The subclass can also override the base class's statics, methods, getters, and setters.
The extends keyword still uses prototype-based inheritance under the hood. That means you can use
prototype-based patterns in combination with ES6 classes.
Copyright Â© MeanIT Software, Inc.
In object-oriented programming, a class
is a template for creating objects. JavaScript's class keyword is how you declare a new class in JavaScript.
A method is a function defined in your
class that JavaScript adds to every instance of that class. For example, suppose
you wanted to compute the area of a Rectangle. You can define an area() method
as shown below.
In a method, the this keyword refers to the class instance the method is attached to. In the above example,
this refers to obj.
A static is a
a function that is defined on the class itself. In JavaScript, a class is just
another variable, so you can call static functions on a class.
An alternative way to define the area of a Rectangle is using getters. Using a getter,
you can make area a dynamically computed property of a Rectangle, rather
than a method.
You can also define a custom setter, which gets called when you set a property.
For example, suppose you want to be absolutely certain that height and width
are numbers. You can define a custom setter that throws an exception whenever
someone tries the set height to a non-numeric value.
When a class extends another class, that means the subclass has all the same statics, methods, getters, and setters as the parent class by default. But then the subclass can define additional
statics, methods, getters, and setters. The subclass can also override the base class's statics, methods, getters, and setters.
The extends keyword still uses prototype-based inheritance under the hood. That means you can use
prototype-based patterns in combination with ES6 classes.
Copyright Â© MeanIT Software, Inc.
In object-oriented programming, a class
is a template for creating objects. JavaScript's class keyword is how you declare a new class in JavaScript.
A method is a function defined in your
class that JavaScript adds to every instance of that class. For example, suppose
you wanted to compute the area of a Rectangle. You can define an area() method
as shown below.
In a method, the this keyword refers to the class instance the method is attached to. In the above example,
this refers to obj.
A static is a
a function that is defined on the class itself. In JavaScript, a class is just
another variable, so you can call static functions on a class.
An alternative way to define the area of a Rectangle is using getters. Using a getter,
you can make area a dynamically computed property of a Rectangle, rather
than a method.
You can also define a custom setter, which gets called when you set a property.
For example, suppose you want to be absolutely certain that height and width
are numbers. You can define a custom setter that throws an exception whenever
someone tries the set height to a non-numeric value.
When a class extends another class, that means the subclass has all the same statics, methods, getters, and setters as the parent class by default. But then the subclass can define additional
statics, methods, getters, and setters. The subclass can also override the base class's statics, methods, getters, and setters.
The extends keyword still uses prototype-based inheritance under the hood. That means you can use
prototype-based patterns in combination with ES6 classes.
Copyright Â© MeanIT Software, Inc.
In Mongoose, a schema is a configuration
object for a model. Schemas do not allow you to read and write from MongoDB,
that's what models are for. But they do:
The first parameter to the Schema class constructor is a definition object.
This object defines what paths a schema has. For example, the below userSchema has a name path and an age path.
To create a model in Mongoose, you call the mongoose.model() function with a schema as the 2nd parameter. For example, UserModel in the below example will
have name and age properties, and will strip out any properties that aren't
defined in userSchema.
Furthermore, Mongoose will cast documents to match the given schema types. This
means you can safely pass untrusted data to Mongoose and trust that the data
will match your schema.
In addition to casting values, Mongoose also lets you define validation
in your schemas. For example, suppose you want to ensure your users have a
name. You can make the name property required in your schema as shown below.
The schema constructor takes 2 parameters: definition and options.
You can find a complete list of schema options on the Mongoose docs.
For example, the typeKey option lets you configure what key Mongoose looks for to determine if you're defining a nested path. Suppose you wanted to define a nested key named type:
There are several workarounds
for this use case. One is to set the typeKey option as shown below.
Copyright Â© MeanIT Software, Inc.
Third normal form (3NF) is a database schema design approach for relational databases which uses normalizing principles to reduce the duplication of data, avoid data anomalies, ensure referential integrity, and simplify data management. It was defined in 1971 by Edgar F. Codd, an English computer scientist who invented the relational model for database management.

A database relation (e.g. a database table) is said to meet third normal form standards if all the attributes (e.g. database columns) are functionally dependent on solely the primary key. Codd defined this as a relation in second normal form where all non-prime attributes depend only on the candidate keys and do not have a transitive dependency on another key.[1]

A hypothetical example of a failure to meet third normal form would be a hospital database having a table of patients which included a column for the telephone number of their doctor. The phone number is dependent on the doctor, rather than the patient, thus would be better stored in a table of doctors. The negative outcome of such a design is that a doctor's number will be duplicated in the database if they have multiple patients, thus increasing both the chance of input error and the cost and risk of updating that number should it change (compared to a third normal form-compliant data model that only stores a doctor's number once on a doctor table).

Codd later realized that 3NF did not eliminate all undesirable data anomalies and developed a stronger version to address this in 1974, known as Boyce–Codd normal form.

The third normal form (3NF) is a normal form used in database normalization. 3NF was originally defined by E. F. Codd in 1971.[2]

Codd's definition states that a table is in 3NF if and only if both of the following conditions hold:

A non-prime attribute of R is an attribute that does not belong to any candidate key of R.[3] A transitive dependency is a functional dependency in which X → Z (X determines Z) indirectly, by virtue of X → Y and Y → Z (where it is not the case that Y → X).[4]

A 3NF definition that is equivalent to Codd's, but expressed differently, was given by Carlo Zaniolo in 1982. This definition states that a table is in 3NF if and only if for each of its functional dependencies X → Y, at least one of the following conditions holds:[5][6][need quotation to verify]

To rephrase Zaniolo's definition more simply, the relation is in 3NF if and only if for every non-trivial functional dependency X → Y, X is a superkey or Y \ X consists of prime attributes. Zaniolo's definition gives a clear sense of the difference between 3NF and the more stringent Boyce–Codd normal form (BCNF). BCNF simply eliminates the third alternative ("Every element of Y \ X, the set difference between Y and X, is a prime attribute.").

An approximation of Codd's definition of 3NF, paralleling the traditional oath to give true evidence in a court of law, was given by Bill Kent: "[every] non-key [attribute] must provide a fact about the key, the whole key, and nothing but the key".[7] A common variation supplements this definition with the oath "so help me Codd".[8]

Requiring existence of "the key" ensures that the table is in 1NF; requiring that non-key attributes be dependent on "the whole key" ensures 2NF; further requiring that non-key attributes be dependent on "nothing but the key" ensures 3NF. While this phrase is a useful mnemonic, the fact that it only mentions a single key means it defines some necessary but not sufficient conditions to satisfy the 2nd and 3rd normal forms. Both 2NF and 3NF are concerned equally with all candidate keys of a table and not just any one key.

Chris Date refers to Kent's summary as "an intuitively attractive characterization" of 3NF and notes that with slight adaptation it may serve as a definition of the slightly stronger Boyce–Codd normal form: "Each attribute must represent a fact about the key, the whole key, and nothing but the key."[9] The 3NF version of the definition is weaker than Date's BCNF variation, as the former is concerned only with ensuring that non-key attributes are dependent on keys. Prime attributes (which are keys or parts of keys) must not be functionally dependent at all; they each represent a fact about the key in the sense of providing part or all of the key itself. (This rule applies only to functionally dependent attributes, as applying it to all attributes would implicitly prohibit composite candidate keys, since each part of any such key would violate the "whole key" clause.)

An example of a table that fails to meet the requirements of 3NF is:

Because each row in the table needs to tell us who won a particular Tournament in a particular Year, the composite key {Tournament, Year} is a minimal set of attributes guaranteed to uniquely identify a row. That is, {Tournament, Year} is a candidate key for the table.

The breach of 3NF occurs because the non-prime attribute (Winner's date of birth) is transitively dependent on the candidate key {Tournament, Year} through the non-prime attribute Winner. The fact that Winner's date of birth is functionally dependent on Winner makes the table vulnerable to logical inconsistencies, as there is nothing to stop the same person from being shown with different dates of birth on different records.

In order to express the same facts without violating 3NF, it is necessary to split the table into two:

Update anomalies cannot occur in these tables, because unlike before, Winner is now a candidate key in the second table, thus allowing only one value for Date of birth for each Winner.

A relation can always be decomposed in third normal form, that is, the relation R is rewritten to projections R1, ..., Rn whose join is equal to the original relation. Further, this decomposition does not lose any functional dependency, in the sense that every functional dependency on R can be derived from the functional dependencies that hold on the projections R1, ..., Rn. What is more, such a decomposition can be computed in polynomial time.[10]

To decompose a relation into 3NF from 2NF, break the table into the canonical cover functional dependencies, then create a relation for every candidate key of the original relation which was not already a subset of a relation in the decomposition.[11]

The definition of 3NF offered by Carlo Zaniolo in 1982, and given above, can be shown to be equivalent to the Codd definition in the following way: Let X → A be a nontrivial FD (i.e. one where X does not contain A) and let A be a non-prime attribute. Also let Y be a candidate key of R. Then Y → X. Therefore, A is not transitively dependent on Y if there is a functional dependency X → Y iff X is a superkey of R.

Most 3NF tables are free of update, insertion, and deletion anomalies. Certain types of 3NF tables, rarely met with in practice, are affected by such anomalies; these are tables which either fall short of Boyce–Codd normal form (BCNF) or, if they meet BCNF, fall short of the higher normal forms 4NF or 5NF.

While 3NF was ideal for machine processing, the segmented nature of the data model can be difficult to consume by a human user. Analytics via query, reporting, and dashboards were often facilitated by a different type of data model that provided pre-calculated analysis such as trend lines, period-to-date calculations (month-to-date, quarter-to-date, year-to-date), cumulative calculations, basic statistics (average, standard deviation, moving averages) and previous period comparisons (year ago, month ago, week ago) e.g. dimensional modeling and beyond dimensional modeling, flattening of stars via Hadoop and data science.[12][13]

“I have lots of experience with SQL and normalized databases, but I’m just a beginner with MongoDB. How do I model a one-to-N relationship?” This is one of the more common questions I get from users attending MongoDB office hours.
I don’t have a short answer to this question, because there isn’t just one way, there’s a whole rainbow’s worth of ways. MongoDB has a rich and nuanced vocabulary for expressing what, in SQL, gets flattened into the term “One-to-N.” Let me take you on a tour of your choices in modeling One-to-N relationships.
There’s so much to talk about here, In this post, I’ll talk about the three basic ways to model One-to-N relationships. I’ll also cover more sophisticated schema designs, including denormalization and two-way referencing. And I’ll review the entire rainbow of choices, and give you some suggestions for choosing among the thousands (really, thousands) of choices that you may consider when modeling a single One-to-N relationship. Jump the end of the post for an explanation of what database denormalization is and when and when not to denormalize data.
Many beginners think that the only way to model “One-to-N” in MongoDB is to embed an array of sub-documents into the parent document, but that’s just not true. Just because you can embed a document, doesn’t mean you should embed a document.
When designing a MongoDB schema, you need to start with a question that you’d never consider when using SQL and normalized tables: What is the cardinality of the relationship? Put less formally: You need to characterize your “One-to-N” relationship with a bit more nuance: Is it “one-to-few,” “one-to-many,” or “one-to-squillions”? Depending on which one it is, you’d use a different format to model the relationship.
An example of “one-to-few” might be the addresses for a person. This is a good use case for embedding. You’d put the addresses in an array inside of your Person object:
This design has all of the advantages and disadvantages of embedding. The main advantage is that you don’t have to perform a separate query to get the embedded details; the main disadvantage is that you have no way of accessing the embedded details as stand-alone entities.
For example, if you were modeling a task-tracking system, each Person would have a number of Tasks assigned to them. Embedding Tasks inside the Person document would make queries like “Show me all Tasks due tomorrow” much more difficult than they need to be. I will cover a more appropriate design for retrieving data for this use case later in the post.
An example of “one-to-many” might be parts for a product in a replacement parts ordering system. Each product may have up to several hundred replacement parts, but never more than a couple thousand or so. (All of those different-sized bolts, washers, and gaskets add up.) This is a good use case for referencing. You’d put the ObjectIDs of the parts in an array in product document. (For these examples I’m using 2-byte ObjectIDs because they’re easier to read. Real-world code would use 12-byte ObjectIDs.)
Each Part would have its own document:
Each Product would have its own document, which would contain an array of ObjectID references to the Parts that make up that Product:
You would then use an application-level join to retrieve the parts for a particular product:
For efficient operation, you’d need to have an index on "products.catalog_number." Note that there will always be an index on "parts._id," so that query will always be efficient.
This style of referencing has a complementary set of advantages and disadvantages to embedding. Each Part is a stand-alone document, so it’s easy to search them and update them independently. One trade off for using this schema is having to perform a second query to get details about the Parts for a Product. (But hold that thought until we get to denormalization.)
As an added bonus, this schema lets you have individual Parts used by multiple Products, so your One-to-N schema just became an N-to-N schema without any need for a join table!
An example of “one-to-squillions” might be an event logging system that collects log messages for different machines. Any given host could generate enough messages to overflow the 16 MB document size, even if all you stored in the array was the ObjectID. This is the classic use case for “parent-referencing.” You’d have a document for the host, and then store the ObjectID of the host in the documents for the log messages.
You’d use a (slightly different) application-level join to find the most recent 5,000 messages for a host:
So, even at this basic level, there is more to think about when designing a MongoDB schema than when designing a comparable relational database schema for a normalized database. You need to consider two factors:
Will the entities on the “N” side of the One-to-N ever need to stand alone?
What is the cardinality of the relationship: Is it one-to-few; one-to-many; or one-to-squillions?
Based on these factors, you can pick one of the three basic One-to-N schema designs:
Embed the N side if the cardinality is one-to-few and there is no need to access the embedded object outside the context of the parent object.
Use an array of references to the N-side objects if the cardinality is one-to-many or if the N-side objects should stand alone for any reasons.
Use a reference to the One-side in the N-side objects if the cardinality is one-to-squillions.
If you want to get a little bit fancier, you can combine two techniques and include both styles of reference in your schema, having both references from the “one” side to the “many” side and references from the “many” side to the “one” side.
For an example, let’s go back to that task-tracking system. There’s a “people” collection holding Person documents, a “tasks” collection holding Task documents, and a One-to-N relationship from Person to Task. The application will need to track all of the Tasks owned by a Person, so we will need to reference Person to Task.
With the array of references to Task documents, a single Person document might look like this:
On the other hand, in some other contexts this application will display a list of Tasks (for example, all of the Tasks in a multi-person Project) and it will need to quickly find which Person is responsible for each Task. You can optimize data retrieval for this purpose by putting an additional reference to the Person in the Task document.
This design has all of the advantages and disadvantages of the “One-to-Many” schema, but with some additions. Putting in the extra "owner" reference into the Task document means that its quick and easy to find the task’s owner, but it also means that if you need to reassign the task to another person, you need to perform two updates instead of just one. Specifically, you’ll have to update both the reference from the Person to the Task document, and the reference from the Task to the Person. (And to the relational database gurus who are reading this, you’re right; using this schema design over a normalized database model means that it is no longer possible to reassign a Task to a new Person with a single atomic update. This is OK for our task-tracking system; you need to consider if this works with your particular use case.)
Beyond just modeling the various flavors of relationships, you can also add denormalization into your schema. This can eliminate the need to perform the application-level join for certain cases, at the price of some additional complexity when performing updates. An example will help make this clear.
For the parts example, you could denormalize the name of the part into the ‘parts[]’ array. For reference, here’s the version of the Product document without denormalization.
Denormalization would mean that you don’t have to perform the application-level join when displaying all of the part names for the product, but you would have to perform that join if you needed any other information about a part.
While making it easier to get the part names, this would add just a bit of client-side work to the application-level join:
Denormalization saves you a lookup of the denormalized data at the cost of a more expensive update since you're adding a little data redundancy to the database: If you’ve denormalized the Part name into the Product document, then when you update the Part name you must also update every place it occurs in the "products" collection.
Denormalization only makes sense when there’s an high ratio of reads to updates. If you’ll be reading the denormalized data frequently, but updating it only rarely, it often makes sense to pay the price of slower write performance—and more complex updates for redundant data—in order to get more efficient query performance. As updates become more frequent relative to queries, the savings from denormalization decreases.
For instance, assume the part name changes infrequently, but the quantity on hand changes frequently. This means that while it makes sense to denormalize the part name into the Product document, for data integrity purposes, it does not make sense to denormalize the quantity on hand.
Also note that if you denormalize a field, you lose the ability to perform atomic and isolated updates on that field. Just like with two-way referencing, if you update the part name in the Part document, and then in the Product document, data anomalies could occur since there will be a sub-second interval where the denormalized name in the Product document will not reflect the new, updated value in the Part document.
You can also denormalize fields from the “one” side into the “many” side:
However, if you’ve denormalized the Product name into the Part document, then when you update the Product name you must also update every place it occurs in the ‘parts’ collection to avoid data anomalies. This is likely to be a more expensive update, since you’re updating multiple Parts instead of a single Product. As such, it’s significantly more important to consider the read-to-write ratio when denormalizing in this way.
You can also denormalize the one-to-squillions relationship. This works in one of two ways: you can either put information about the “one” side (from the "hosts" document) into the “squillions” side (the log entries), or you can put summary information from the “squillions” side into the “one” side.
Here’s an example of denormalization into the “squillions” side. I’m going to add the IP address of the host (from the ‘one’ side) into the individual log message:
Your query for the most recent messages from a particular IP address just got easier: It’s now just one query instead of two.
In fact, if there’s only a limited amount of information you want to store at the “one” side, you can denormalize it all into the “squillions” side and get rid of the “one” collection altogether:
On the other hand, you can also denormalize into the “one” side. Let's say you want to keep the last 1,000 messages from a host in the "Hosts" document. You could use the $each / $slice functionality introduced in MongoDB 2.4 to keep that list sorted, and only retain the last 1,000 messages:
The log messages get saved in the "logmsg" collection as well as in the denormalized list in the 'hosts’ document. That way the message isn’t lost when it ages out of the "hosts.logmsgs" array.
Note the use of the projection specification ( {_id:1} ) to prevent MongoDB from having to ship the entire "hosts" document over the network. By telling MongoDB to only return the _id field, you reduce the network overhead down to just the few bytes that it takes to store that field (plus just a little bit more for the wire protocol overhead).
Just as with denormalization in the “One-to-Many” case, you’ll want to consider the ratio of reads to updates. Denormalization of the log messages into the "Host" document makes sense only if log messages are infrequent relative to the number of times the application needs to look at all of the messages for a single host. This particular denormalization is a bad idea if you want to look at the data less frequently than you update it.
In this section, I’ve covered the additional choices that you have past the basics of embed, child-reference, or parent-reference.
You can use bi-directional referencing if it optimizes your schema, and if you are willing to pay the price of not having atomic updates.
If you are referencing, you can denormalize data either from the “one” side into the “N” side, or from the “N” side into the “one” side.
When deciding on database denormalization, consider the following factors:
You cannot perform an atomic update on denormalized data.
Denormalization only makes sense when you have a high read-to-write ratio.
Database Denormalization, in particular, gives you a lot of choices: if there are 8 candidates for denormalization in a relationship, there are 2 8 (1,024) different ways to denormalize (including no denormalization at all). Multiply that by the three different ways to do referencing, and you have over 3,000 different ways to model the relationship.
Guess what? You now are stuck in the “paradox of choice." Because you have so many potential ways to model a “one-to-N” relationship, your choice on how to model it just got a lot harder.
Here are some “rules of thumb” to guide you through these innumerable (but not infinite) choices:
One: Favor embedding unless there is a compelling reason not to.
Two: Needing to access an object on its own is a compelling reason not to embed it.
Three: Arrays should not grow without bound. If there are more than a couple of hundred documents on the “many” side, don’t embed them; if there are more than a few thousand documents on the “many” side, don’t use an array of ObjectID references. High-cardinality arrays are a compelling reason not to embed.
Four: Don’t be afraid of application-level joins: If you index correctly and use the projection specifier, then application-level joins are barely more expensive than server-side joins in a relational database.
Five: Consider the read-to-write ratio with denormalization. A field that will mostly be read and only seldom updated is a good candidate for denormalization. If you denormalize a field that is updated frequently then the extra work of finding and updating all the instances of redundant data is likely to overwhelm the savings that you get from denormalization.
Six: As always with MongoDB, how you model your data depends entirely on your particular application’s data access patterns. You want to structure your data to match the ways that your application queries and updates it.
When modeling “One-to-N” relationships in MongoDB, you have a variety of choices, so you have to carefully think through the structure of your data. The main criteria you need to consider are:
What is the cardinality of the relationship? Is it “one-to-few,” “one-to-many,” or “one-to-squillions”?
Do you need to access the object on the “N” side separately, or only in the context of the parent object?
What is the ratio of updates-to-reads for a particular field?
Your main choices for structuring the data are:
For “one-to-few,” you can use an array of embedded documents.
For “one-to-many,” or on occasions when the “N” side must stand alone, you should use an array of references. You can also use a “parent-reference” on the “N” side if it optimizes your data access pattern.
For “one-to-squillions,” you should use a “parent-reference” in the document storing the “N” side.
Once you’ve decided on the overall structure of the data in your database design, then you can, if you choose, denormalize data across multiple documents, by either denormalizing data from the “One” side into the “N” side, or from the “N” side into the “One” side. You’d do this only for fields that are frequently read, get read much more often than they get updated, and where you don’t require strong consistency, since updating a denormalized value is slower, more expensive, and is not atomic.
The upshot of all of this is that MongoDB gives you the ability to design your database schema to match the needs of your application. You can structure your data in MongoDB so that it adapts easily to change, and supports the queries and updates that you need to get the most out of your application.
There is a very simple principle behind database denormalization techniques and it is this: Data that is accessed together should be stored together. Denormalization is the process of duplicating fields or deriving new fields from existing ones. Denormalized databases can improve read performance and query performance in a variety of cases, such as:
A recurring query requires a few fields from a large document in another collection. You may choose to maintain a copy of those fields in an embedded document in the collection that the recurring query targets to avoid merging two distinct collections or performing frequent $lookup operations.
An average value of some field in a collection is frequently requested. You may choose to create a derived field in a separate collection that is updated as part of your writes and maintains a running average for that field.
While embedding documents or arrays without data duplication is preferred for grouping related data, denormalization can improve read performance when separate collections must be maintained.
A single document can represent an entire customer order or the energy production for a day for a specific solar panel. Some users coming from the relational database who are more familiar with a normalized database model world treat the document as a row in a table or spread across multiple tables. While nothing is stopping you from architecting your schema this way, it isn’t the more efficient way to store data or query large amounts of data, especially IoT data.
Denormalization enables you to increase performance of the database while having fewer joins compared with the normalized database model of a relational database.
Although MongoDB supports multi-document transactions for replica sets (starting in version 4.0) and sharded clusters (starting in version 4.2), for many scenarios, the denormalized database model will continue to be optimal for your data and use cases.
Note that with a denormalized database, it's important to maintain consistent duplicate data . However, in most cases, the increase in data retrieval performance and query execution will outweigh the presence of redundant copies of data and the need to avoid data inconsistency.
Denormalization makes sense when you have a high read-to-write ratio. With denormalization you can avoid costly joins, at the expense of having more complex and expensive updates. Therefore, you should practice denormalization on only those fields that are read most often and are seldom updated since data redundancy is less of an issue.
$lookup operations join data from two collections in the same database based on a specified field. $lookup operations can be useful when your data is structured similarly to a relational database and you need to model large hierarchical datasets that wold normally be spread across multiple tables. However, these operations can be slow and resource-intensive because they need to read and perform logic on two collections instead of a single collection.
If you frequently run $lookup operations, consider restructuring your schema through denormalization such that the your application can query a single collection to obtain all of the information it needs. Use embedded documents and arrays to capture relationships between data in a single document structure. Use database denormalization to take advantage of MongoDB’s rich document model, which allows your application to retrieve and manipulate related data in a single query execution.
Typically it is most advantageous to embrace database denormalization for operational databases—the efficiency of reading or writing an entire record in a single operation outweighing any modest increase in storage requirements.
Normalized data models describe relationships using references between documents. In general, use normalized data models in the following scenarios:
When embedding would result in duplication of data but would not provide sufficient read performance advantages to outweigh the implications of data duplication.
To represent more complex many-to-many relationships.
To model large hierarchical data sets.
Today, there's a wide range of database design options in today's market. The relational database model and the practice of database normalization has its merits and limitations. The need to perform join operations across tables impedes performance, inhibits scaling, and introduces technical and cognitive overhead. Developers often create workarounds in their databases to achieve efficiency advantages. Those applications based on the relational databases that are performant will often incorporate ad hoc denormalization, materialized views, and external caching layers to get around the limitations of the normalized relational database.
Schema Design and Consulting Services
Thinking in Documents (recorded webinar)
Run MongoDB in the cloud for free with MongoDB Atlas. No credit card required.

One of the most common scenarios for needing to go to a backup is human error. Someone releases buggy code to production. You drop a collection by accident. Some hackers breached your internal network and dumped political slogans all over your data. Or maybe you’re just worried about such a thing happening, and you want to try starting over, just in case the worst should happen.


Note: Before being able to restore, you would have to configure how to receive the backup files, either HTTPS “pull” or SCP “push” restore formats. If you’re not sure about using HTTPS vs SCP, watch this 
tutorial
.


Now that you’ve decided what format you’d like to use, make sure that whatever machine will be downloading the snapshot has sufficient disk space. On a Linux server, this can be done by running 
df -h
. In Windows, this information can be found in My Computer by right-clicking on the drive in question. In OSX, you can option-click the relevant hard drive in any Finder window. Then compare it to the size in your Sharded Cluster Status or Replica Set Status page in MMS.


Once you&rsquo;ve decided on the delivery method, and know you have enough disk space, you can retrieve the snapshot. If you’re running sharded clusters, you can find your snapshots in your Sharded Cluster Status page. If you’re running replica sets, you can find them in your Replica Set Status page. In either case, you then click on the replica set or shard you want to restore and you will see your snapshots. Click on the “restore this snapshot” link for the snapshot you wish to restore. The popup will give you the ability to select the delivery method, and in the case of SCP, test it.


To restore your data, you can either:




Restore from a Stored Snapshot
, which is the fastest restoration method to restore historical information.  Data from the stored snapshot will be slightly older than the very latest data received by the backup service, however, depending on the cause of the data corruption this might be acceptable or even ideal.



Restore from a point in time in the last 24 hours.
 If you’re sure that your corruption issue took place within the last 24 hours, you can restore from a snapshot prior to when the failure occurred.  MMS will then generate a new snapshot at your request based on the your selection via the UI.  If the data corruption is from before 24 hours ago, you must restore from a stored snapshot (option #1).



If only one collection or database is corrupted, you may opt to use the 
mongodump
 utility in combination with your backup snapshots to pull out the data you need.  Once extracted, the data can be imported into a running mongod using 
mongorestore
.


To have this same functionality for sharded clusters, you should configure cluster checkpoints for MongoDB by going to the Sharded Cluster node within MMS Backup and clicking the gear icon that appears next to the cluster. These checkpoints can be configured to take place every 15, 30, or 60 minutes, and you can also decide how long to store each clustershot.



Monoova
 is a fast-growing Australian Fintech scale-up providing real-time payment solutions to businesses. Having grown from 200,000 to 6 million business accounts in 5 years, Monoova has also created 13% of Australia’s PayID since the inception of NPP, and processed over $100 billion in payments.


Dealing with critical financial information from hundreds of organisations in highly regulated industries means Monoova needs to put data security and compliance first.


But this is easier said than done.


Navigating an increasingly complex security and compliance landscape


The increased reliance on the cloud, combined with more regulations requiring extra resilience capabilities, means that financial services organisations are facing increasingly complex data security and compliance challenges.


APRA’s chair John Lonsdale recently warned the financial sector about cybersecurity non-compliance, mentioning 
CPS 230 as one of the upcoming regulations organisations
 should start preparing to comply with ahead of the July 2025 deadline.


On November 3rd, 2023, Brad Jones, Assistant Governor of the RBA gave a speech in which he listed the “Outside Operational Risk (Cloud Concentration Risk)” as 
one of the main threats to financial stability
.


“Multi” “cloud” or “multi-cloud”?


A significant number of financial services institutions today aren’t using multi-cloud in a way that would make them resilient in the event of a data security or outage issue.


Many say that they are using “multi-cloud” but what they are doing is hosting individual workloads and data sets in different clouds, which doesn’t provide full resilience and data security or meet requirements from regulating industry bodies and government.


True multi-cloud resilience means having critical data hosted in different clouds 
at the same time.



For Monoova’s CTO, Nicholas Tan, future-proofing compliance and data security lies in adopting a true multi-cloud approach, and this is exactly the path Monoova has taken by working with MongoDB Atlas.


“Whether it’s in critical sectors like financial services or telecommunications, time and time again we see events such as outages seriously impact Australian organisations and their sometimes millions of users - the 
Optus outage from November 2023
 is a perfect example," explains Nicholas Tan.


“There are great operational risks of not having diversity in an organisations’ core infrastructure, and this is why building real resilience with a proper multi-cloud approach should be a no-brainer.”


Working with MongoDB Atlas a game changer


MongoDB Atlas, which is the operational database underpinning all of Monoova’s services, was chosen by Monoova to support extra scale requirements as it was - and still is fast growing, as well as empower developer productivity as the company needs to bring new products to market and innovate fast.


Another key driver in working with MongoDB Atlas was the unique functionality that enables users to simply “turn a switch on” to automatically enable selected critical data and workloads to go multi-cloud, allowing data to be easily distributed across different clouds and providing resilience and protection for the workload.


Tan and his team simply have to choose the UI to reconfigure their posture to be multi-cloud, which takes only a few minutes.










Monoova’s MongoDB Atlas multi-cloud console  




According to Tan: “Our multi-cloud approach is pretty unique in the current Australian financial services landscape, and this is what has set Monoova to be one of the first Australian financial services organisations to align with the CPS230 framework, as well as at the forefront of ensuring compliance and resilience in an environment heavily reliant on third parties.”


“Working with MongoDB has been a game changer because it means we were able to quickly scale up at a fraction of the manpower required; it saved us from recruiting a full team if we had had to do all that multi-cloud work in-house.”




Learn more about MongoDB Atlas and our multi-cloud feature on our 
resources page
.




Failed to fetch https://http://bit.ly/mongodb-schema-design: HTTPSConnectionPool(host='http', port=443): Max retries exceeded with url: //bit.ly/mongodb-schema-design (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f20e9350eb0>: Failed to resolve 'http' ([Errno -2] Name or service not known)"))
Failed to fetch https://http://npmjs.com/package/mongodb: HTTPSConnectionPool(host='http', port=443): Max retries exceeded with url: //npmjs.com/package/mongodb (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f20e9350670>: Failed to resolve 'http' ([Errno -2] Name or service not known)"))
