This method returns a String value. The contents of the String are implementation-defined, but are intended to represent the Date in the current time zone in a convenient, human-readable form that corresponds to the conventions of the host environment's current locale.
The meaning of the optional parameters to this method are defined in the ECMA-402 specification; implementations that do not include ECMA-402 support must not use those parameter positions for anything else.
An ECMAScript implementation that includes the ECMA-402 Internationalization API must implement this method as specified in the ECMA-402 specification. If an ECMAScript implementation does not include the ECMA-402 API the following specification of this method is used:
This method returns a String value. The contents of the String are implementation-defined, but are intended to represent the ‚Äútime‚Äù portion of the Date in the current time zone in a convenient, human-readable form that corresponds to the conventions of the host environment's current locale.
The meaning of the optional parameters to this method are defined in the ECMA-402 specification; implementations that do not include ECMA-402 support must not use those parameter positions for anything else.
This method performs the following steps when called:
For any Date d such that d.[[DateValue]] is evenly divisible by 1000, the result of Date.parse(d.toString()) = d.valueOf(). See 21.4.3.2.
This method is not generic; it throws a TypeError exception if its this value is not a Date. Therefore, it cannot be transferred to other kinds of objects for use as a method.
The abstract operation TimeString takes argument tv (a Number, but not NaN) and returns a String. It performs the following steps when called:
The abstract operation DateString takes argument tv (a Number, but not NaN) and returns a String. It performs the following steps when called:
The abstract operation TimeZoneString takes argument tv (an integral Number) and returns a String. It performs the following steps when called:
The abstract operation ToDateString takes argument tv (an integral Number or NaN) and returns a String. It performs the following steps when called:
This method performs the following steps when called:
This method returns a String value representing the instant in time corresponding to the this value. The format of the String is based upon "HTTP-date" from RFC 7231, generalized to support the full range of times supported by ECMAScript Dates.
It performs the following steps when called:
This method performs the following steps when called:
This method is called by ECMAScript language operators to convert a Date to a primitive value. The allowed values for hint are "default", "number", and "string". Dates are unique among built-in ECMAScript object in that they treat "default" as being equivalent to "string", All other built-in ECMAScript objects treat "default" as being equivalent to "number".
It performs the following steps when called:
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The value of the "name" property of this method is "[Symbol.toPrimitive]".
Date instances are ordinary objects that inherit properties from the Date prototype object. Date instances also have a [[DateValue]] internal slot. The [[DateValue]] internal slot is the time value represented by this Date.
The String constructor:
This function performs the following steps when called:
The String constructor:
This function may be called with any number of arguments which form the rest parameter codeUnits.
It performs the following steps when called:
The "length" property of this function is 1ùîΩ.
This function may be called with any number of arguments which form the rest parameter codePoints.
It performs the following steps when called:
The "length" property of this function is 1ùîΩ.
The initial value of String.prototype is the String prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
This function may be called with a variable number of arguments. The first argument is template and the remainder of the arguments form the List substitutions.
It performs the following steps when called:
This function is intended for use as a tag function of a Tagged Template (13.3.11). When called as such, the first argument will be a well formed template object and the rest parameter will contain the substitution values.
The String prototype object:
Unless explicitly stated otherwise, the methods of the String prototype object defined below are not generic and the this value passed to them must be either a String value or an object that has a [[StringData]] internal slot that has been initialized to a String value.
This method returns a single element String containing the code unit at index pos within the String value resulting from converting this object to a String. If there is no element at that index, the result is the empty String. The result is a String value, not a String object.
If pos is an integral Number, then the result of x.charAt(pos) is equivalent to the result of x.substring(pos, pos + 1).
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method returns a Number (a non-negative integral Number less than 216) that is the numeric value of the code unit at index pos within the String resulting from converting this object to a String. If there is no element at that index, the result is NaN.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore it can be transferred to other kinds of objects for use as a method.
This method returns a non-negative integral Number less than or equal to 0x10FFFFùîΩ that is the numeric value of the UTF-16 encoded code point (6.1.4) starting at the string element at index pos within the String resulting from converting this object to a String. If there is no element at that index, the result is undefined. If a valid UTF-16 surrogate pair does not begin at pos, the result is the code unit at pos.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore it can be transferred to other kinds of objects for use as a method.
When this method is called it returns the String value consisting of the code units of the this value (converted to a String) followed by the code units of each of the arguments converted to a String. The result is a String value, not a String object.
This method performs the following steps when called:
The "length" property of this method is 1ùîΩ.
This method is intentionally generic; it does not require that its this value be a String object. Therefore it can be transferred to other kinds of objects for use as a method.
The initial value of String.prototype.constructor is %String%.
This method performs the following steps when called:
This method returns true if the sequence of code units of searchString converted to a String is the same as the corresponding code units of this object (converted to a String) starting at endPosition - length(this). Otherwise it returns false.
Throwing an exception if the first argument is a RegExp is specified in order to allow future editions to define extensions that allow such argument values.
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
If searchString appears as a substring of the result of converting this object to a String, at one or more indices that are greater than or equal to position, this function returns true; otherwise, it returns false. If position is undefined, 0 is assumed, so as to search all of the String.
Throwing an exception if the first argument is a RegExp is specified in order to allow future editions to define extensions that allow such argument values.
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
If searchString appears as a substring of the result of converting this object to a String, at one or more indices that are greater than or equal to position, then the smallest such index is returned; otherwise, -1ùîΩ is returned. If position is undefined, +0ùîΩ is assumed, so as to search all of the String.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
If searchString appears as a substring of the result of converting this object to a String at one or more indices that are smaller than or equal to position, then the greatest such index is returned; otherwise, -1ùîΩ is returned. If position is undefined, the length of the String value is assumed, so as to search all of the String.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
An ECMAScript implementation that includes the ECMA-402 Internationalization API must implement this method as specified in the ECMA-402 specification. If an ECMAScript implementation does not include the ECMA-402 API the following specification of this method is used:
This method returns a Number other than NaN representing the result of an implementation-defined locale-sensitive String comparison of the this value (converted to a String S) with that (converted to a String thatValue). The result is intended to correspond with a sort order of String values according to conventions of the host environment's current locale, and will be negative when S is ordered before thatValue, positive when S is ordered after thatValue, and zero in all other cases (representing no relative ordering between S and thatValue).
Before performing the comparisons, this method performs the following steps to prepare the Strings:
The meaning of the optional second and third parameters to this method are defined in the ECMA-402 specification; implementations that do not include ECMA-402 support must not assign any other interpretation to those parameter positions.
The actual return values are implementation-defined to permit encoding additional information in them, but this method, when considered as a method of two arguments, is required to be a consistent comparator defining a total ordering on the set of all Strings. This method is also required to recognize and honour canonical equivalence according to the Unicode Standard, including returning +0ùîΩ when comparing distinguishable Strings that are canonically equivalent.
This method itself is not directly suitable as an argument to Array.prototype.sort because the latter requires a function of two arguments.
This method may rely on whatever language- and/or locale-sensitive comparison functionality is available to the ECMAScript environment from the host environment, and is intended to compare according to the conventions of the host environment's current locale. However, regardless of comparison capabilities, this method must recognize and honour canonical equivalence according to the Unicode Standard‚Äîfor example, the following comparisons must all return +0ùîΩ:
For a definition and discussion of canonical equivalence see the Unicode Standard, chapters 2 and 3, as well as Unicode Standard Annex #15, Unicode Normalization Forms and Unicode Technical Note #5, Canonical Equivalence in Applications. Also see Unicode Technical Standard #10, Unicode Collation Algorithm.
It is recommended that this method should not honour Unicode compatibility equivalents or compatibility decompositions as defined in the Unicode Standard, chapter 3, section 3.7.
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method performs a regular expression match of the String representing the this value against regexp and returns an iterator. Each iteration result's value is an Array containing the results of the match, or null if the String did not match.
It performs the following steps when called:
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
This method performs the following steps when called:
The abstract operation StringPaddingBuiltinsImpl takes arguments O (an ECMAScript language value), maxLength (an ECMAScript language value), fillString (an ECMAScript language value), and placement (start or end) and returns either a normal completion containing a String or a throw completion. It performs the following steps when called:
The abstract operation StringPad takes arguments S (a String), maxLength (a non-negative integer), fillString (a String), and placement (start or end) and returns a String. It performs the following steps when called:
The argument maxLength will be clamped such that it can be no smaller than the length of S.
The argument fillString defaults to " " (the String value consisting of the code unit 0x0020 SPACE).
The abstract operation ToZeroPaddedDecimalString takes arguments n (a non-negative integer) and minLength (a non-negative integer) and returns a String. It performs the following steps when called:
This method performs the following steps when called:
This method creates the String value consisting of the code units of the this value (converted to String) repeated count times.
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
The abstract operation GetSubstitution takes arguments matched (a String), str (a String), position (a non-negative integer), captures (a List of either Strings or undefined), namedCaptures (an Object or undefined), and replacementTemplate (a String) and returns either a normal completion containing a String or a throw completion. For the purposes of this abstract operation, a decimal digit is a code unit in the inclusive interval from 0x0030 (DIGIT ZERO) to 0x0039 (DIGIT NINE). It performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method returns a substring of the result of converting this object to a String, starting from index start and running to, but not including, index end (or through the end of the String if end is undefined). If start is negative, it is treated as sourceLength + start where sourceLength is the length of the String. If end is negative, it is treated as sourceLength + end where sourceLength is the length of the String. The result is a String value, not a String object.
It performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore it can be transferred to other kinds of objects for use as a method.
This method returns an Array into which substrings of the result of converting this object to a String have been stored. The substrings are determined by searching from left to right for occurrences of separator; these occurrences are not part of any String in the returned array, but serve to divide up the String value. The value of separator may be a String of any length or it may be an object, such as a RegExp, that has a @@split method.
It performs the following steps when called:
The value of separator may be an empty String. In this case, separator does not match the empty substring at the beginning or end of the input String, nor does it match the empty substring at the end of the previous separator match. If separator is the empty String, the String is split up into individual code unit elements; the length of the result array equals the length of the String, and each substring contains one code unit.
If the this value is (or converts to) the empty String, the result depends on whether separator can match the empty String. If it can, the result array contains no elements. Otherwise, the result array contains one element, which is the empty String.
If separator is undefined, then the result array contains just one String, which is the this value (converted to a String). If limit is not undefined, then the output array is truncated so that it contains no more than limit elements.
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
This method returns true if the sequence of code units of searchString converted to a String is the same as the corresponding code units of this object (converted to a String) starting at index position. Otherwise it returns false.
Throwing an exception if the first argument is a RegExp is specified in order to allow future editions to define extensions that allow such argument values.
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method returns a substring of the result of converting this object to a String, starting from index start and running to, but not including, index end of the String (or through the end of the String if end is undefined). The result is a String value, not a String object.
If either argument is NaN or negative, it is replaced with zero; if either argument is strictly greater than the length of the String, it is replaced with the length of the String.
If start is strictly greater than end, they are swapped.
It performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
An ECMAScript implementation that includes the ECMA-402 Internationalization API must implement this method as specified in the ECMA-402 specification. If an ECMAScript implementation does not include the ECMA-402 API the following specification of this method is used:
This method interprets a String value as a sequence of UTF-16 encoded code points, as described in 6.1.4.
It works exactly the same as toLowerCase except that it is intended to yield a locale-sensitive result corresponding with conventions of the host environment's current locale. There will only be a difference in the few cases (such as Turkish) where the rules for that language conflict with the regular Unicode case mappings.
The meaning of the optional parameters to this method are defined in the ECMA-402 specification; implementations that do not include ECMA-402 support must not use those parameter positions for anything else.
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
An ECMAScript implementation that includes the ECMA-402 Internationalization API must implement this method as specified in the ECMA-402 specification. If an ECMAScript implementation does not include the ECMA-402 API the following specification of this method is used:
This method interprets a String value as a sequence of UTF-16 encoded code points, as described in 6.1.4.
It works exactly the same as toUpperCase except that it is intended to yield a locale-sensitive result corresponding with conventions of the host environment's current locale. There will only be a difference in the few cases (such as Turkish) where the rules for that language conflict with the regular Unicode case mappings.
The meaning of the optional parameters to this method are defined in the ECMA-402 specification; implementations that do not include ECMA-402 support must not use those parameter positions for anything else.
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method interprets a String value as a sequence of UTF-16 encoded code points, as described in 6.1.4.
It performs the following steps when called:
The result must be derived according to the locale-insensitive case mappings in the Unicode Character Database (this explicitly includes not only the file UnicodeData.txt, but also all locale-insensitive mappings in the file SpecialCasing.txt that accompanies it).
The case mapping of some code points may produce multiple code points. In this case the result String may not be the same length as the source String. Because both toUpperCase and toLowerCase have context-sensitive behaviour, the methods are not symmetrical. In other words, s.toUpperCase().toLowerCase() is not necessarily equal to s.toLowerCase().
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
For a String object, this method happens to return the same thing as the valueOf method.
This method interprets a String value as a sequence of UTF-16 encoded code points, as described in 6.1.4.
It behaves in exactly the same way as String.prototype.toLowerCase, except that the String is mapped using the toUppercase algorithm of the Unicode Default Case Conversion.
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method returns a String representation of this object with all leading surrogates and trailing surrogates that are not part of a surrogate pair replaced with U+FFFD (REPLACEMENT CHARACTER).
It performs the following steps when called:
This method interprets a String value as a sequence of UTF-16 encoded code points, as described in 6.1.4.
It performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
The abstract operation TrimString takes arguments string (an ECMAScript language value) and where (start, end, or start+end) and returns either a normal completion containing a String or a throw completion. It interprets string as a sequence of UTF-16 encoded code points, as described in 6.1.4. It performs the following steps when called:
The definition of white space is the union of WhiteSpace and LineTerminator. When determining whether a Unicode code point is in Unicode general category ‚ÄúSpace_Separator‚Äù (‚ÄúZs‚Äù), code unit sequences are interpreted as UTF-16 encoded code point sequences as specified in 6.1.4.
This method interprets a String value as a sequence of UTF-16 encoded code points, as described in 6.1.4.
It performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method interprets a String value as a sequence of UTF-16 encoded code points, as described in 6.1.4.
It performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore, it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
The abstract operation ThisStringValue takes argument value (an ECMAScript language value) and returns either a normal completion containing a String or a throw completion. It performs the following steps when called:
This method returns an Iterator object (27.1.1.2) that iterates over the code points of a String value, returning each code point as a String value.
It performs the following steps when called:
The value of the "name" property of this method is "[Symbol.iterator]".
String instances are String exotic objects and have the internal methods specified for such objects. String instances inherit properties from the String prototype object. String instances also have a [[StringData]] internal slot. The [[StringData]] internal slot is the String value represented by this String object.
String instances have a "length" property, and a set of enumerable properties with integer-indexed names.
The number of elements in the String value represented by this String object.
Once a String object is initialized, this property is unchanging. It has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
A String Iterator is an object, that represents a specific iteration over some specific String instance object. There is not a named constructor for String Iterator objects. Instead, String iterator objects are created by calling certain methods of String instance objects.
The %StringIteratorPrototype% object:
The initial value of the @@toStringTag property is the String value "String Iterator".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
A RegExp object contains a regular expression and the associated flags.
The form and functionality of regular expressions is modelled after the regular expression facility in the Perl 5 programming language.
The RegExp constructor applies the following grammar to the input pattern String. An error occurs if the grammar cannot interpret the String as an expansion of Pattern.
Each \u HexTrailSurrogate for which the choice of associated u HexLeadSurrogate is ambiguous shall be associated with the nearest possible u HexLeadSurrogate that would otherwise have no corresponding \u HexTrailSurrogate.
The first two lines here are equivalent to CharacterClass.
A number of productions in this section are given alternative definitions in section B.1.2.
This section is amended in B.1.2.1.
The abstract operation CountLeftCapturingParensWithin takes argument node (a Parse Node) and returns a non-negative integer. It returns the number of left-capturing parentheses in node. A left-capturing parenthesis is any ( pattern character that is matched by the ( terminal of the 
Atom :: 
(
GroupSpecifieropt
Disjunction
)


 production.
This section is amended in B.1.2.2.
It performs the following steps when called:
The abstract operation CountLeftCapturingParensBefore takes argument node (a Parse Node) and returns a non-negative integer. It returns the number of left-capturing parentheses within the enclosing pattern that occur to the left of node.
This section is amended in B.1.2.2.
It performs the following steps when called:
The syntax-directed operation CapturingGroupNumber takes no arguments and returns a positive integer.
This section is amended in B.1.2.1.
It is defined piecewise over the following productions:
The definitions of ‚Äúthe MV of NonZeroDigit‚Äù and ‚Äúthe MV of DecimalDigits‚Äù are in 12.9.3.
The syntax-directed operation IsCharacterClass takes no arguments and returns a Boolean.
This section is amended in B.1.2.3.
It is defined piecewise over the following productions:
The syntax-directed operation CharacterValue takes no arguments and returns a non-negative integer.
This section is amended in B.1.2.4.
It is defined piecewise over the following productions:
\0 represents the <NUL> character and cannot be followed by a decimal digit.
The syntax-directed operation MayContainStrings takes no arguments and returns a Boolean. It is defined piecewise over the following productions:
The abstract operation GroupSpecifiersThatMatch takes argument thisGroupName (a GroupName Parse Node) and returns a List of GroupSpecifier Parse Nodes. It performs the following steps when called:
The syntax-directed operation CapturingGroupName takes no arguments and returns a String. It is defined piecewise over the following productions:
The syntax-directed operation RegExpIdentifierCodePoints takes no arguments and returns a List of code points. It is defined piecewise over the following productions:
The syntax-directed operation RegExpIdentifierCodePoint takes no arguments and returns a code point. It is defined piecewise over the following productions:
A regular expression pattern is converted into an Abstract Closure using the process described below. An implementation is encouraged to use more efficient algorithms than the ones listed below, as long as the results are the same. The Abstract Closure is used as the value of a RegExp object's [[RegExpMatcher]] internal slot.
A Pattern is a BMP pattern if its associated flags contain neither a u nor a v. Otherwise, it is a Unicode pattern. A BMP pattern matches against a String interpreted as consisting of a sequence of 16-bit values that are Unicode code points in the range of the Basic Multilingual Plane. A Unicode pattern matches against a String interpreted as consisting of Unicode code points encoded using UTF-16. In the context of describing the behaviour of a BMP pattern ‚Äúcharacter‚Äù means a single 16-bit Unicode BMP code point. In the context of describing the behaviour of a Unicode pattern ‚Äúcharacter‚Äù means a UTF-16 encoded code point (6.1.4). In either context, ‚Äúcharacter value‚Äù means the numeric value of the corresponding non-encoded code point.
The syntax and semantics of Pattern is defined as if the source text for the Pattern was a List of SourceCharacter values where each SourceCharacter corresponds to a Unicode code point. If a BMP pattern contains a non-BMP SourceCharacter the entire pattern is encoded using UTF-16 and the individual code units of that encoding are used as the elements of the List.
For example, consider a pattern expressed in source text as the single non-BMP character U+1D11E (MUSICAL SYMBOL G CLEF). Interpreted as a Unicode pattern, it would be a single element (character) List consisting of the single code point U+1D11E. However, interpreted as a BMP pattern, it is first UTF-16 encoded to produce a two element List consisting of the code units 0xD834 and 0xDD1E.
Patterns are passed to the RegExp constructor as ECMAScript String values in which non-BMP characters are UTF-16 encoded. For example, the single character MUSICAL SYMBOL G CLEF pattern, expressed as a String value, is a String of length 2 whose elements were the code units 0xD834 and 0xDD1E. So no further translation of the string would be necessary to process it as a BMP pattern consisting of two pattern characters. However, to process it as a Unicode pattern UTF16SurrogatePairToCodePoint must be used in producing a List whose sole element is a single pattern character, the code point U+1D11E.
An implementation may not actually perform such translations to or from UTF-16, but the semantics of this specification requires that the result of pattern matching be as if such translations were performed.
The descriptions below use the following internal data structures:
A RegExp Record is a Record value used to store information about a RegExp that is needed during compilation and possibly during matching.
It has the following fields:
The syntax-directed operation CompilePattern takes argument rer (a RegExp Record) and returns an Abstract Closure that takes a List of characters and a non-negative integer and returns a MatchResult. It is defined piecewise over the following productions:
A Pattern compiles to an Abstract Closure value. RegExpBuiltinExec can then apply this procedure to a List of characters and an offset within that List to determine whether the pattern would match starting at exactly that offset within the List, and, if it does match, what the values of the capturing parentheses would be. The algorithms in 22.2.2 are designed so that compiling a pattern may throw a SyntaxError exception; on the other hand, once the pattern is successfully compiled, applying the resulting Abstract Closure to find a match in a List of characters cannot throw an exception (except for any implementation-defined exceptions that can occur anywhere such as out-of-memory).
The syntax-directed operation CompileSubpattern takes arguments rer (a RegExp Record) and direction (forward or backward) and returns a Matcher.
This section is amended in B.1.2.5.
It is defined piecewise over the following productions:
The | regular expression operator separates two alternatives. The pattern first tries to match the left Alternative (followed by the sequel of the regular expression); if it fails, it tries to match the right Disjunction (followed by the sequel of the regular expression). If the left Alternative, the right Disjunction, and the sequel all have choice points, all choices in the sequel are tried before moving on to the next choice in the left Alternative. If choices in the left Alternative are exhausted, the right Disjunction is tried instead of the left Alternative. Any capturing parentheses inside a portion of the pattern skipped by | produce undefined values instead of Strings. Thus, for example,
returns the result "a" and not "ab". Moreover,
returns the array
and not
The order in which the two alternatives are tried is independent of the value of direction.
Consecutive Terms try to simultaneously match consecutive portions of Input. When direction is forward, if the left Alternative, the right Term, and the sequel of the regular expression all have choice points, all choices in the sequel are tried before moving on to the next choice in the right Term, and all choices in the right Term are tried before moving on to the next choice in the left Alternative. When direction is backward, the evaluation order of Alternative and Term are reversed.
The resulting Matcher is independent of direction.
The abstract operation RepeatMatcher takes arguments m (a Matcher), min (a non-negative integer), max (a non-negative integer or +‚àû), greedy (a Boolean), x (a MatchState), c (a MatcherContinuation), parenIndex (a non-negative integer), and parenCount (a non-negative integer) and returns a MatchResult. It performs the following steps when called:
An Atom followed by a Quantifier is repeated the number of times specified by the Quantifier. A Quantifier can be non-greedy, in which case the Atom pattern is repeated as few times as possible while still matching the sequel, or it can be greedy, in which case the Atom pattern is repeated as many times as possible while still matching the sequel. The Atom pattern is repeated rather than the input character sequence that it matches, so different repetitions of the Atom can match different input substrings.
If the Atom and the sequel of the regular expression all have choice points, the Atom is first matched as many (or as few, if non-greedy) times as possible. All choices in the sequel are tried before moving on to the next choice in the last repetition of Atom. All choices in the last (nth) repetition of Atom are tried before moving on to the next choice in the next-to-last (n - 1)st repetition of Atom; at which point it may turn out that more or fewer repetitions of Atom are now possible; these are exhausted (again, starting with either as few or as many as possible) before moving on to the next choice in the (n - 1)st repetition of Atom and so on.
Compare
which returns "abcde" with
which returns "abc".
Consider also
which, by the choice point ordering above, returns the array
and not any of:
The above ordering of choice points can be used to write a regular expression that calculates the greatest common divisor of two numbers (represented in unary notation). The following example calculates the gcd of 10 and 15:
which returns the gcd in unary notation "aaaaa".
Step 4 of the RepeatMatcher clears Atom's captures each time Atom is repeated. We can see its behaviour in the regular expression
which returns the array
and not
because each iteration of the outermost * clears all captured Strings contained in the quantified Atom, which in this case includes capture Strings numbered 2, 3, 4, and 5.
Step 2.b of the RepeatMatcher states that once the minimum number of repetitions has been satisfied, any more expansions of Atom that match the empty character sequence are not considered for further repetitions. This prevents the regular expression engine from falling into an infinite loop on patterns such as:
or the slightly more complicated:
which returns the array
The abstract operation EmptyMatcher takes no arguments and returns a Matcher. It performs the following steps when called:
The abstract operation MatchTwoAlternatives takes arguments m1 (a Matcher) and m2 (a Matcher) and returns a Matcher. It performs the following steps when called:
The abstract operation MatchSequence takes arguments m1 (a Matcher), m2 (a Matcher), and direction (forward or backward) and returns a Matcher. It performs the following steps when called:
The syntax-directed operation CompileAssertion takes argument rer (a RegExp Record) and returns a Matcher.
This section is amended in B.1.2.6.
It is defined piecewise over the following productions:
Even when the y flag is used with a pattern, ^ always matches only at the beginning of Input, or (if rer.[[Multiline]] is true) at the beginning of a line.
The form (?= Disjunction ) specifies a zero-width positive lookahead. In order for it to succeed, the pattern inside Disjunction must match at the current position, but the current position is not advanced before matching the sequel. If Disjunction can match at the current position in several ways, only the first one is tried. Unlike other regular expression operators, there is no backtracking into a (?= form (this unusual behaviour is inherited from Perl). This only matters when the Disjunction contains capturing parentheses and the sequel of the pattern contains backreferences to those captures.
For example,
matches the empty String immediately after the first b and therefore returns the array:
To illustrate the lack of backtracking into the lookahead, consider:
This expression returns
and not:
The form (?! Disjunction ) specifies a zero-width negative lookahead. In order for it to succeed, the pattern inside Disjunction must fail to match at the current position. The current position is not advanced before matching the sequel. Disjunction can contain capturing parentheses, but backreferences to them only make sense from within Disjunction itself. Backreferences to these capturing parentheses from elsewhere in the pattern always return undefined because the negative lookahead must fail for the pattern to succeed. For example,
looks for an a not immediately followed by some positive number n of a's, a b, another n a's (specified by the first \2) and a c. The second \2 is outside the negative lookahead, so it matches against undefined and therefore always succeeds. The whole expression returns the array:
The abstract operation IsWordChar takes arguments rer (a RegExp Record), Input (a List of characters), and e (an integer) and returns a Boolean. It performs the following steps when called:
The syntax-directed operation CompileQuantifier takes no arguments and returns a Record with fields [[Min]] (a non-negative integer), [[Max]] (a non-negative integer or +‚àû), and [[Greedy]] (a Boolean). It is defined piecewise over the following productions:
The syntax-directed operation CompileQuantifierPrefix takes no arguments and returns a Record with fields [[Min]] (a non-negative integer) and [[Max]] (a non-negative integer or +‚àû). It is defined piecewise over the following productions:
The syntax-directed operation CompileAtom takes arguments rer (a RegExp Record) and direction (forward or backward) and returns a Matcher.
This section is amended in B.1.2.7.
It is defined piecewise over the following productions:
Parentheses of the form ( Disjunction ) serve both to group the components of the Disjunction pattern together and to save the result of the match. The result can be used either in a backreference (\ followed by a non-zero decimal number), referenced in a replace String, or returned as part of an array from the regular expression matching Abstract Closure. To inhibit the capturing behaviour of parentheses, use the form (?: Disjunction ) instead.
An escape sequence of the form \ followed by a non-zero decimal number n matches the result of the nth set of capturing parentheses (22.2.2.1). It is an error if the regular expression has fewer than n capturing parentheses. If the regular expression has n or more capturing parentheses but the nth one is undefined because it has not captured anything, then the backreference always succeeds.
The abstract operation CharacterSetMatcher takes arguments rer (a RegExp Record), A (a CharSet), invert (a Boolean), and direction (forward or backward) and returns a Matcher. It performs the following steps when called:
The abstract operation BackreferenceMatcher takes arguments rer (a RegExp Record), n (a positive integer), and direction (forward or backward) and returns a Matcher. It performs the following steps when called:
The abstract operation Canonicalize takes arguments rer (a RegExp Record) and ch (a character) and returns a character. It performs the following steps when called:
In case-insignificant matches when HasEitherUnicodeFlag(rer) is true, all characters are implicitly case-folded using the simple mapping provided by the Unicode Standard immediately before they are compared. The simple mapping always maps to a single code point, so it does not map, for example, √ü (U+00DF LATIN SMALL LETTER SHARP S) to ss or SS. It may however map code points outside the Basic Latin block to code points within it‚Äîfor example, ≈ø (U+017F LATIN SMALL LETTER LONG S) case-folds to s (U+0073 LATIN SMALL LETTER S) and ‚Ñ™ (U+212A KELVIN SIGN) case-folds to k (U+006B LATIN SMALL LETTER K). Strings containing those code points are matched by regular expressions such as /[a-z]/ui.
In case-insignificant matches when HasEitherUnicodeFlag(rer) is false, the mapping is based on Unicode Default Case Conversion algorithm toUppercase rather than toCasefold, which results in some subtle differences. For example, ‚Ñ¶ (U+2126 OHM SIGN) is mapped by toUppercase to itself but by toCasefold to œâ (U+03C9 GREEK SMALL LETTER OMEGA) along with Œ© (U+03A9 GREEK CAPITAL LETTER OMEGA), so "\u2126" is matched by /[œâ]/ui and /[\u03A9]/ui but not by /[œâ]/i or /[\u03A9]/i. Also, no code point outside the Basic Latin block is mapped to a code point within it, so strings such as "\u017F ≈ø" and "\u212A ‚Ñ™" are not matched by /[a-z]/i.
The syntax-directed operation CompileCharacterClass takes argument rer (a RegExp Record) and returns a Record with fields [[CharSet]] (a CharSet) and [[Invert]] (a Boolean). It is defined piecewise over the following productions:
The syntax-directed operation CompileToCharSet takes argument rer (a RegExp Record) and returns a CharSet.
This section is amended in B.1.2.8.
It is defined piecewise over the following productions:
ClassContents can expand into a single ClassAtom and/or ranges of two ClassAtom separated by dashes. In the latter case the ClassContents includes all characters between the first ClassAtom and the second ClassAtom, inclusive; an error occurs if either ClassAtom does not represent a single character (for example, if one is \w) or if the first ClassAtom's character value is strictly greater than the second ClassAtom's character value.
Even if the pattern ignores case, the case of the two ends of a range is significant in determining which characters belong to the range. Thus, for example, the pattern /[E-F]/i matches only the letters E, F, e, and f, while the pattern /[E-f]/i matches all uppercase and lowercase letters in the Unicode Basic Latin block as well as the symbols [, \, ], ^, _, and `.
A - character can be treated literally or it can denote a range. It is treated literally if it is the first or last character of ClassContents, the beginning or end limit of a range specification, or immediately follows a range specification.
A ClassAtom can use any of the escape sequences that are allowed in the rest of the regular expression except for \b, \B, and backreferences. Inside a CharacterClass, \b means the backspace character, while \B and backreferences raise errors. Using a backreference inside a ClassAtom causes an error.
The result will often consist of two or more ranges. When UnicodeSets is true and IgnoreCase is true, then MaybeSimpleCaseFolding(rer, [ƒÄ-ƒç]) will include only the odd-numbered code points of that range.
The abstract operation CharacterRange takes arguments A (a CharSet) and B (a CharSet) and returns a CharSet. It performs the following steps when called:
The abstract operation HasEitherUnicodeFlag takes argument rer (a RegExp Record) and returns a Boolean. It performs the following steps when called:
The abstract operation WordCharacters takes argument rer (a RegExp Record) and returns a CharSet. Returns a CharSet containing the characters considered "word characters" for the purposes of \b, \B, \w, and \W It performs the following steps when called:
The abstract operation AllCharacters takes argument rer (a RegExp Record) and returns a CharSet. Returns the set of ‚Äúall characters‚Äù according to the regular expression flags. It performs the following steps when called:
The abstract operation MaybeSimpleCaseFolding takes arguments rer (a RegExp Record) and A (a CharSet) and returns a CharSet. If rer.[[UnicodeSets]] is false or rer.[[IgnoreCase]] is false, it returns A. Otherwise, it uses the Simple Case Folding (scf(cp)) definitions in the file CaseFolding.txt of the Unicode Character Database (each of which maps a single code point to another single code point) to map each CharSetElement of A character-by-character into a canonical form and returns the resulting CharSet. It performs the following steps when called:
The abstract operation CharacterComplement takes arguments rer (a RegExp Record) and S (a CharSet) and returns a CharSet. It performs the following steps when called:
The abstract operation UnicodeMatchProperty takes arguments rer (a RegExp Record) and p (ECMAScript source text) and returns a Unicode property name. It performs the following steps when called:
Implementations must support the Unicode property names and aliases listed in Table 67, Table 68, and Table 69. To ensure interoperability, implementations must not support any other property names or aliases.
For example, Script_Extensions (property name) and scx (property alias) are valid, but script_extensions or Scx aren't.
The listed properties form a superset of what UTS18 RL1.2 requires.
The spellings of entries in these tables (including casing) match the spellings used in the file PropertyAliases.txt in the Unicode Character Database. The precise spellings in that file are guaranteed to be stable.
The abstract operation UnicodeMatchPropertyValue takes arguments p (ECMAScript source text) and v (ECMAScript source text) and returns a Unicode property value. It performs the following steps when called:
Implementations must support the Unicode property values and property value aliases listed in PropertyValueAliases.txt for the properties listed in Table 67. To ensure interoperability, implementations must not support any other property values or property value aliases.
For example, Xpeo and Old_Persian are valid Script_Extensions values, but xpeo and Old Persian aren't.
This algorithm differs from the matching rules for symbolic values listed in UAX44: case, white space, U+002D (HYPHEN-MINUS), and U+005F (LOW LINE) are not ignored, and the Is prefix is not supported.
The syntax-directed operation CompileClassSetString takes argument rer (a RegExp Record) and returns a sequence of characters. It is defined piecewise over the following productions:
The abstract operation RegExpCreate takes arguments P (an ECMAScript language value) and F (a String or undefined) and returns either a normal completion containing an Object or a throw completion. It performs the following steps when called:
The abstract operation RegExpAlloc takes argument newTarget (a constructor) and returns either a normal completion containing an Object or a throw completion. It performs the following steps when called:
The abstract operation RegExpInitialize takes arguments obj (an Object), pattern (an ECMAScript language value), and flags (an ECMAScript language value) and returns either a normal completion containing an Object or a throw completion. It performs the following steps when called:
The abstract operation ParsePattern takes arguments patternText (a sequence of Unicode code points), u (a Boolean), and v (a Boolean) and returns a Parse Node or a non-empty List of SyntaxError objects.
This section is amended in B.1.2.9.
It performs the following steps when called:
The RegExp constructor:
This function performs the following steps when called:
If pattern is supplied using a StringLiteral, the usual escape sequence substitutions are performed before the String is processed by this function. If pattern must contain an escape sequence to be recognized by this function, any U+005C (REVERSE SOLIDUS) code points must be escaped within the StringLiteral to prevent them being removed when the contents of the StringLiteral are formed.
The RegExp constructor:
The initial value of RegExp.prototype is the RegExp prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
RegExp[@@species] is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The value of the "name" property of this function is "get [Symbol.species]".
RegExp prototype methods normally use their this value's constructor to create a derived object. However, a subclass constructor may over-ride that default behaviour by redefining its @@species property.
The RegExp prototype object:
The RegExp prototype object does not have a "valueOf" property of its own; however, it inherits the "valueOf" property from the Object prototype object.
The initial value of RegExp.prototype.constructor is %RegExp%.
This method searches string for an occurrence of the regular expression pattern and returns an Array containing the results of the match, or null if string did not match.
It performs the following steps when called:
RegExp.prototype.dotAll is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
RegExp.prototype.flags is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The abstract operation RegExpHasFlag takes arguments R (an ECMAScript language value) and codeUnit (a code unit) and returns either a normal completion containing either a Boolean or undefined, or a throw completion. It performs the following steps when called:
RegExp.prototype.global is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
RegExp.prototype.hasIndices is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
RegExp.prototype.ignoreCase is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
This method performs the following steps when called:
The value of the "name" property of this method is "[Symbol.match]".
The @@match property is used by the IsRegExp abstract operation to identify objects that have the basic behaviour of regular expressions. The absence of a @@match property or the existence of such a property whose value does not Boolean coerce to true indicates that the object is not intended to be used as a regular expression object.
This method performs the following steps when called:
The value of the "name" property of this method is "[Symbol.matchAll]".
RegExp.prototype.multiline is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
This method performs the following steps when called:
The value of the "name" property of this method is "[Symbol.replace]".
This method performs the following steps when called:
The value of the "name" property of this method is "[Symbol.search]".
The "lastIndex" and "global" properties of this RegExp object are ignored when performing the search. The "lastIndex" property is left unchanged.
RegExp.prototype.source is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The abstract operation EscapeRegExpPattern takes arguments P (a String) and F (a String) and returns a String. It performs the following steps when called:
This method returns an Array into which substrings of the result of converting string to a String have been stored. The substrings are determined by searching from left to right for matches of the this value regular expression; these occurrences are not part of any String in the returned array, but serve to divide up the String value.
The this value may be an empty regular expression or a regular expression that can match an empty String. In this case, the regular expression does not match the empty substring at the beginning or end of the input String, nor does it match the empty substring at the end of the previous separator match. (For example, if the regular expression matches the empty String, the String is split up into individual code unit elements; the length of the result array equals the length of the String, and each substring contains one code unit.) Only the first match at a given index of the String is considered, even if backtracking could yield a non-empty substring match at that index. (For example, /a*?/[Symbol.split]("ab") evaluates to the array ["a", "b"], while /a*/[Symbol.split]("ab") evaluates to the array ["","b"].)
If string is (or converts to) the empty String, the result depends on whether the regular expression can match the empty String. If it can, the result array contains no elements. Otherwise, the result array contains one element, which is the empty String.
If the regular expression contains capturing parentheses, then each time separator is matched the results (including any undefined results) of the capturing parentheses are spliced into the output array. For example,
evaluates to the array
If limit is not undefined, then the output array is truncated so that it contains no more than limit elements.
This method performs the following steps when called:
The value of the "name" property of this method is "[Symbol.split]".
This method ignores the value of the "global" and "sticky" properties of this RegExp object.
RegExp.prototype.sticky is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
This method performs the following steps when called:
The returned String has the form of a RegularExpressionLiteral that evaluates to another RegExp object with the same behaviour as this object.
RegExp.prototype.unicode is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
RegExp.prototype.unicodeSets is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The abstract operation RegExpExec takes arguments R (an Object) and S (a String) and returns either a normal completion containing either an Object or null, or a throw completion. It performs the following steps when called:
If a callable "exec" property is not found this algorithm falls back to attempting to use the built-in RegExp matching algorithm. This provides compatible behaviour for code written for prior editions where most built-in algorithms that use regular expressions did not perform a dynamic property lookup of "exec".
The abstract operation RegExpBuiltinExec takes arguments R (an initialized RegExp instance) and S (a String) and returns either a normal completion containing either an Array exotic object or null, or a throw completion. It performs the following steps when called:
The abstract operation AdvanceStringIndex takes arguments S (a String), index (a non-negative integer), and unicode (a Boolean) and returns an integer. It performs the following steps when called:
The abstract operation GetStringIndex takes arguments S (a String) and codePointIndex (a non-negative integer) and returns a non-negative integer. It interprets S as a sequence of UTF-16 encoded code points, as described in 6.1.4, and returns the code unit index corresponding to code point index codePointIndex when such an index exists. Otherwise, it returns the length of S. It performs the following steps when called:
A Match Record is a Record value used to encapsulate the start and end indices of a regular expression match or capture.
Match Records have the fields listed in Table 70.
The abstract operation GetMatchString takes arguments S (a String) and match (a Match Record) and returns a String. It performs the following steps when called:
The abstract operation GetMatchIndexPair takes arguments S (a String) and match (a Match Record) and returns an Array. It performs the following steps when called:
The abstract operation MakeMatchIndicesIndexPairArray takes arguments S (a String), indices (a List of either Match Records or undefined), groupNames (a List of either Strings or undefined), and hasGroups (a Boolean) and returns an Array. It performs the following steps when called:
RegExp instances are ordinary objects that inherit properties from the RegExp prototype object. RegExp instances have internal slots [[OriginalSource]], [[OriginalFlags]], [[RegExpRecord]], and [[RegExpMatcher]]. The value of the [[RegExpMatcher]] internal slot is an Abstract Closure representation of the Pattern of the RegExp object.
Prior to ECMAScript 2015, RegExp instances were specified as having the own data properties "source", "global", "ignoreCase", and "multiline". Those properties are now specified as accessor properties of RegExp.prototype.
RegExp instances also have the following property:
The value of the "lastIndex" property specifies the String index at which to start the next match. It is coerced to an integral Number when used (see 22.2.7.2). This property shall have the attributes { [[Writable]]: true, [[Enumerable]]: false, [[Configurable]]: false }.
A RegExp String Iterator is an object, that represents a specific iteration over some specific String instance object, matching against some specific RegExp instance object. There is not a named constructor for RegExp String Iterator objects. Instead, RegExp String Iterator objects are created by calling certain methods of RegExp instance objects.
The abstract operation CreateRegExpStringIterator takes arguments R (an Object), S (a String), global (a Boolean), and fullUnicode (a Boolean) and returns a Generator. It performs the following steps when called:
The %RegExpStringIteratorPrototype% object:
The initial value of the @@toStringTag property is the String value "RegExp String Iterator".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
Arrays are exotic objects that give special treatment to a certain class of property names. See 10.4.2 for a definition of this special treatment.
The Array constructor:
This function performs the following steps when called:
The Array constructor:
This method performs the following steps when called:
This method is an intentionally generic factory method; it does not require that its this value be the Array constructor. Therefore it can be transferred to or inherited by any other constructors that may be called with a single numeric argument.
This function performs the following steps when called:
This method performs the following steps when called:
This method is an intentionally generic factory method; it does not require that its this value be the Array constructor. Therefore it can be transferred to or inherited by other constructors that may be called with a single numeric argument.
The value of Array.prototype is the Array prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
Array[@@species] is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The value of the "name" property of this function is "get [Symbol.species]".
Array prototype methods normally use their this value's constructor to create a derived object. However, a subclass constructor may over-ride that default behaviour by redefining its @@species property.
The Array prototype object:
The Array prototype object is specified to be an Array exotic object to ensure compatibility with ECMAScript code that was created prior to the ECMAScript 2015 specification.
This method returns an array containing the array elements of the object followed by the array elements of each argument.
It performs the following steps when called:
The "length" property of this method is 1ùîΩ.
The explicit setting of the "length" property in step 6 is intended to ensure the length is correct when the final non-empty element of items has trailing holes or when A is not a built-in Array.
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
The abstract operation IsConcatSpreadable takes argument O (an ECMAScript language value) and returns either a normal completion containing a Boolean or a throw completion. It performs the following steps when called:
The initial value of Array.prototype.constructor is %Array%.
The end argument is optional. If it is not provided, the length of the this value is used.
If target is negative, it is treated as length + target where length is the length of the array. If start is negative, it is treated as length + start. If end is negative, it is treated as length + end.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
callbackfn should be a function that accepts three arguments and returns a value that is coercible to a Boolean value. every calls callbackfn once for each element present in the array, in ascending order, until it finds one where callbackfn returns false. If such an element is found, every immediately returns false. Otherwise, every returns true. callbackfn is called only for elements of the array which actually exist; it is not called for missing elements of the array.
If a thisArg parameter is provided, it will be used as the this value for each invocation of callbackfn. If it is not provided, undefined is used instead.
callbackfn is called with three arguments: the value of the element, the index of the element, and the object being traversed.
every does not directly mutate the object on which it is called but the object may be mutated by the calls to callbackfn.
The range of elements processed by every is set before the first call to callbackfn. Elements which are appended to the array after the call to every begins will not be visited by callbackfn. If existing elements of the array are changed, their value as passed to callbackfn will be the value at the time every visits them; elements that are deleted after the call to every begins and before being visited are not visited. every acts like the "for all" quantifier in mathematics. In particular, for an empty array, it returns true.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
The start argument is optional. If it is not provided, +0ùîΩ is used.
The end argument is optional. If it is not provided, the length of the this value is used.
If start is negative, it is treated as length + start where length is the length of the array. If end is negative, it is treated as length + end.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
callbackfn should be a function that accepts three arguments and returns a value that is coercible to a Boolean value. filter calls callbackfn once for each element in the array, in ascending order, and constructs a new array of all the values for which callbackfn returns true. callbackfn is called only for elements of the array which actually exist; it is not called for missing elements of the array.
If a thisArg parameter is provided, it will be used as the this value for each invocation of callbackfn. If it is not provided, undefined is used instead.
callbackfn is called with three arguments: the value of the element, the index of the element, and the object being traversed.
filter does not directly mutate the object on which it is called but the object may be mutated by the calls to callbackfn.
The range of elements processed by filter is set before the first call to callbackfn. Elements which are appended to the array after the call to filter begins will not be visited by callbackfn. If existing elements of the array are changed their value as passed to callbackfn will be the value at the time filter visits them; elements that are deleted after the call to filter begins and before being visited are not visited.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method calls predicate once for each element of the array, in ascending index order, until it finds one where predicate returns a value that coerces to true. If such an element is found, find immediately returns that element value. Otherwise, find returns undefined.
See FindViaPredicate for additional information.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method calls predicate once for each element of the array, in ascending index order, until it finds one where predicate returns a value that coerces to true. If such an element is found, findIndex immediately returns the index of that element value. Otherwise, findIndex returns -1.
See FindViaPredicate for additional information.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method calls predicate once for each element of the array, in descending index order, until it finds one where predicate returns a value that coerces to true. If such an element is found, findLast immediately returns that element value. Otherwise, findLast returns undefined.
See FindViaPredicate for additional information.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array object. Therefore it can be transferred to other kinds of objects for use as a method.
This method calls predicate once for each element of the array, in descending index order, until it finds one where predicate returns a value that coerces to true. If such an element is found, findLastIndex immediately returns the index of that element value. Otherwise, findLastIndex returns -1.
See FindViaPredicate for additional information.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array object. Therefore it can be transferred to other kinds of objects for use as a method.
The abstract operation FindViaPredicate takes arguments O (an Object), len (a non-negative integer), direction (ascending or descending), predicate (an ECMAScript language value), and thisArg (an ECMAScript language value) and returns either a normal completion containing a Record with fields [[Index]] (an integral Number) and [[Value]] (an ECMAScript language value) or a throw completion.
O should be an array-like object or a TypedArray. This operation calls predicate once for each element of O, in either ascending index order or descending index order (as indicated by direction), until it finds one where predicate returns a value that coerces to true. At that point, this operation returns a Record that gives the index and value of the element found. If no such element is found, this operation returns a Record that specifies -1ùîΩ for the index and undefined for the value.
predicate should be a function. When called for an element of the array, it is passed three arguments: the value of the element, the index of the element, and the object being traversed. Its return value will be coerced to a Boolean value.
thisArg will be used as the this value for each invocation of predicate.
This operation does not directly mutate the object on which it is called, but the object may be mutated by the calls to predicate.
The range of elements processed is set before the first call to predicate, just before the traversal begins. Elements that are appended to the array after this will not be visited by predicate. If existing elements of the array are changed, their value as passed to predicate will be the value at the time that this operation visits them. Elements that are deleted after traversal begins and before being visited are still visited and are either looked up from the prototype or are undefined.
It performs the following steps when called:
This method performs the following steps when called:
The abstract operation FlattenIntoArray takes arguments target (an Object), source (an Object), sourceLen (a non-negative integer), start (a non-negative integer), and depth (a non-negative integer or +‚àû) and optional arguments mapperFunction (a function object) and thisArg (an ECMAScript language value) and returns either a normal completion containing a non-negative integer or a throw completion. It performs the following steps when called:
This method performs the following steps when called:
callbackfn should be a function that accepts three arguments. forEach calls callbackfn once for each element present in the array, in ascending order. callbackfn is called only for elements of the array which actually exist; it is not called for missing elements of the array.
If a thisArg parameter is provided, it will be used as the this value for each invocation of callbackfn. If it is not provided, undefined is used instead.
callbackfn is called with three arguments: the value of the element, the index of the element, and the object being traversed.
forEach does not directly mutate the object on which it is called but the object may be mutated by the calls to callbackfn.
The range of elements processed by forEach is set before the first call to callbackfn. Elements which are appended to the array after the call to forEach begins will not be visited by callbackfn. If existing elements of the array are changed, their value as passed to callbackfn will be the value at the time forEach visits them; elements that are deleted after the call to forEach begins and before being visited are not visited.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method compares searchElement to the elements of the array, in ascending order, using the SameValueZero algorithm, and if found at any position, returns true; otherwise, it returns false.
The optional second argument fromIndex defaults to +0ùîΩ (i.e. the whole array is searched). If it is greater than or equal to the length of the array, false is returned, i.e. the array will not be searched. If it is less than -0ùîΩ, it is used as the offset from the end of the array to compute fromIndex. If the computed index is less than or equal to +0ùîΩ, the whole array will be searched.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method intentionally differs from the similar indexOf method in two ways. First, it uses the SameValueZero algorithm, instead of IsStrictlyEqual, allowing it to detect NaN array elements. Second, it does not skip missing array elements, instead treating them as undefined.
This method compares searchElement to the elements of the array, in ascending order, using the IsStrictlyEqual algorithm, and if found at one or more indices, returns the smallest such index; otherwise, it returns -1ùîΩ.
The optional second argument fromIndex defaults to +0ùîΩ (i.e. the whole array is searched). If it is greater than or equal to the length of the array, -1ùîΩ is returned, i.e. the array will not be searched. If it is less than -0ùîΩ, it is used as the offset from the end of the array to compute fromIndex. If the computed index is less than or equal to +0ùîΩ, the whole array will be searched.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method converts the elements of the array to Strings, and then concatenates these Strings, separated by occurrences of the separator. If no separator is provided, a single comma is used as the separator.
It performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore, it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
This method compares searchElement to the elements of the array in descending order using the IsStrictlyEqual algorithm, and if found at one or more indices, returns the largest such index; otherwise, it returns -1ùîΩ.
The optional second argument fromIndex defaults to the array's length minus one (i.e. the whole array is searched). If it is greater than or equal to the length of the array, the whole array will be searched. If it is less than -0ùîΩ, it is used as the offset from the end of the array to compute fromIndex. If the computed index is less than or equal to +0ùîΩ, -1ùîΩ is returned.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
callbackfn should be a function that accepts three arguments. map calls callbackfn once for each element in the array, in ascending order, and constructs a new Array from the results. callbackfn is called only for elements of the array which actually exist; it is not called for missing elements of the array.
If a thisArg parameter is provided, it will be used as the this value for each invocation of callbackfn. If it is not provided, undefined is used instead.
callbackfn is called with three arguments: the value of the element, the index of the element, and the object being traversed.
map does not directly mutate the object on which it is called but the object may be mutated by the calls to callbackfn.
The range of elements processed by map is set before the first call to callbackfn. Elements which are appended to the array after the call to map begins will not be visited by callbackfn. If existing elements of the array are changed, their value as passed to callbackfn will be the value at the time map visits them; elements that are deleted after the call to map begins and before being visited are not visited.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method removes the last element of the array and returns it.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method appends the arguments to the end of the array, in the order in which they appear. It returns the new length of the array.
This method performs the following steps when called:
The "length" property of this method is 1ùîΩ.
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
callbackfn should be a function that takes four arguments. reduce calls the callback, as a function, once for each element after the first element present in the array, in ascending order.
callbackfn is called with four arguments: the previousValue (value from the previous call to callbackfn), the currentValue (value of the current element), the currentIndex, and the object being traversed. The first time that callback is called, the previousValue and currentValue can be one of two values. If an initialValue was supplied in the call to reduce, then previousValue will be initialValue and currentValue will be the first value in the array. If no initialValue was supplied, then previousValue will be the first value in the array and currentValue will be the second. It is a TypeError if the array contains no elements and initialValue is not provided.
reduce does not directly mutate the object on which it is called but the object may be mutated by the calls to callbackfn.
The range of elements processed by reduce is set before the first call to callbackfn. Elements that are appended to the array after the call to reduce begins will not be visited by callbackfn. If existing elements of the array are changed, their value as passed to callbackfn will be the value at the time reduce visits them; elements that are deleted after the call to reduce begins and before being visited are not visited.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
callbackfn should be a function that takes four arguments. reduceRight calls the callback, as a function, once for each element after the first element present in the array, in descending order.
callbackfn is called with four arguments: the previousValue (value from the previous call to callbackfn), the currentValue (value of the current element), the currentIndex, and the object being traversed. The first time the function is called, the previousValue and currentValue can be one of two values. If an initialValue was supplied in the call to reduceRight, then previousValue will be initialValue and currentValue will be the last value in the array. If no initialValue was supplied, then previousValue will be the last value in the array and currentValue will be the second-to-last value. It is a TypeError if the array contains no elements and initialValue is not provided.
reduceRight does not directly mutate the object on which it is called but the object may be mutated by the calls to callbackfn.
The range of elements processed by reduceRight is set before the first call to callbackfn. Elements that are appended to the array after the call to reduceRight begins will not be visited by callbackfn. If existing elements of the array are changed by callbackfn, their value as passed to callbackfn will be the value at the time reduceRight visits them; elements that are deleted after the call to reduceRight begins and before being visited are not visited.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method rearranges the elements of the array so as to reverse their order. It returns the object as the result of the call.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore, it can be transferred to other kinds of objects for use as a method.
This method removes the first element of the array and returns it.
It performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method returns an array containing the elements of the array from element start up to, but not including, element end (or through the end of the array if end is undefined). If start is negative, it is treated as length + start where length is the length of the array. If end is negative, it is treated as length + end where length is the length of the array.
It performs the following steps when called:
The explicit setting of the "length" property in step 15 is intended to ensure the length is correct even when A is not a built-in Array.
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
callbackfn should be a function that accepts three arguments and returns a value that is coercible to a Boolean value. some calls callbackfn once for each element present in the array, in ascending order, until it finds one where callbackfn returns true. If such an element is found, some immediately returns true. Otherwise, some returns false. callbackfn is called only for elements of the array which actually exist; it is not called for missing elements of the array.
If a thisArg parameter is provided, it will be used as the this value for each invocation of callbackfn. If it is not provided, undefined is used instead.
callbackfn is called with three arguments: the value of the element, the index of the element, and the object being traversed.
some does not directly mutate the object on which it is called but the object may be mutated by the calls to callbackfn.
The range of elements processed by some is set before the first call to callbackfn. Elements that are appended to the array after the call to some begins will not be visited by callbackfn. If existing elements of the array are changed, their value as passed to callbackfn will be the value at the time that some visits them; elements that are deleted after the call to some begins and before being visited are not visited. some acts like the "exists" quantifier in mathematics. In particular, for an empty array, it returns false.
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method sorts the elements of this array. The sort must be stable (that is, elements that compare equal must remain in their original order). If comparefn is not undefined, it should be a function that accepts two arguments x and y and returns a negative Number if x < y, a positive Number if x > y, or a zero otherwise.
It performs the following steps when called:
Because non-existent property values always compare greater than undefined property values, and undefined always compares greater than any other value (see CompareArrayElements), undefined property values always sort to the end of the result, followed by non-existent property values.
Method calls performed by the ToString abstract operations in steps 5 and 6 have the potential to cause SortCompare to not behave as a consistent comparator.
This method is intentionally generic; it does not require that its this value be an Array. Therefore, it can be transferred to other kinds of objects for use as a method.
The abstract operation SortIndexedProperties takes arguments obj (an Object), len (a non-negative integer), SortCompare (an Abstract Closure with two parameters), and holes (skip-holes or read-through-holes) and returns either a normal completion containing a List of ECMAScript language values or a throw completion. It performs the following steps when called:
The sort order is the ordering of items after completion of step 4 of the algorithm above. The sort order is implementation-defined if SortCompare is not a consistent comparator for the elements of items. When SortIndexedProperties is invoked by Array.prototype.sort, the sort order is also implementation-defined if comparefn is undefined, and all applications of ToString, to any specific value passed as an argument to SortCompare, do not produce the same result.
Unless the sort order is specified to be implementation-defined, it must satisfy all of the following conditions:
Here the notation old[j] is used to refer to items[j] before step 4 is executed, and the notation new[j] to refer to items[j] after step 4 has been executed.
An abstract closure or function comparator is a consistent comparator for a set of values S if all of the requirements below are met for all values a, b, and c (possibly the same value) in the set S: The notation a <C b means ‚Ñù(comparator(a, b)) < 0; a =C b means ‚Ñù(comparator(a, b)) = 0; and a >C b means ‚Ñù(comparator(a, b)) > 0.
The above conditions are necessary and sufficient to ensure that comparator divides the set S into equivalence classes and that these equivalence classes are totally ordered.
The abstract operation CompareArrayElements takes arguments x (an ECMAScript language value), y (an ECMAScript language value), and comparefn (a function object or undefined) and returns either a normal completion containing a Number or an abrupt completion. It performs the following steps when called:
This method deletes the deleteCount elements of the array starting at integer index start and replaces them with the elements of items. It returns an Array containing the deleted elements (if any).
This method performs the following steps when called:
The explicit setting of the "length" property in steps 15 and 20 is intended to ensure the lengths are correct even when the objects are not built-in Arrays.
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
An ECMAScript implementation that includes the ECMA-402 Internationalization API must implement this method as specified in the ECMA-402 specification. If an ECMAScript implementation does not include the ECMA-402 API the following specification of this method is used.
The first edition of ECMA-402 did not include a replacement specification for this method.
The meanings of the optional parameters to this method are defined in the ECMA-402 specification; implementations that do not include ECMA-402 support must not use those parameter positions for anything else.
This method performs the following steps when called:
This method converts the elements of the array to Strings using their toLocaleString methods, and then concatenates these Strings, separated by occurrences of an implementation-defined locale-sensitive separator String. This method is analogous to toString except that it is intended to yield a locale-sensitive result corresponding with conventions of the host environment's current locale.
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method prepends the arguments to the start of the array, such that their order within the array is the same as the order in which they appear in the argument list.
It performs the following steps when called:
The "length" property of this method is 1ùîΩ.
This method is intentionally generic; it does not require that its this value be an Array. Therefore it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
This method performs the following steps when called:
The initial value of the @@iterator property is %Array.prototype.values%, defined in 23.1.3.38.
The initial value of the @@unscopables data property is an object created by the following steps:
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The own property names of this object are property names that were not included as standard properties of Array.prototype prior to the ECMAScript 2015 specification. These names are ignored for with statement binding purposes in order to preserve the behaviour of existing code that might use one of these names as a binding in an outer scope that is shadowed by a with statement whose binding object is an Array.
The reason that "with" is not included in the unscopableList is because it is already a reserved word.
Array instances are Array exotic objects and have the internal methods specified for such objects. Array instances inherit properties from the Array prototype object.
Array instances have a "length" property, and a set of enumerable properties with array index names.
The "length" property of an Array instance is a data property whose value is always numerically greater than the name of every configurable own property whose name is an array index.
The "length" property initially has the attributes { [[Writable]]: true, [[Enumerable]]: false, [[Configurable]]: false }.
Reducing the value of the "length" property has the side-effect of deleting own array elements whose array index is between the old and new length values. However, non-configurable properties can not be deleted. Attempting to set the "length" property of an Array to a value that is numerically less than or equal to the largest numeric own property name of an existing non-configurable array-indexed property of the array will result in the length being set to a numeric value that is one greater than that non-configurable numeric own property name. See 10.4.2.1.
An Array Iterator is an object, that represents a specific iteration over some specific Array instance object. There is not a named constructor for Array Iterator objects. Instead, Array iterator objects are created by calling certain methods of Array instance objects.
The abstract operation CreateArrayIterator takes arguments array (an Object) and kind (key+value, key, or value) and returns a Generator. It is used to create iterator objects for Array methods that return such iterators. It performs the following steps when called:
The %ArrayIteratorPrototype% object:
The initial value of the @@toStringTag property is the String value "Array Iterator".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
A TypedArray presents an array-like view of an underlying binary data buffer (25.1). A TypedArray element type is the underlying binary scalar data type that all elements of a TypedArray instance have. There is a distinct TypedArray constructor, listed in Table 71, for each of the supported element types. Each constructor in Table 71 has a corresponding distinct prototype object.
In the definitions below, references to TypedArray should be replaced with the appropriate constructor name from the above table.
The %TypedArray% intrinsic object:
This function performs the following steps when called:
The "length" property of this function is +0ùîΩ.
The %TypedArray% intrinsic object:
This method performs the following steps when called:
This method performs the following steps when called:
The initial value of %TypedArray%.prototype is the %TypedArray% prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
%TypedArray%[@@species] is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The value of the "name" property of this function is "get [Symbol.species]".
%TypedArray.prototype% methods normally use their this value's constructor to create a derived object. However, a subclass constructor may over-ride that default behaviour by redefining its @@species property.
The %TypedArray% prototype object:
%TypedArray%.prototype.buffer is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
%TypedArray%.prototype.byteLength is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
%TypedArray%.prototype.byteOffset is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The initial value of %TypedArray%.prototype.constructor is %TypedArray%.
The interpretation and use of the arguments of this method are the same as for Array.prototype.copyWithin as defined in 23.1.3.4.
This method performs the following steps when called:
This method performs the following steps when called:
The interpretation and use of the arguments of this method are the same as for Array.prototype.every as defined in 23.1.3.6.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.fill as defined in 23.1.3.7.
This method performs the following steps when called:
The interpretation and use of the arguments of this method are the same as for Array.prototype.filter as defined in 23.1.3.8.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.find as defined in 23.1.3.9.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.findIndex as defined in 23.1.3.10.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.findLast as defined in 23.1.3.11.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.findLastIndex as defined in 23.1.3.12.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.forEach as defined in 23.1.3.15.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.includes as defined in 23.1.3.16.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.indexOf as defined in 23.1.3.17.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.join as defined in 23.1.3.18.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
This method performs the following steps when called:
The interpretation and use of the arguments of this method are the same as for Array.prototype.lastIndexOf as defined in 23.1.3.20.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
%TypedArray%.prototype.length is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
This function is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.map as defined in 23.1.3.21.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.reduce as defined in 23.1.3.24.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.reduceRight as defined in 23.1.3.25.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.reverse as defined in 23.1.3.26.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
This method sets multiple values in this TypedArray, reading the values from source. The details differ based upon the type of source. The optional offset value indicates the first element index in this TypedArray where values are written. If omitted, it is assumed to be 0.
It performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The abstract operation SetTypedArrayFromTypedArray takes arguments target (a TypedArray), targetOffset (a non-negative integer or +‚àû), and source (a TypedArray) and returns either a normal completion containing unused or a throw completion. It sets multiple values in target, starting at index targetOffset, reading the values from source. It performs the following steps when called:
The abstract operation SetTypedArrayFromArrayLike takes arguments target (a TypedArray), targetOffset (a non-negative integer or +‚àû), and source (an ECMAScript language value, but not a TypedArray) and returns either a normal completion containing unused or a throw completion. It sets multiple values in target, starting at index targetOffset, reading the values from source. It performs the following steps when called:
The interpretation and use of the arguments of this method are the same as for Array.prototype.slice as defined in 23.1.3.28.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
The interpretation and use of the arguments of this method are the same as for Array.prototype.some as defined in 23.1.3.29.
This method performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
This is a distinct method that, except as described below, implements the same requirements as those of Array.prototype.sort as defined in 23.1.3.30. The implementation of this method may be optimized with the knowledge that the this value is an object that has a fixed length and whose integer-indexed properties are not sparse.
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
It performs the following steps when called:
Because NaN always compares greater than any other value (see CompareTypedArrayElements), NaN property values always sort to the end of the result when comparefn is not provided.
This method returns a new TypedArray whose element type is the element type of this TypedArray and whose ArrayBuffer is the ArrayBuffer of this TypedArray, referencing the elements in the interval from start (inclusive) to end (exclusive). If either start or end is negative, it refers to an index from the end of the array, as opposed to from the beginning.
It performs the following steps when called:
This method is not generic. The this value must be an object with a [[TypedArrayName]] internal slot.
This is a distinct method that implements the same algorithm as Array.prototype.toLocaleString as defined in 23.1.3.32 except that TypedArrayLength is called in place of performing a [[Get]] of "length". The implementation of the algorithm may be optimized with the knowledge that the this value has a fixed length when the underlying buffer is not resizable and whose integer-indexed properties are not sparse. However, such optimization must not introduce any observable changes in the specified behaviour of the algorithm.
This method is not generic. ValidateTypedArray is called with the this value and seq-cst as arguments prior to evaluating the algorithm. If its result is an abrupt completion that exception is thrown instead of evaluating the algorithm.
If the ECMAScript implementation includes the ECMA-402 Internationalization API this method is based upon the algorithm for Array.prototype.toLocaleString that is in the ECMA-402 specification.
This method performs the following steps when called:
This method performs the following steps when called:
The initial value of the "toString" property is %Array.prototype.toString%, defined in 23.1.3.36.
This method performs the following steps when called:
This method performs the following steps when called:
The initial value of the @@iterator property is %TypedArray.prototype.values%, defined in 23.2.3.35.
%TypedArray%.prototype[@@toStringTag] is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
This property has the attributes { [[Enumerable]]: false, [[Configurable]]: true }.
The initial value of the "name" property of this function is "get [Symbol.toStringTag]".
The abstract operation TypedArraySpeciesCreate takes arguments exemplar (a TypedArray) and argumentList (a List of ECMAScript language values) and returns either a normal completion containing a TypedArray or a throw completion. It is used to specify the creation of a new TypedArray using a constructor function that is derived from exemplar. Unlike ArraySpeciesCreate, which can create non-Array objects through the use of @@species, this operation enforces that the constructor function creates an actual TypedArray. It performs the following steps when called:
The abstract operation TypedArrayCreateFromConstructor takes arguments constructor (a constructor) and argumentList (a List of ECMAScript language values) and returns either a normal completion containing a TypedArray or a throw completion. It is used to specify the creation of a new TypedArray using a constructor function. It performs the following steps when called:
The abstract operation TypedArrayCreateSameType takes arguments exemplar (a TypedArray) and argumentList (a List of ECMAScript language values) and returns either a normal completion containing a TypedArray or a throw completion. It is used to specify the creation of a new TypedArray using a constructor function that is derived from exemplar. Unlike TypedArraySpeciesCreate, which can construct custom TypedArray subclasses through the use of @@species, this operation always uses one of the built-in TypedArray constructors. It performs the following steps when called:
The abstract operation ValidateTypedArray takes arguments O (an ECMAScript language value) and order (seq-cst or unordered) and returns either a normal completion containing a TypedArray With Buffer Witness Record or a throw completion. It performs the following steps when called:
The abstract operation TypedArrayElementSize takes argument O (a TypedArray) and returns a non-negative integer. It performs the following steps when called:
The abstract operation TypedArrayElementType takes argument O (a TypedArray) and returns a TypedArray element type. It performs the following steps when called:
The abstract operation CompareTypedArrayElements takes arguments x (a Number or a BigInt), y (a Number or a BigInt), and comparefn (a function object or undefined) and returns either a normal completion containing a Number or an abrupt completion. It performs the following steps when called:
Each TypedArray constructor:
Each TypedArray constructor performs the following steps when called:
The abstract operation AllocateTypedArray takes arguments constructorName (a String which is the name of a TypedArray constructor in Table 71), newTarget (a constructor), and defaultProto (a String) and optional argument length (a non-negative integer) and returns either a normal completion containing a TypedArray or a throw completion. It is used to validate and create an instance of a TypedArray constructor. If the length argument is passed, an ArrayBuffer of that length is also allocated and associated with the new TypedArray instance. AllocateTypedArray provides common semantics that is used by TypedArray. It performs the following steps when called:
The abstract operation InitializeTypedArrayFromTypedArray takes arguments O (a TypedArray) and srcArray (a TypedArray) and returns either a normal completion containing unused or a throw completion. It performs the following steps when called:
The abstract operation InitializeTypedArrayFromArrayBuffer takes arguments O (a TypedArray), buffer (an ArrayBuffer or a SharedArrayBuffer), byteOffset (an ECMAScript language value), and length (an ECMAScript language value) and returns either a normal completion containing unused or a throw completion. It performs the following steps when called:
The abstract operation InitializeTypedArrayFromList takes arguments O (a TypedArray) and values (a List of ECMAScript language values) and returns either a normal completion containing unused or a throw completion. It performs the following steps when called:
The abstract operation InitializeTypedArrayFromArrayLike takes arguments O (a TypedArray) and arrayLike (an Object, but not a TypedArray or an ArrayBuffer) and returns either a normal completion containing unused or a throw completion. It performs the following steps when called:
The abstract operation AllocateTypedArrayBuffer takes arguments O (a TypedArray) and length (a non-negative integer) and returns either a normal completion containing unused or a throw completion. It allocates and associates an ArrayBuffer with O. It performs the following steps when called:
Each TypedArray constructor:
The value of TypedArray.BYTES_PER_ELEMENT is the Element Size value specified in Table 71 for TypedArray.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The initial value of TypedArray.prototype is the corresponding TypedArray prototype intrinsic object (23.2.7).
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
Each TypedArray prototype object:
The value of TypedArray.prototype.BYTES_PER_ELEMENT is the Element Size value specified in Table 71 for TypedArray.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The initial value of the "constructor" property of the prototype for a given TypedArray constructor is the constructor itself.
TypedArray instances are TypedArrays. Each TypedArray instance inherits properties from the corresponding TypedArray prototype object. Each TypedArray instance has the following internal slots: [[TypedArrayName]], [[ViewedArrayBuffer]], [[ByteLength]], [[ByteOffset]], and [[ArrayLength]].
Maps are collections of key/value pairs where both the keys and values may be arbitrary ECMAScript language values. A distinct key value may only occur in one key/value pair within the Map's collection. Distinct key values are discriminated using the SameValueZero comparison algorithm.
Maps must be implemented using either hash tables or other mechanisms that, on average, provide access times that are sublinear on the number of elements in the collection. The data structure used in this specification is only intended to describe the required observable semantics of Maps. It is not intended to be a viable implementation model.
The Map constructor:
This function performs the following steps when called:
If the parameter iterable is present, it is expected to be an object that implements an @@iterator method that returns an iterator object that produces a two element array-like object whose first element is a value that will be used as a Map key and whose second element is the value to associate with that key.
The abstract operation AddEntriesFromIterable takes arguments target (an Object), iterable (an ECMAScript language value, but not undefined or null), and adder (a function object) and returns either a normal completion containing an ECMAScript language value or a throw completion. adder will be invoked, with target as the receiver. It performs the following steps when called:
The parameter iterable is expected to be an object that implements an @@iterator method that returns an iterator object that produces a two element array-like object whose first element is a value that will be used as a Map key and whose second element is the value to associate with that key.
The Map constructor:
callbackfn should be a function that accepts two arguments. groupBy calls callbackfn once for each element in items, in ascending order, and constructs a new Map. Each value returned by callbackfn is used as a key in the Map. For each such key, the result Map has an entry whose key is that key and whose value is an array containing all the elements for which callbackfn returned that key.
callbackfn is called with two arguments: the value of the element and the index of the element.
The return value of groupBy is a Map.
This function performs the following steps when called:
The initial value of Map.prototype is the Map prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
Map[@@species] is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The value of the "name" property of this function is "get [Symbol.species]".
Methods that create derived collection objects should call @@species to determine the constructor to use to create the derived objects. Subclass constructor may over-ride @@species to change the default constructor assignment.
The Map prototype object:
This method performs the following steps when called:
The existing [[MapData]] List is preserved because there may be existing Map Iterator objects that are suspended midway through iterating over that List.
The initial value of Map.prototype.constructor is %Map%.
This method performs the following steps when called:
The value empty is used as a specification device to indicate that an entry has been deleted. Actual implementations may take other actions such as physically removing the entry from internal data structures.
This method performs the following steps when called:
This method performs the following steps when called:
callbackfn should be a function that accepts three arguments. forEach calls callbackfn once for each key/value pair present in the Map, in key insertion order. callbackfn is called only for keys of the Map which actually exist; it is not called for keys that have been deleted from the Map.
If a thisArg parameter is provided, it will be used as the this value for each invocation of callbackfn. If it is not provided, undefined is used instead.
callbackfn is called with three arguments: the value of the item, the key of the item, and the Map being traversed.
forEach does not directly mutate the object on which it is called but the object may be mutated by the calls to callbackfn. Each entry of a map's [[MapData]] is only visited once. New keys added after the call to forEach begins are visited. A key will be revisited if it is deleted after it has been visited and then re-added before the forEach call completes. Keys that are deleted after the call to forEach begins and before being visited are not visited unless the key is added again before the forEach call completes.
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
Map.prototype.size is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
This method performs the following steps when called:
The initial value of the @@iterator property is %Map.prototype.entries%, defined in 24.1.3.4.
The initial value of the @@toStringTag property is the String value "Map".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
Map instances are ordinary objects that inherit properties from the Map prototype. Map instances also have a [[MapData]] internal slot.
A Map Iterator is an object, that represents a specific iteration over some specific Map instance object. There is not a named constructor for Map Iterator objects. Instead, map iterator objects are created by calling certain methods of Map instance objects.
The abstract operation CreateMapIterator takes arguments map (an ECMAScript language value) and kind (key+value, key, or value) and returns either a normal completion containing a Generator or a throw completion. It is used to create iterator objects for Map methods that return such iterators. It performs the following steps when called:
The %MapIteratorPrototype% object:
The initial value of the @@toStringTag property is the String value "Map Iterator".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
Set objects are collections of ECMAScript language values. A distinct value may only occur once as an element of a Set's collection. Distinct values are discriminated using the SameValueZero comparison algorithm.
Set objects must be implemented using either hash tables or other mechanisms that, on average, provide access times that are sublinear on the number of elements in the collection. The data structure used in this specification is only intended to describe the required observable semantics of Set objects. It is not intended to be a viable implementation model.
The Set constructor:
This function performs the following steps when called:
The Set constructor:
The initial value of Set.prototype is the Set prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
Set[@@species] is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The value of the "name" property of this function is "get [Symbol.species]".
Methods that create derived collection objects should call @@species to determine the constructor to use to create the derived objects. Subclass constructor may over-ride @@species to change the default constructor assignment.
The Set prototype object:
This method performs the following steps when called:
This method performs the following steps when called:
The existing [[SetData]] List is preserved because there may be existing Set Iterator objects that are suspended midway through iterating over that List.
The initial value of Set.prototype.constructor is %Set%.
This method performs the following steps when called:
The value empty is used as a specification device to indicate that an entry has been deleted. Actual implementations may take other actions such as physically removing the entry from internal data structures.
This method performs the following steps when called:
For iteration purposes, a Set appears similar to a Map where each entry has the same value for its key and value.
This method performs the following steps when called:
callbackfn should be a function that accepts three arguments. forEach calls callbackfn once for each value present in the Set object, in value insertion order. callbackfn is called only for values of the Set which actually exist; it is not called for keys that have been deleted from the set.
If a thisArg parameter is provided, it will be used as the this value for each invocation of callbackfn. If it is not provided, undefined is used instead.
callbackfn is called with three arguments: the first two arguments are a value contained in the Set. The same value is passed for both arguments. The Set object being traversed is passed as the third argument.
The callbackfn is called with three arguments to be consistent with the call back functions used by forEach methods for Map and Array. For Sets, each item value is considered to be both the key and the value.
forEach does not directly mutate the object on which it is called but the object may be mutated by the calls to callbackfn.
Each value is normally visited only once. However, a value will be revisited if it is deleted after it has been visited and then re-added before the forEach call completes. Values that are deleted after the call to forEach begins and before being visited are not visited unless the value is added again before the forEach call completes. New values added after the call to forEach begins are visited.
This method performs the following steps when called:
The initial value of the "keys" property is %Set.prototype.values%, defined in 24.2.3.10.
For iteration purposes, a Set appears similar to a Map where each entry has the same value for its key and value.
Set.prototype.size is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
This method performs the following steps when called:
The initial value of the @@iterator property is %Set.prototype.values%, defined in 24.2.3.10.
The initial value of the @@toStringTag property is the String value "Set".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
Set instances are ordinary objects that inherit properties from the Set prototype. Set instances also have a [[SetData]] internal slot.
A Set Iterator is an ordinary object, with the structure defined below, that represents a specific iteration over some specific Set instance object. There is not a named constructor for Set Iterator objects. Instead, set iterator objects are created by calling certain methods of Set instance objects.
The abstract operation CreateSetIterator takes arguments set (an ECMAScript language value) and kind (key+value or value) and returns either a normal completion containing a Generator or a throw completion. It is used to create iterator objects for Set methods that return such iterators. It performs the following steps when called:
The %SetIteratorPrototype% object:
The initial value of the @@toStringTag property is the String value "Set Iterator".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
WeakMaps are collections of key/value pairs where the keys are objects and/or symbols and values may be arbitrary ECMAScript language values. A WeakMap may be queried to see if it contains a key/value pair with a specific key, but no mechanism is provided for enumerating the values it holds as keys. In certain conditions, values which are not live are removed as WeakMap keys, as described in 9.10.3.
An implementation may impose an arbitrarily determined latency between the time a key/value pair of a WeakMap becomes inaccessible and the time when the key/value pair is removed from the WeakMap. If this latency was observable to ECMAScript program, it would be a source of indeterminacy that could impact program execution. For that reason, an ECMAScript implementation must not provide any means to observe a key of a WeakMap that does not require the observer to present the observed key.
WeakMaps must be implemented using either hash tables or other mechanisms that, on average, provide access times that are sublinear on the number of key/value pairs in the collection. The data structure used in this specification is only intended to describe the required observable semantics of WeakMaps. It is not intended to be a viable implementation model.
WeakMap and WeakSet are intended to provide mechanisms for dynamically associating state with an object or symbol in a manner that does not ‚Äúleak‚Äù memory resources if, in the absence of the WeakMap or WeakSet instance, the object or symbol otherwise became inaccessible and subject to resource reclamation by the implementation's garbage collection mechanisms. This characteristic can be achieved by using an inverted per-object/symbol mapping of WeakMap or WeakSet instances to keys. Alternatively, each WeakMap or WeakSet instance may internally store its key and value data, but this approach requires coordination between the WeakMap or WeakSet implementation and the garbage collector. The following references describe mechanism that may be useful to implementations of WeakMap and WeakSet:
Barry Hayes. 1997. Ephemerons: a new finalization mechanism. In Proceedings of the 12th ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications (OOPSLA '97), A. Michael Berman (Ed.). ACM, New York, NY, USA, 176-183, http://doi.acm.org/10.1145/263698.263733.
Alexandra Barros, Roberto Ierusalimschy, Eliminating Cycles in Weak Tables. Journal of Universal Computer Science - J.UCS, vol. 14, no. 21, pp. 3481-3497, 2008, http://www.jucs.org/jucs_14_21/eliminating_cycles_in_weak
The WeakMap constructor:
This function performs the following steps when called:
If the parameter iterable is present, it is expected to be an object that implements an @@iterator method that returns an iterator object that produces a two element array-like object whose first element is a value that will be used as a WeakMap key and whose second element is the value to associate with that key.
The WeakMap constructor:
The initial value of WeakMap.prototype is the WeakMap prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The WeakMap prototype object:
The initial value of WeakMap.prototype.constructor is %WeakMap%.
This method performs the following steps when called:
The value empty is used as a specification device to indicate that an entry has been deleted. Actual implementations may take other actions such as physically removing the entry from internal data structures.
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
The initial value of the @@toStringTag property is the String value "WeakMap".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
WeakMap instances are ordinary objects that inherit properties from the WeakMap prototype. WeakMap instances also have a [[WeakMapData]] internal slot.
WeakSets are collections of objects and/or symbols. A distinct object or symbol may only occur once as an element of a WeakSet's collection. A WeakSet may be queried to see if it contains a specific value, but no mechanism is provided for enumerating the values it holds. In certain conditions, values which are not live are removed as WeakSet elements, as described in 9.10.3.
An implementation may impose an arbitrarily determined latency between the time a value contained in a WeakSet becomes inaccessible and the time when the value is removed from the WeakSet. If this latency was observable to ECMAScript program, it would be a source of indeterminacy that could impact program execution. For that reason, an ECMAScript implementation must not provide any means to determine if a WeakSet contains a particular value that does not require the observer to present the observed value.
WeakSets must be implemented using either hash tables or other mechanisms that, on average, provide access times that are sublinear on the number of elements in the collection. The data structure used in this specification is only intended to describe the required observable semantics of WeakSets. It is not intended to be a viable implementation model.
See the NOTE in 24.3.
The WeakSet constructor:
This function performs the following steps when called:
The WeakSet constructor:
The initial value of WeakSet.prototype is the WeakSet prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The WeakSet prototype object:
This method performs the following steps when called:
The initial value of WeakSet.prototype.constructor is %WeakSet%.
This method performs the following steps when called:
The value empty is used as a specification device to indicate that an entry has been deleted. Actual implementations may take other actions such as physically removing the entry from internal data structures.
This method performs the following steps when called:
The initial value of the @@toStringTag property is the String value "WeakSet".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
WeakSet instances are ordinary objects that inherit properties from the WeakSet prototype. WeakSet instances also have a [[WeakSetData]] internal slot.
The descriptions below in this section, 25.4, and 29 use the read-modify-write modification function internal data structure.
A read-modify-write modification function is a mathematical function that is notationally represented as an abstract closure that takes two Lists of byte values as arguments and returns a List of byte values. These abstract closures satisfy all of the following properties:
To aid verifying that a read-modify-write modification function's algorithm steps constitute a pure, mathematical function, the following editorial conventions are recommended:
A fixed-length ArrayBuffer is an ArrayBuffer whose byte length cannot change after creation.
A resizable ArrayBuffer is an ArrayBuffer whose byte length may change after creation via calls to ArrayBuffer.prototype.resize ( newLength ).
The kind of ArrayBuffer object that is created depends on the arguments passed to ArrayBuffer ( length [ , options ] ).
The abstract operation AllocateArrayBuffer takes arguments constructor (a constructor) and byteLength (a non-negative integer) and optional argument maxByteLength (a non-negative integer or empty) and returns either a normal completion containing an ArrayBuffer or a throw completion. It is used to create an ArrayBuffer. It performs the following steps when called:
The abstract operation ArrayBufferByteLength takes arguments arrayBuffer (an ArrayBuffer or SharedArrayBuffer) and order (seq-cst or unordered) and returns a non-negative integer. It performs the following steps when called:
The abstract operation ArrayBufferCopyAndDetach takes arguments arrayBuffer (an ECMAScript language value), newLength (an ECMAScript language value), and preserveResizability (preserve-resizability or fixed-length) and returns either a normal completion containing an ArrayBuffer or a throw completion. It performs the following steps when called:
The abstract operation IsDetachedBuffer takes argument arrayBuffer (an ArrayBuffer or a SharedArrayBuffer) and returns a Boolean. It performs the following steps when called:
The abstract operation DetachArrayBuffer takes argument arrayBuffer (an ArrayBuffer) and optional argument key (anything) and returns either a normal completion containing unused or a throw completion. It performs the following steps when called:
Detaching an ArrayBuffer instance disassociates the Data Block used as its backing store from the instance and sets the byte length of the buffer to 0.
The abstract operation CloneArrayBuffer takes arguments srcBuffer (an ArrayBuffer or a SharedArrayBuffer), srcByteOffset (a non-negative integer), and srcLength (a non-negative integer) and returns either a normal completion containing an ArrayBuffer or a throw completion. It creates a new ArrayBuffer whose data is a copy of srcBuffer's data over the range starting at srcByteOffset and continuing for srcLength bytes. It performs the following steps when called:
The abstract operation GetArrayBufferMaxByteLengthOption takes argument options (an ECMAScript language value) and returns either a normal completion containing either a non-negative integer or empty, or a throw completion. It performs the following steps when called:
The host-defined abstract operation HostResizeArrayBuffer takes arguments buffer (an ArrayBuffer) and newByteLength (a non-negative integer) and returns either a normal completion containing either handled or unhandled, or a throw completion. It gives the host an opportunity to perform implementation-defined resizing of buffer. If the host chooses not to handle resizing of buffer, it may return unhandled for the default behaviour.
The implementation of HostResizeArrayBuffer must conform to the following requirements:
The default implementation of HostResizeArrayBuffer is to return NormalCompletion(unhandled).
The abstract operation IsFixedLengthArrayBuffer takes argument arrayBuffer (an ArrayBuffer or a SharedArrayBuffer) and returns a Boolean. It performs the following steps when called:
The abstract operation IsUnsignedElementType takes argument type (a TypedArray element type) and returns a Boolean. It verifies if the argument type is an unsigned TypedArray element type. It performs the following steps when called:
The abstract operation IsUnclampedIntegerElementType takes argument type (a TypedArray element type) and returns a Boolean. It verifies if the argument type is an Integer TypedArray element type not including uint8clamped. It performs the following steps when called:
The abstract operation IsBigIntElementType takes argument type (a TypedArray element type) and returns a Boolean. It verifies if the argument type is a BigInt TypedArray element type. It performs the following steps when called:
The abstract operation IsNoTearConfiguration takes arguments type (a TypedArray element type) and order (seq-cst, unordered, or init) and returns a Boolean. It performs the following steps when called:
The abstract operation RawBytesToNumeric takes arguments type (a TypedArray element type), rawBytes (a List of byte values), and isLittleEndian (a Boolean) and returns a Number or a BigInt. It performs the following steps when called:
The abstract operation GetRawBytesFromSharedBlock takes arguments block (a Shared Data Block), byteIndex (a non-negative integer), type (a TypedArray element type), isTypedArray (a Boolean), and order (seq-cst or unordered) and returns a List of byte values. It performs the following steps when called:
The abstract operation GetValueFromBuffer takes arguments arrayBuffer (an ArrayBuffer or SharedArrayBuffer), byteIndex (a non-negative integer), type (a TypedArray element type), isTypedArray (a Boolean), and order (seq-cst or unordered) and optional argument isLittleEndian (a Boolean) and returns a Number or a BigInt. It performs the following steps when called:
The abstract operation NumericToRawBytes takes arguments type (a TypedArray element type), value (a Number or a BigInt), and isLittleEndian (a Boolean) and returns a List of byte values. It performs the following steps when called:
The abstract operation SetValueInBuffer takes arguments arrayBuffer (an ArrayBuffer or SharedArrayBuffer), byteIndex (a non-negative integer), type (a TypedArray element type), value (a Number or a BigInt), isTypedArray (a Boolean), and order (seq-cst, unordered, or init) and optional argument isLittleEndian (a Boolean) and returns unused. It performs the following steps when called:
The abstract operation GetModifySetValueInBuffer takes arguments arrayBuffer (an ArrayBuffer or a SharedArrayBuffer), byteIndex (a non-negative integer), type (a TypedArray element type), value (a Number or a BigInt), and op (a read-modify-write modification function) and returns a Number or a BigInt. It performs the following steps when called:
The ArrayBuffer constructor:
This function performs the following steps when called:
The ArrayBuffer constructor:
This function performs the following steps when called:
The initial value of ArrayBuffer.prototype is the ArrayBuffer prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
ArrayBuffer[@@species] is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The value of the "name" property of this function is "get [Symbol.species]".
ArrayBuffer.prototype.slice ( start, end ) normally uses its this value's constructor to create a derived object. However, a subclass constructor may over-ride that default behaviour for the ArrayBuffer.prototype.slice ( start, end ) method by redefining its @@species property.
The ArrayBuffer prototype object:
ArrayBuffer.prototype.byteLength is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The initial value of ArrayBuffer.prototype.constructor is %ArrayBuffer%.
ArrayBuffer.prototype.detached is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
ArrayBuffer.prototype.maxByteLength is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
ArrayBuffer.prototype.resizable is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
The initial value of the @@toStringTag property is the String value "ArrayBuffer".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
ArrayBuffer instances inherit properties from the ArrayBuffer prototype object. ArrayBuffer instances each have an [[ArrayBufferData]] internal slot, an [[ArrayBufferByteLength]] internal slot, and an [[ArrayBufferDetachKey]] internal slot. ArrayBuffer instances which are resizable each have an [[ArrayBufferMaxByteLength]] internal slot.
ArrayBuffer instances whose [[ArrayBufferData]] is null are considered to be detached and all operators to access or modify data contained in the ArrayBuffer instance will fail.
ArrayBuffer instances whose [[ArrayBufferDetachKey]] is set to a value other than undefined need to have all DetachArrayBuffer calls passing that same "detach key" as an argument, otherwise a TypeError will result. This internal slot is only ever set by certain embedding environments, not by algorithms in this specification.
The following are guidelines for ECMAScript programmers working with resizable ArrayBuffer.
We recommend that programs be tested in their deployment environments where possible. The amount of available physical memory differs greatly between hardware devices. Similarly, virtual memory subsystems also differ greatly between hardware devices as well as operating systems. An application that runs without out-of-memory errors on a 64-bit desktop web browser could run out of memory on a 32-bit mobile web browser.
When choosing a value for the "maxByteLength" option for resizable ArrayBuffer, we recommend that the smallest possible size for the application be chosen. We recommend that "maxByteLength" does not exceed 1,073,741,824 (230 bytes or 1GiB).
Please note that successfully constructing a resizable ArrayBuffer for a particular maximum size does not guarantee that future resizes will succeed.
The following are guidelines for ECMAScript implementers implementing resizable ArrayBuffer.
Resizable ArrayBuffer can be implemented as copying upon resize, as in-place growth via reserving virtual memory up front, or as a combination of both for different values of the constructor's "maxByteLength" option.
If a host is multi-tenanted (i.e. it runs many ECMAScript applications simultaneously), such as a web browser, and its implementations choose to implement in-place growth by reserving virtual memory, we recommend that both 32-bit and 64-bit implementations throw for values of "maxByteLength" ‚â• 1GiB to 1.5GiB. This is to reduce the likelihood a single application can exhaust the virtual memory address space and to reduce interoperability risk.
If a host does not have virtual memory, such as those running on embedded devices without an MMU, or if a host only implements resizing by copying, it may accept any Number value for the "maxByteLength" option. However, we recommend a RangeError be thrown if a memory block of the requested size can never be allocated. For example, if the requested size is greater than the maximium amount of usable memory on the device.
A fixed-length SharedArrayBuffer is a SharedArrayBuffer whose byte length cannot change after creation.
A growable SharedArrayBuffer is a SharedArrayBuffer whose byte length may increase after creation via calls to SharedArrayBuffer.prototype.grow ( newLength ).
The kind of SharedArrayBuffer object that is created depends on the arguments passed to SharedArrayBuffer ( length [ , options ] ).
The abstract operation AllocateSharedArrayBuffer takes arguments constructor (a constructor) and byteLength (a non-negative integer) and optional argument maxByteLength (a non-negative integer or empty) and returns either a normal completion containing a SharedArrayBuffer or a throw completion. It is used to create a SharedArrayBuffer. It performs the following steps when called:
The abstract operation IsSharedArrayBuffer takes argument obj (an ArrayBuffer or a SharedArrayBuffer) and returns a Boolean. It tests whether an object is an ArrayBuffer, a SharedArrayBuffer, or a subtype of either. It performs the following steps when called:
The host-defined abstract operation HostGrowSharedArrayBuffer takes arguments buffer (a SharedArrayBuffer) and newByteLength (a non-negative integer) and returns either a normal completion containing either handled or unhandled, or a throw completion. It gives the host an opportunity to perform implementation-defined growing of buffer. If the host chooses not to handle growing of buffer, it may return unhandled for the default behaviour.
The implementation of HostGrowSharedArrayBuffer must conform to the following requirements:
The second requirement above is intentionally vague about how or when the current byte length of buffer is read. Because the byte length must be updated via an atomic read-modify-write operation on the underlying hardware, architectures that use load-link/store-conditional or load-exclusive/store-exclusive instruction pairs may wish to keep the paired instructions close in the instruction stream. As such, SharedArrayBuffer.prototype.grow itself does not perform bounds checking on newByteLength before calling HostGrowSharedArrayBuffer, nor is there a requirement on when the current byte length is read.
This is in contrast with HostResizeArrayBuffer, which is guaranteed that the value of newByteLength is ‚â• 0 and ‚â§ buffer.[[ArrayBufferMaxByteLength]].
The default implementation of HostGrowSharedArrayBuffer is to return NormalCompletion(unhandled).
The SharedArrayBuffer constructor:
Whenever a host does not provide concurrent access to SharedArrayBuffers it may omit the "SharedArrayBuffer" property of the global object.
Unlike an ArrayBuffer, a SharedArrayBuffer cannot become detached, and its internal [[ArrayBufferData]] slot is never null.
This function performs the following steps when called:
The SharedArrayBuffer constructor:
The initial value of SharedArrayBuffer.prototype is the SharedArrayBuffer prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
SharedArrayBuffer[@@species] is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The value of the "name" property of this function is "get [Symbol.species]".
The SharedArrayBuffer prototype object:
SharedArrayBuffer.prototype.byteLength is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The initial value of SharedArrayBuffer.prototype.constructor is %SharedArrayBuffer%.
This method performs the following steps when called:
Spurious failures of the compare-exchange to update the length are prohibited. If the bounds checking for the new length passes and the implementation is not out of memory, a ReadModifyWriteSharedMemory event (i.e. a successful compare-exchange) is always added into the candidate execution.
Parallel calls to SharedArrayBuffer.prototype.grow are totally ordered. For example, consider two racing calls: sab.grow(10) and sab.grow(20). One of the two calls is guaranteed to win the race. The call to sab.grow(10) will never shrink sab even if sab.grow(20) happened first; in that case it will instead throw a RangeError.
SharedArrayBuffer.prototype.growable is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
SharedArrayBuffer.prototype.maxByteLength is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
This method performs the following steps when called:
The initial value of the @@toStringTag property is the String value "SharedArrayBuffer".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
SharedArrayBuffer instances inherit properties from the SharedArrayBuffer prototype object. SharedArrayBuffer instances each have an [[ArrayBufferData]] internal slot. SharedArrayBuffer instances which are not growable each have an [[ArrayBufferByteLength]] internal slot. SharedArrayBuffer instances which are growable each have an [[ArrayBufferByteLengthData]] internal slot and an [[ArrayBufferMaxByteLength]] internal slot.
SharedArrayBuffer instances, unlike ArrayBuffer instances, are never detached.
The following are guidelines for ECMAScript programmers working with growable SharedArrayBuffer.
We recommend that programs be tested in their deployment environments where possible. The amount of available physical memory differ greatly between hardware devices. Similarly, virtual memory subsystems also differ greatly between hardware devices as well as operating systems. An application that runs without out-of-memory errors on a 64-bit desktop web browser could run out of memory on a 32-bit mobile web browser.
When choosing a value for the "maxByteLength" option for growable SharedArrayBuffer, we recommend that the smallest possible size for the application be chosen. We recommend that "maxByteLength" does not exceed 1073741824, or 1GiB.
Please note that successfully constructing a growable SharedArrayBuffer for a particular maximum size does not guarantee that future grows will succeed.
Not all loads of a growable SharedArrayBuffer's length are synchronizing seq-cst loads. Loads of the length that are for bounds-checking of an integer-indexed property access, e.g. u8[idx], are not synchronizing. In general, in the absence of explicit synchronization, one property access being in-bound does not imply a subsequent property access in the same agent is also in-bound. In contrast, explicit loads of the length via the length and byteLength getters on SharedArrayBuffer, %TypedArray%.prototype, and DataView.prototype are synchronizing. Loads of the length that are performed by built-in methods to check if a TypedArray is entirely out-of-bounds are also synchronizing.
The following are guidelines for ECMAScript implementers implementing growable SharedArrayBuffer.
We recommend growable SharedArrayBuffer be implemented as in-place growth via reserving virtual memory up front.
Because grow operations can happen in parallel with memory accesses on a growable SharedArrayBuffer, the constraints of the memory model require that even unordered accesses do not "tear" (bits of their values will not be mixed). In practice, this means the underlying data block of a growable SharedArrayBuffer cannot be grown by being copied without stopping the world. We do not recommend stopping the world as an implementation strategy because it introduces a serialization point and is slow.
Grown memory must appear zeroed from the moment of its creation, including to any racy accesses in parallel. This can be accomplished via zero-filled-on-demand virtual memory pages, or careful synchronization if manually zeroing memory.
Integer-indexed property access on TypedArray views of growable SharedArrayBuffers is intended to be optimizable similarly to access on TypedArray views of non-growable SharedArrayBuffers, because integer-indexed property loads on are not synchronizing on the underlying buffer's length (see programmer guidelines above). For example, bounds checks for property accesses may still be hoisted out of loops.
In practice it is difficult to implement growable SharedArrayBuffer by copying on hosts that do not have virtual memory, such as those running on embedded devices without an MMU. Memory usage behaviour of growable SharedArrayBuffers on such hosts may significantly differ from that of hosts with virtual memory. Such hosts should clearly communicate memory usage expectations to users.
A DataView With Buffer Witness Record is a Record value used to encapsulate a DataView along with a cached byte length of the viewed buffer. It is used to help ensure there is a single shared memory read event of the byte length data block when the viewed buffer is a growable SharedArrayBuffers.
DataView With Buffer Witness Records have the fields listed in Table 72.
The abstract operation MakeDataViewWithBufferWitnessRecord takes arguments obj (a DataView) and order (seq-cst or unordered) and returns a DataView With Buffer Witness Record. It performs the following steps when called:
The abstract operation GetViewByteLength takes argument viewRecord (a DataView With Buffer Witness Record) and returns a non-negative integer. It performs the following steps when called:
The abstract operation IsViewOutOfBounds takes argument viewRecord (a DataView With Buffer Witness Record) and returns a Boolean. It performs the following steps when called:
The abstract operation GetViewValue takes arguments view (an ECMAScript language value), requestIndex (an ECMAScript language value), isLittleEndian (an ECMAScript language value), and type (a TypedArray element type) and returns either a normal completion containing either a Number or a BigInt, or a throw completion. It is used by functions on DataView instances to retrieve values from the view's buffer. It performs the following steps when called:
The abstract operation SetViewValue takes arguments view (an ECMAScript language value), requestIndex (an ECMAScript language value), isLittleEndian (an ECMAScript language value), type (a TypedArray element type), and value (an ECMAScript language value) and returns either a normal completion containing undefined or a throw completion. It is used by functions on DataView instances to store values into the view's buffer. It performs the following steps when called:
The DataView constructor:
This function performs the following steps when called:
The DataView constructor:
The initial value of DataView.prototype is the DataView prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The DataView prototype object:
DataView.prototype.buffer is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
DataView.prototype.byteLength is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
DataView.prototype.byteOffset is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The initial value of DataView.prototype.constructor is %DataView%.
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
The initial value of the @@toStringTag property is the String value "DataView".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
DataView instances are ordinary objects that inherit properties from the DataView prototype object. DataView instances each have [[DataView]], [[ViewedArrayBuffer]], [[ByteLength]], and [[ByteOffset]] internal slots.
The value of the [[DataView]] internal slot is not used within this specification. The simple presence of that internal slot is used within the specification to identify objects created using the DataView constructor.
The Atomics object:
The Atomics object provides functions that operate indivisibly (atomically) on shared memory array cells as well as functions that let agents wait for and dispatch primitive events. When used with discipline, the Atomics functions allow multi-agent programs that communicate through shared memory to execute in a well-understood order even on parallel CPUs. The rules that govern shared-memory communication are provided by the memory model, defined below.
For informative guidelines for programming and implementing shared memory in ECMAScript, please see the notes at the end of the memory model section.
A Waiter Record is a Record value used to denote a particular call to Atomics.wait or Atomics.waitAsync.
A Waiter Record has fields listed in Table 73.
A WaiterList Record is used to explain waiting and notification of agents via Atomics.wait, Atomics.waitAsync, and Atomics.notify.
A WaiterList Record has fields listed in Table 74.
There can be multiple Waiter Records in a WaiterList with the same agent signifier.
The agent cluster has a store of WaiterList Records; the store is indexed by (block, i), where block is a Shared Data Block and i a byte offset into the memory of block. WaiterList Records are agent-independent: a lookup in the store of WaiterList Records by (block, i) will result in the same WaiterList Record in any agent in the agent cluster.
Each WaiterList Record has a critical section that controls exclusive access to that WaiterList Record during evaluation. Only a single agent may enter a WaiterList Record's critical section at one time. Entering and leaving a WaiterList Record's critical section is controlled by the abstract operations EnterCriticalSection and LeaveCriticalSection. Operations on a WaiterList Record‚Äîadding and removing waiting agents, traversing the list of agents, suspending and notifying agents on the list, setting and retrieving the Synchronize event‚Äîmay only be performed by agents that have entered the WaiterList Record's critical section.
The abstract operation ValidateIntegerTypedArray takes arguments typedArray (an ECMAScript language value) and waitable (a Boolean) and returns either a normal completion containing a TypedArray With Buffer Witness Record, or a throw completion. It performs the following steps when called:
The abstract operation ValidateAtomicAccess takes arguments taRecord (a TypedArray With Buffer Witness Record) and requestIndex (an ECMAScript language value) and returns either a normal completion containing an integer or a throw completion. It performs the following steps when called:
The abstract operation ValidateAtomicAccessOnIntegerTypedArray takes arguments typedArray (an ECMAScript language value) and requestIndex (an ECMAScript language value) and optional argument waitable (a Boolean) and returns either a normal completion containing an integer or a throw completion. It performs the following steps when called:
The abstract operation RevalidateAtomicAccess takes arguments typedArray (a TypedArray) and byteIndexInBuffer (an integer) and returns either a normal completion containing unused or a throw completion. This operation revalidates the index within the backing buffer for atomic operations after all argument coercions are performed in Atomics methods, as argument coercions can have arbitrary side effects, which could cause the buffer to become out of bounds. This operation does not throw when typedArray's backing buffer is a SharedArrayBuffer. It performs the following steps when called:
The abstract operation GetWaiterList takes arguments block (a Shared Data Block) and i (a non-negative integer that is evenly divisible by 4) and returns a WaiterList Record. It performs the following steps when called:
The abstract operation EnterCriticalSection takes argument WL (a WaiterList Record) and returns unused. It performs the following steps when called:
EnterCriticalSection has contention when an agent attempting to enter the critical section must wait for another agent to leave it. When there is no contention, FIFO order of EnterCriticalSection calls is observable. When there is contention, an implementation may choose an arbitrary order but may not cause an agent to wait indefinitely.
The abstract operation LeaveCriticalSection takes argument WL (a WaiterList Record) and returns unused. It performs the following steps when called:
The abstract operation AddWaiter takes arguments WL (a WaiterList Record) and waiterRecord (a Waiter Record) and returns unused. It performs the following steps when called:
The abstract operation RemoveWaiter takes arguments WL (a WaiterList Record) and waiterRecord (a Waiter Record) and returns unused. It performs the following steps when called:
The abstract operation RemoveWaiters takes arguments WL (a WaiterList Record) and c (a non-negative integer or +‚àû) and returns a List of Waiter Records. It performs the following steps when called:
The abstract operation SuspendThisAgent takes arguments WL (a WaiterList Record) and waiterRecord (a Waiter Record) and returns unused. It performs the following steps when called:
The abstract operation NotifyWaiter takes arguments WL (a WaiterList Record) and waiterRecord (a Waiter Record) and returns unused. It performs the following steps when called:
An agent must not access another agent's promise capability in any capacity beyond passing it to the host.
The abstract operation EnqueueResolveInAgentJob takes arguments agentSignifier (an agent signifier), promiseCapability (a PromiseCapability Record), and resolution (an ECMAScript language value) and returns unused. It performs the following steps when called:
The abstract operation DoWait takes arguments mode (sync or async), typedArray (an ECMAScript language value), index (an ECMAScript language value), value (an ECMAScript language value), and timeout (an ECMAScript language value) and returns either a normal completion containing either an Object, "not-equal", "timed-out", or "ok", or a throw completion. It performs the following steps when called:
additionalTimeout allows implementations to pad timeouts as necessary, such as for reducing power consumption or coarsening timer resolution to mitigate timing attacks. This value may differ from call to call of DoWait.
The abstract operation EnqueueAtomicsWaitAsyncTimeoutJob takes arguments WL (a WaiterList Record) and waiterRecord (a Waiter Record) and returns unused. It performs the following steps when called:
The abstract operation AtomicCompareExchangeInSharedBlock takes arguments block (a Shared Data Block), byteIndexInBuffer (an integer), elementSize (a non-negative integer), expectedBytes (a List of byte values), and replacementBytes (a List of byte values) and returns a List of byte values. It performs the following steps when called:
The abstract operation AtomicReadModifyWrite takes arguments typedArray (an ECMAScript language value), index (an ECMAScript language value), value (an ECMAScript language value), and op (a read-modify-write modification function) and returns either a normal completion containing either a Number or a BigInt, or a throw completion. op takes two List of byte values arguments and returns a List of byte values. This operation atomically loads a value, combines it with another value, and stores the result of the combination. It returns the loaded value. It performs the following steps when called:
The abstract operation ByteListBitwiseOp takes arguments op (&, ^, or |), xBytes (a List of byte values), and yBytes (a List of byte values) and returns a List of byte values. The operation atomically performs a bitwise operation on all byte values of the arguments and returns a List of byte values. It performs the following steps when called:
The abstract operation ByteListEqual takes arguments xBytes (a List of byte values) and yBytes (a List of byte values) and returns a Boolean. It performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function is an optimization primitive. The intuition is that if the atomic step of an atomic primitive (compareExchange, load, store, add, sub, and, or, xor, or exchange) on a datum of size n bytes will be performed without the surrounding agent acquiring a lock outside the n bytes comprising the datum, then Atomics.isLockFree(n) will return true. High-performance algorithms will use this function to determine whether to use locks or atomic operations in critical sections. If an atomic primitive is not lock-free then it is often more efficient for an algorithm to provide its own locking.
Atomics.isLockFree(4) always returns true as that can be supported on all known relevant hardware. Being able to assume this will generally simplify programs.
Regardless of the value returned by this function, all atomic operations are guaranteed to be atomic. For example, they will never have a visible operation take place in the middle of the operation (e.g., "tearing").
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function puts the surrounding agent in a wait queue and suspends it until notified or until the wait times out, returning a String differentiating those cases.
It performs the following steps when called:
This function returns a Promise that is resolved when the calling agent is notified or the the timeout is reached.
It performs the following steps when called:
This function notifies some agents that are sleeping in the wait queue.
It performs the following steps when called:
This function performs the following steps when called:
The initial value of the @@toStringTag property is the String value "Atomics".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The JSON object:
The JSON Data Interchange Format is defined in ECMA-404. The JSON interchange format used in this specification is exactly that described by ECMA-404. Conforming implementations of JSON.parse and JSON.stringify must support the exact interchange format described in the ECMA-404 specification without any deletions or extensions to the format.
This function parses a JSON text (a JSON-formatted String) and produces an ECMAScript language value. The JSON format represents literals, arrays, and objects with a syntax similar to the syntax for ECMAScript literals, Array Initializers, and Object Initializers. After parsing, JSON objects are realized as ECMAScript objects. JSON arrays are realized as ECMAScript Array instances. JSON strings, numbers, booleans, and null are realized as ECMAScript Strings, Numbers, Booleans, and null.
The optional reviver parameter is a function that takes two parameters, key and value. It can filter and transform the results. It is called with each of the key/value pairs produced by the parse, and its return value is used instead of the original value. If it returns what it received, the structure is not modified. If it returns undefined then the property is deleted from the result.
The "length" property of this function is 2ùîΩ.
Valid JSON text is a subset of the ECMAScript PrimaryExpression syntax. Step 2 verifies that jsonString conforms to that subset, and step 10 asserts that that parsing and evaluation returns a value of an appropriate type.
However, because 13.2.5.5 behaves differently during JSON.parse, the same source text can produce different results when evaluated as a PrimaryExpression rather than as JSON. Furthermore, the Early Error for duplicate "__proto__" properties in object literals, which likewise does not apply during JSON.parse, means that not all texts accepted by JSON.parse are valid as a PrimaryExpression, despite matching the grammar.
The abstract operation InternalizeJSONProperty takes arguments holder (an Object), name (a String), and reviver (a function object) and returns either a normal completion containing an ECMAScript language value or a throw completion.
This algorithm intentionally does not throw an exception if either [[Delete]] or CreateDataProperty return false.
It performs the following steps when called:
It is not permitted for a conforming implementation of JSON.parse to extend the JSON grammars. If an implementation wishes to support a modified or extended JSON interchange format it must do so by defining a different parse function.
In the case where there are duplicate name Strings within an object, lexically preceding values for the same key shall be overwritten.
This function returns a String in UTF-16 encoded JSON format representing an ECMAScript language value, or undefined. It can take three parameters. The value parameter is an ECMAScript language value, which is usually an object or array, although it can also be a String, Boolean, Number or null. The optional replacer parameter is either a function that alters the way objects and arrays are stringified, or an array of Strings and Numbers that acts as an inclusion list for selecting the object properties that will be stringified. The optional space parameter is a String or Number that allows the result to have white space injected into it to improve human readability.
It performs the following steps when called:
The "length" property of this function is 3ùîΩ.
JSON structures are allowed to be nested to any depth, but they must be acyclic. If value is or contains a cyclic structure, then this function must throw a TypeError exception. This is an example of a value that cannot be stringified:
Symbolic primitive values are rendered as follows:
String values are wrapped in QUOTATION MARK (") code units. The code units " and \ are escaped with \ prefixes. Control characters code units are replaced with escape sequences \uHHHH, or with the shorter forms, \b (BACKSPACE), \f (FORM FEED), \n (LINE FEED), \r (CARRIAGE RETURN), \t (CHARACTER TABULATION).
Finite numbers are stringified as if by calling ToString(number). NaN and Infinity regardless of sign are represented as the String value "null".
Values that do not have a JSON representation (such as undefined and functions) do not produce a String. Instead they produce the undefined value. In arrays these values are represented as the String value "null". In objects an unrepresentable value causes the property to be excluded from stringification.
An object is rendered as U+007B (LEFT CURLY BRACKET) followed by zero or more properties, separated with a U+002C (COMMA), closed with a U+007D (RIGHT CURLY BRACKET). A property is a quoted String representing the property name, a U+003A (COLON), and then the stringified property value. An array is rendered as an opening U+005B (LEFT SQUARE BRACKET) followed by zero or more values, separated with a U+002C (COMMA), closed with a U+005D (RIGHT SQUARE BRACKET).
A JSON Serialization Record is a Record value used to enable serialization to the JSON format.
JSON Serialization Records have the fields listed in Table 75.
The abstract operation SerializeJSONProperty takes arguments state (a JSON Serialization Record), key (a String), and holder (an Object) and returns either a normal completion containing either a String or undefined, or a throw completion. It performs the following steps when called:
The abstract operation QuoteJSONString takes argument value (a String) and returns a String. It wraps value in 0x0022 (QUOTATION MARK) code units and escapes certain other code units within it. This operation interprets value as a sequence of UTF-16 encoded code points, as described in 6.1.4. It performs the following steps when called:
The abstract operation UnicodeEscape takes argument C (a code unit) and returns a String. It represents C as a Unicode escape sequence. It performs the following steps when called:
The abstract operation SerializeJSONObject takes arguments state (a JSON Serialization Record) and value (an Object) and returns either a normal completion containing a String or a throw completion. It serializes an object. It performs the following steps when called:
The abstract operation SerializeJSONArray takes arguments state (a JSON Serialization Record) and value (an ECMAScript language value) and returns either a normal completion containing a String or a throw completion. It serializes an array. It performs the following steps when called:
The representation of arrays includes only the elements in the interval from +0ùîΩ (inclusive) to array.length (exclusive). Properties whose keys are not array indices are excluded from the stringification. An array is stringified as an opening LEFT SQUARE BRACKET, elements separated by COMMA, and a closing RIGHT SQUARE BRACKET.
The initial value of the @@toStringTag property is the String value "JSON".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
A WeakRef is an object that is used to refer to a target object or symbol without preserving it from garbage collection. WeakRefs can be dereferenced to allow access to the target value, if the target hasn't been reclaimed by garbage collection.
The WeakRef constructor:
This function performs the following steps when called:
The WeakRef constructor:
The initial value of WeakRef.prototype is the WeakRef prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The WeakRef prototype object:
The initial value of WeakRef.prototype.constructor is %WeakRef%.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
This method performs the following steps when called:
If the WeakRef returns a target value that is not undefined, then this target value should not be garbage collected until the current execution of ECMAScript code has completed. The AddToKeptObjects operation makes sure read consistency is maintained.
In the above example, if the first deref does not evaluate to undefined then the second deref cannot either.
The initial value of the @@toStringTag property is the String value "WeakRef".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The abstract operation WeakRefDeref takes argument weakRef (a WeakRef) and returns an ECMAScript language value. It performs the following steps when called:
This abstract operation is defined separately from WeakRef.prototype.deref strictly to make it possible to succinctly define liveness.
WeakRef instances are ordinary objects that inherit properties from the WeakRef prototype. WeakRef instances also have a [[WeakRefTarget]] internal slot.
A FinalizationRegistry is an object that manages registration and unregistration of cleanup operations that are performed when target objects and symbols are garbage collected.
The FinalizationRegistry constructor:
This function performs the following steps when called:
The FinalizationRegistry constructor:
The initial value of FinalizationRegistry.prototype is the FinalizationRegistry prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The FinalizationRegistry prototype object:
The initial value of FinalizationRegistry.prototype.constructor is %FinalizationRegistry%.
This method performs the following steps when called:
Based on the algorithms and definitions in this specification, cell.[[HeldValue]] is live when finalizationRegistry.[[Cells]] contains cell; however, this does not necessarily mean that cell.[[UnregisterToken]] or cell.[[Target]] are live. For example, registering an object with itself as its unregister token would not keep the object alive forever.
This method performs the following steps when called:
The initial value of the @@toStringTag property is the String value "FinalizationRegistry".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
FinalizationRegistry instances are ordinary objects that inherit properties from the FinalizationRegistry prototype. FinalizationRegistry instances also have [[Cells]] and [[CleanupCallback]] internal slots.
An interface is a set of property keys whose associated values match a specific specification. Any object that provides all the properties as described by an interface's specification conforms to that interface. An interface is not represented by a distinct object. There may be many separately implemented objects that conform to any interface. An individual object may conform to multiple interfaces.
The Iterable interface includes the property described in Table 77:
An object that implements the Iterator interface must include the property in Table 78. Such objects may also implement the properties in Table 79.
Arguments may be passed to the next function but their interpretation and validity is dependent upon the target Iterator. The for-of statement and other common users of Iterators do not pass any arguments, so Iterator objects that expect to be used in such a manner must be prepared to deal with being called with no arguments.
Typically callers of these methods should check for their existence before invoking them. Certain ECMAScript language features including for-of, yield*, and array destructuring call these methods after performing an existence check. Most ECMAScript library functions that accept Iterable objects as arguments also conditionally call them.
The AsyncIterable interface includes the properties described in Table 80:
An object that implements the AsyncIterator interface must include the properties in Table 81. Such objects may also implement the properties in Table 82.
The returned promise, when fulfilled, must fulfill with an object that conforms to the IteratorResult interface. If a previous call to the next method of an AsyncIterator has returned a promise for an IteratorResult object whose "done" property is true, then all subsequent calls to the next method of that object should also return a promise for an IteratorResult object whose "done" property is true. However, this requirement is not enforced.
Additionally, the IteratorResult object that serves as a fulfillment value should have a "value" property whose value is not a promise (or "thenable"). However, this requirement is also not enforced.
Arguments may be passed to the next function but their interpretation and validity is dependent upon the target AsyncIterator. The for-await-of statement and other common users of AsyncIterators do not pass any arguments, so AsyncIterator objects that expect to be used in such a manner must be prepared to deal with being called with no arguments.
The returned promise, when fulfilled, must fulfill with an object that conforms to the IteratorResult interface. Invoking this method notifies the AsyncIterator object that the caller does not intend to make any more next method calls to the AsyncIterator. The returned promise will fulfill with an IteratorResult object which will typically have a "done" property whose value is true, and a "value" property with the value passed as the argument of the return method. However, this requirement is not enforced.
Additionally, the IteratorResult object that serves as a fulfillment value should have a "value" property whose value is not a promise (or "thenable"). If the argument value is used in the typical manner, then if it is a rejected promise, a promise rejected with the same reason should be returned; if it is a fulfilled promise, then its fulfillment value should be used as the "value" property of the returned promise's IteratorResult object fulfillment value. However, these requirements are also not enforced.
The returned promise, when fulfilled, must fulfill with an object that conforms to the IteratorResult interface. Invoking this method notifies the AsyncIterator object that the caller has detected an error condition. The argument may be used to identify the error condition and typically will be an exception object. A typical response is to return a rejected promise which rejects with the value passed as the argument.
If the returned promise is fulfilled, the IteratorResult fulfillment value will typically have a "done" property whose value is true. Additionally, it should have a "value" property whose value is not a promise (or "thenable"), but this requirement is not enforced.
Typically callers of these methods should check for their existence before invoking them. Certain ECMAScript language features including for-await-of and yield* call these methods after performing an existence check.
The IteratorResult interface includes the properties listed in Table 83:
The %IteratorPrototype% object:
All objects defined in this specification that implement the Iterator interface also inherit from %IteratorPrototype%. ECMAScript code may also define objects that inherit from %IteratorPrototype%. The %IteratorPrototype% object provides a place where additional methods that are applicable to all iterator objects may be added.
The following expression is one way that ECMAScript code can access the %IteratorPrototype% object:
This function performs the following steps when called:
The value of the "name" property of this function is "[Symbol.iterator]".
The %AsyncIteratorPrototype% object:
All objects defined in this specification that implement the AsyncIterator interface also inherit from %AsyncIteratorPrototype%. ECMAScript code may also define objects that inherit from %AsyncIteratorPrototype%. The %AsyncIteratorPrototype% object provides a place where additional methods that are applicable to all async iterator objects may be added.
This function performs the following steps when called:
The value of the "name" property of this function is "[Symbol.asyncIterator]".
An Async-from-Sync Iterator object is an async iterator that adapts a specific synchronous iterator. There is not a named constructor for Async-from-Sync Iterator objects. Instead, Async-from-Sync iterator objects are created by the CreateAsyncFromSyncIterator abstract operation as needed.
The abstract operation CreateAsyncFromSyncIterator takes argument syncIteratorRecord (an Iterator Record) and returns an Iterator Record. It is used to create an async Iterator Record from a synchronous Iterator Record. It performs the following steps when called:
The %AsyncFromSyncIteratorPrototype% object:
Async-from-Sync Iterator instances are ordinary objects that inherit properties from the %AsyncFromSyncIteratorPrototype% intrinsic object. Async-from-Sync Iterator instances are initially created with the internal slots listed in Table 84. Async-from-Sync Iterator instances are not directly observable from ECMAScript code.
The abstract operation AsyncFromSyncIteratorContinuation takes arguments result (an Object) and promiseCapability (a PromiseCapability Record for an intrinsic %Promise%) and returns a Promise. It performs the following steps when called:
A Promise is an object that is used as a placeholder for the eventual results of a deferred (and possibly asynchronous) computation.
Any Promise is in one of three mutually exclusive states: fulfilled, rejected, and pending:
A promise is said to be settled if it is not pending, i.e. if it is either fulfilled or rejected.
A promise is resolved if it is settled or if it has been ‚Äúlocked in‚Äù to match the state of another promise. Attempting to resolve or reject a resolved promise has no effect. A promise is unresolved if it is not resolved. An unresolved promise is always in the pending state. A resolved promise may be pending, fulfilled or rejected.
A PromiseCapability Record is a Record value used to encapsulate a Promise or promise-like object along with the functions that are capable of resolving or rejecting that promise. PromiseCapability Records are produced by the NewPromiseCapability abstract operation.
PromiseCapability Records have the fields listed in Table 85.
IfAbruptRejectPromise is a shorthand for a sequence of algorithm steps that use a PromiseCapability Record. An algorithm step of the form:
means the same thing as:
A PromiseReaction Record is a Record value used to store information about how a promise should react when it becomes resolved or rejected with a given value. PromiseReaction Records are created by the PerformPromiseThen abstract operation, and are used by the Abstract Closure returned by NewPromiseReactionJob.
PromiseReaction Records have the fields listed in Table 86.
The abstract operation CreateResolvingFunctions takes argument promise (a Promise) and returns a Record with fields [[Resolve]] (a function object) and [[Reject]] (a function object). It performs the following steps when called:
A promise reject function is an anonymous built-in function that has [[Promise]] and [[AlreadyResolved]] internal slots.
When a promise reject function is called with argument reason, the following steps are taken:
The "length" property of a promise reject function is 1ùîΩ.
A promise resolve function is an anonymous built-in function that has [[Promise]] and [[AlreadyResolved]] internal slots.
When a promise resolve function is called with argument resolution, the following steps are taken:
The "length" property of a promise resolve function is 1ùîΩ.
The abstract operation FulfillPromise takes arguments promise (a Promise) and value (an ECMAScript language value) and returns unused. It performs the following steps when called:
The abstract operation NewPromiseCapability takes argument C (an ECMAScript language value) and returns either a normal completion containing a PromiseCapability Record or a throw completion. It attempts to use C as a constructor in the fashion of the built-in Promise constructor to create a promise and extract its resolve and reject functions. The promise plus the resolve and reject functions are used to initialize a new PromiseCapability Record. It performs the following steps when called:
This abstract operation supports Promise subclassing, as it is generic on any constructor that calls a passed executor function argument in the same way as the Promise constructor. It is used to generalize static methods of the Promise constructor to any subclass.
The abstract operation IsPromise takes argument x (an ECMAScript language value) and returns a Boolean. It checks for the promise brand on an object. It performs the following steps when called:
The abstract operation RejectPromise takes arguments promise (a Promise) and reason (an ECMAScript language value) and returns unused. It performs the following steps when called:
The abstract operation TriggerPromiseReactions takes arguments reactions (a List of PromiseReaction Records) and argument (an ECMAScript language value) and returns unused. It enqueues a new Job for each record in reactions. Each such Job processes the [[Type]] and [[Handler]] of the PromiseReaction Record, and if the [[Handler]] is not empty, calls it passing the given argument. If the [[Handler]] is empty, the behaviour is determined by the [[Type]]. It performs the following steps when called:
The host-defined abstract operation HostPromiseRejectionTracker takes arguments promise (a Promise) and operation ("reject" or "handle") and returns unused. It allows host environments to track promise rejections.
The default implementation of HostPromiseRejectionTracker is to return unused.
HostPromiseRejectionTracker is called in two scenarios:
A typical implementation of HostPromiseRejectionTracker might try to notify developers of unhandled rejections, while also being careful to notify them if such previous notifications are later invalidated by new handlers being attached.
If operation is "handle", an implementation should not hold a reference to promise in a way that would interfere with garbage collection. An implementation may hold a reference to promise if operation is "reject", since it is expected that rejections will be rare and not on hot code paths.
The abstract operation NewPromiseReactionJob takes arguments reaction (a PromiseReaction Record) and argument (an ECMAScript language value) and returns a Record with fields [[Job]] (a Job Abstract Closure) and [[Realm]] (a Realm Record or null). It returns a new Job Abstract Closure that applies the appropriate handler to the incoming value, and uses the handler's return value to resolve or reject the derived promise associated with that handler. It performs the following steps when called:
The abstract operation NewPromiseResolveThenableJob takes arguments promiseToResolve (a Promise), thenable (an Object), and then (a JobCallback Record) and returns a Record with fields [[Job]] (a Job Abstract Closure) and [[Realm]] (a Realm Record). It performs the following steps when called:
This Job uses the supplied thenable and its then method to resolve the given promise. This process must take place as a Job to ensure that the evaluation of the then method occurs after evaluation of any surrounding code has completed.
The Promise constructor:
This function performs the following steps when called:
The executor argument must be a function object. It is called for initiating and reporting completion of the possibly deferred action represented by this Promise. The executor is called with two arguments: resolve and reject. These are functions that may be used by the executor function to report eventual completion or failure of the deferred computation. Returning from the executor function does not mean that the deferred action has been completed but only that the request to eventually perform the deferred action has been accepted.
The resolve function that is passed to an executor function accepts a single argument. The executor code may eventually call the resolve function to indicate that it wishes to resolve the associated Promise. The argument passed to the resolve function represents the eventual value of the deferred action and can be either the actual fulfillment value or another promise which will provide the value if it is fulfilled.
The reject function that is passed to an executor function accepts a single argument. The executor code may eventually call the reject function to indicate that the associated Promise is rejected and will never be fulfilled. The argument passed to the reject function is used as the rejection value of the promise. Typically it will be an Error object.
The resolve and reject functions passed to an executor function by the Promise constructor have the capability to actually resolve and reject the associated promise. Subclasses may have different constructor behaviour that passes in customized values for resolve and reject.
The Promise constructor:
This function returns a new promise which is fulfilled with an array of fulfillment values for the passed promises, or rejects with the reason of the first passed promise that rejects. It resolves all elements of the passed iterable to promises as it runs this algorithm.
This function requires its this value to be a constructor function that supports the parameter conventions of the Promise constructor.
The abstract operation GetPromiseResolve takes argument promiseConstructor (a constructor) and returns either a normal completion containing a function object or a throw completion. It performs the following steps when called:
The abstract operation PerformPromiseAll takes arguments iteratorRecord (an Iterator Record), constructor (a constructor), resultCapability (a PromiseCapability Record), and promiseResolve (a function object) and returns either a normal completion containing an ECMAScript language value or a throw completion. It performs the following steps when called:
A Promise.all resolve element function is an anonymous built-in function that is used to resolve a specific Promise.all element. Each Promise.all resolve element function has [[Index]], [[Values]], [[Capability]], [[RemainingElements]], and [[AlreadyCalled]] internal slots.
When a Promise.all resolve element function is called with argument x, the following steps are taken:
The "length" property of a Promise.all resolve element function is 1ùîΩ.
This function returns a promise that is fulfilled with an array of promise state snapshots, but only after all the original promises have settled, i.e. become either fulfilled or rejected. It resolves all elements of the passed iterable to promises as it runs this algorithm.
This function requires its this value to be a constructor function that supports the parameter conventions of the Promise constructor.
The abstract operation PerformPromiseAllSettled takes arguments iteratorRecord (an Iterator Record), constructor (a constructor), resultCapability (a PromiseCapability Record), and promiseResolve (a function object) and returns either a normal completion containing an ECMAScript language value or a throw completion. It performs the following steps when called:
A Promise.allSettled resolve element function is an anonymous built-in function that is used to resolve a specific Promise.allSettled element. Each Promise.allSettled resolve element function has [[Index]], [[Values]], [[Capability]], [[RemainingElements]], and [[AlreadyCalled]] internal slots.
When a Promise.allSettled resolve element function is called with argument x, the following steps are taken:
The "length" property of a Promise.allSettled resolve element function is 1ùîΩ.
A Promise.allSettled reject element function is an anonymous built-in function that is used to reject a specific Promise.allSettled element. Each Promise.allSettled reject element function has [[Index]], [[Values]], [[Capability]], [[RemainingElements]], and [[AlreadyCalled]] internal slots.
When a Promise.allSettled reject element function is called with argument x, the following steps are taken:
The "length" property of a Promise.allSettled reject element function is 1ùîΩ.
This function returns a promise that is fulfilled by the first given promise to be fulfilled, or rejected with an AggregateError holding the rejection reasons if all of the given promises are rejected. It resolves all elements of the passed iterable to promises as it runs this algorithm.
This function requires its this value to be a constructor function that supports the parameter conventions of the Promise constructor.
The abstract operation PerformPromiseAny takes arguments iteratorRecord (an Iterator Record), constructor (a constructor), resultCapability (a PromiseCapability Record), and promiseResolve (a function object) and returns either a normal completion containing an ECMAScript language value or a throw completion. It performs the following steps when called:
A Promise.any reject element function is an anonymous built-in function that is used to reject a specific Promise.any element. Each Promise.any reject element function has [[Index]], [[Errors]], [[Capability]], [[RemainingElements]], and [[AlreadyCalled]] internal slots.
When a Promise.any reject element function is called with argument x, the following steps are taken:
The "length" property of a Promise.any reject element function is 1ùîΩ.
The initial value of Promise.prototype is the Promise prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
This function returns a new promise which is settled in the same way as the first passed promise to settle. It resolves all elements of the passed iterable to promises as it runs this algorithm.
If the iterable argument yields no values or if none of the promises yielded by iterable ever settle, then the pending promise returned by this method will never be settled.
This function expects its this value to be a constructor function that supports the parameter conventions of the Promise constructor. It also expects that its this value provides a resolve method.
The abstract operation PerformPromiseRace takes arguments iteratorRecord (an Iterator Record), constructor (a constructor), resultCapability (a PromiseCapability Record), and promiseResolve (a function object) and returns either a normal completion containing an ECMAScript language value or a throw completion. It performs the following steps when called:
This function returns a new promise rejected with the passed argument.
This function expects its this value to be a constructor function that supports the parameter conventions of the Promise constructor.
This function returns either a new promise resolved with the passed argument, or the argument itself if the argument is a promise produced by this constructor.
This function expects its this value to be a constructor function that supports the parameter conventions of the Promise constructor.
The abstract operation PromiseResolve takes arguments C (a constructor) and x (an ECMAScript language value) and returns either a normal completion containing an ECMAScript language value or a throw completion. It returns a new promise resolved with x. It performs the following steps when called:
This function returns an object with three properties: a new promise together with the resolve and reject functions associated with it.
Promise[@@species] is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps when called:
The value of the "name" property of this function is "get [Symbol.species]".
Promise prototype methods normally use their this value's constructor to create a derived object. However, a subclass constructor may over-ride that default behaviour by redefining its @@species property.
The Promise prototype object:
This method performs the following steps when called:
The initial value of Promise.prototype.constructor is %Promise%.
This method performs the following steps when called:
This method performs the following steps when called:
The abstract operation PerformPromiseThen takes arguments promise (a Promise), onFulfilled (an ECMAScript language value), and onRejected (an ECMAScript language value) and optional argument resultCapability (a PromiseCapability Record) and returns an ECMAScript language value. It performs the ‚Äúthen‚Äù operation on promise using onFulfilled and onRejected as its settlement actions. If resultCapability is passed, the result is stored by updating resultCapability's promise. If it is not passed, then PerformPromiseThen is being called by a specification-internal operation where the result does not matter. It performs the following steps when called:
The initial value of the @@toStringTag property is the String value "Promise".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
Promise instances are ordinary objects that inherit properties from the Promise prototype object (the intrinsic, %Promise.prototype%). Promise instances are initially created with the internal slots described in Table 87.
GeneratorFunctions are functions that are usually created by evaluating GeneratorDeclarations, GeneratorExpressions, and GeneratorMethods. They may also be created by calling the %GeneratorFunction% intrinsic.
The GeneratorFunction constructor:
The last argument (if any) specifies the body (executable code) of a generator function; any preceding arguments specify formal parameters.
This function performs the following steps when called:
See NOTE for 20.2.1.1.
The GeneratorFunction constructor:
The initial value of GeneratorFunction.prototype is the GeneratorFunction prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The GeneratorFunction prototype object:
The initial value of GeneratorFunction.prototype.constructor is %GeneratorFunction%.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The initial value of GeneratorFunction.prototype.prototype is the Generator prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The initial value of the @@toStringTag property is the String value "GeneratorFunction".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
Every GeneratorFunction instance is an ECMAScript function object and has the internal slots listed in Table 30. The value of the [[IsClassConstructor]] internal slot for all such instances is false.
Each GeneratorFunction instance has the following own properties:
The specification for the "length" property of Function instances given in 20.2.4.1 also applies to GeneratorFunction instances.
The specification for the "name" property of Function instances given in 20.2.4.2 also applies to GeneratorFunction instances.
Whenever a GeneratorFunction instance is created another ordinary object is also created and is the initial value of the generator function's "prototype" property. The value of the prototype property is used to initialize the [[Prototype]] internal slot of a newly created Generator when the generator function object is invoked using [[Call]].
This property has the attributes { [[Writable]]: true, [[Enumerable]]: false, [[Configurable]]: false }.
Unlike Function instances, the object that is the value of a GeneratorFunction's "prototype" property does not have a "constructor" property whose value is the GeneratorFunction instance.
AsyncGeneratorFunctions are functions that are usually created by evaluating AsyncGeneratorDeclaration, AsyncGeneratorExpression, and AsyncGeneratorMethod syntactic productions. They may also be created by calling the %AsyncGeneratorFunction% intrinsic.
The AsyncGeneratorFunction constructor:
The last argument (if any) specifies the body (executable code) of an async generator function; any preceding arguments specify formal parameters.
This function performs the following steps when called:
See NOTE for 20.2.1.1.
The AsyncGeneratorFunction constructor:
The initial value of AsyncGeneratorFunction.prototype is the AsyncGeneratorFunction prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The AsyncGeneratorFunction prototype object:
The initial value of AsyncGeneratorFunction.prototype.constructor is %AsyncGeneratorFunction%.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The initial value of AsyncGeneratorFunction.prototype.prototype is the AsyncGenerator prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The initial value of the @@toStringTag property is the String value "AsyncGeneratorFunction".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
Every AsyncGeneratorFunction instance is an ECMAScript function object and has the internal slots listed in Table 30. The value of the [[IsClassConstructor]] internal slot for all such instances is false.
Each AsyncGeneratorFunction instance has the following own properties:
The value of the "length" property is an integral Number that indicates the typical number of arguments expected by the AsyncGeneratorFunction. However, the language permits the function to be invoked with some other number of arguments. The behaviour of an AsyncGeneratorFunction when invoked on a number of arguments other than the number specified by its "length" property depends on the function.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The specification for the "name" property of Function instances given in 20.2.4.2 also applies to AsyncGeneratorFunction instances.
Whenever an AsyncGeneratorFunction instance is created, another ordinary object is also created and is the initial value of the async generator function's "prototype" property. The value of the prototype property is used to initialize the [[Prototype]] internal slot of a newly created AsyncGenerator when the generator function object is invoked using [[Call]].
This property has the attributes { [[Writable]]: true, [[Enumerable]]: false, [[Configurable]]: false }.
Unlike function instances, the object that is the value of an AsyncGeneratorFunction's "prototype" property does not have a "constructor" property whose value is the AsyncGeneratorFunction instance.
A Generator is an instance of a generator function and conforms to both the Iterator and Iterable interfaces.
Generator instances directly inherit properties from the object that is the initial value of the "prototype" property of the Generator function that created the instance. Generator instances indirectly inherit properties from the Generator Prototype intrinsic, %GeneratorFunction.prototype.prototype%.
The Generator prototype object:
The initial value of Generator.prototype.constructor is %GeneratorFunction.prototype%.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
This method performs the following steps when called:
This method performs the following steps when called:
The initial value of the @@toStringTag property is the String value "Generator".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
Generator instances are initially created with the internal slots described in Table 88.
The abstract operation GeneratorStart takes arguments generator (a Generator) and generatorBody (a FunctionBody Parse Node or an Abstract Closure with no parameters) and returns unused. It performs the following steps when called:
The abstract operation GeneratorValidate takes arguments generator (an ECMAScript language value) and generatorBrand (a String or empty) and returns either a normal completion containing one of suspended-start, suspended-yield, or completed, or a throw completion. It performs the following steps when called:
The abstract operation GeneratorResume takes arguments generator (an ECMAScript language value), value (an ECMAScript language value or empty), and generatorBrand (a String or empty) and returns either a normal completion containing an ECMAScript language value or a throw completion. It performs the following steps when called:
The abstract operation GeneratorResumeAbrupt takes arguments generator (an ECMAScript language value), abruptCompletion (a return completion or a throw completion), and generatorBrand (a String or empty) and returns either a normal completion containing an ECMAScript language value or a throw completion. It performs the following steps when called:
The abstract operation GetGeneratorKind takes no arguments and returns non-generator, sync, or async. It performs the following steps when called:
The abstract operation GeneratorYield takes argument iterNextObj (an Object that conforms to the IteratorResult interface) and returns either a normal completion containing an ECMAScript language value or an abrupt completion. It performs the following steps when called:
The abstract operation Yield takes argument value (an ECMAScript language value) and returns either a normal completion containing an ECMAScript language value or an abrupt completion. It performs the following steps when called:
The abstract operation CreateIteratorFromClosure takes arguments closure (an Abstract Closure with no parameters), generatorBrand (a String or empty), and generatorPrototype (an Object) and returns a Generator. It performs the following steps when called:
An AsyncGenerator is an instance of an async generator function and conforms to both the AsyncIterator and AsyncIterable interfaces.
AsyncGenerator instances directly inherit properties from the object that is the initial value of the "prototype" property of the AsyncGenerator function that created the instance. AsyncGenerator instances indirectly inherit properties from the AsyncGenerator Prototype intrinsic, %AsyncGeneratorFunction.prototype.prototype%.
The AsyncGenerator prototype object:
The initial value of AsyncGenerator.prototype.constructor is %AsyncGeneratorFunction.prototype%.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The initial value of the @@toStringTag property is the String value "AsyncGenerator".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
AsyncGenerator instances are initially created with the internal slots described below:
An AsyncGeneratorRequest is a Record value used to store information about how an async generator should be resumed and contains capabilities for fulfilling or rejecting the corresponding promise.
They have the following fields:
The abstract operation AsyncGeneratorStart takes arguments generator (an AsyncGenerator) and generatorBody (a FunctionBody Parse Node or an Abstract Closure with no parameters) and returns unused. It performs the following steps when called:
The abstract operation AsyncGeneratorValidate takes arguments generator (an ECMAScript language value) and generatorBrand (a String or empty) and returns either a normal completion containing unused or a throw completion. It performs the following steps when called:
The abstract operation AsyncGeneratorEnqueue takes arguments generator (an AsyncGenerator), completion (a Completion Record), and promiseCapability (a PromiseCapability Record) and returns unused. It performs the following steps when called:
The abstract operation AsyncGeneratorCompleteStep takes arguments generator (an AsyncGenerator), completion (a Completion Record), and done (a Boolean) and optional argument realm (a Realm Record) and returns unused. It performs the following steps when called:
The abstract operation AsyncGeneratorResume takes arguments generator (an AsyncGenerator) and completion (a Completion Record) and returns unused. It performs the following steps when called:
The abstract operation AsyncGeneratorUnwrapYieldResumption takes argument resumptionValue (a Completion Record) and returns either a normal completion containing an ECMAScript language value or an abrupt completion. It performs the following steps when called:
The abstract operation AsyncGeneratorYield takes argument value (an ECMAScript language value) and returns either a normal completion containing an ECMAScript language value or an abrupt completion. It performs the following steps when called:
The abstract operation AsyncGeneratorAwaitReturn takes argument generator (an AsyncGenerator) and returns either a normal completion containing unused or a throw completion. It performs the following steps when called:
The abstract operation AsyncGeneratorDrainQueue takes argument generator (an AsyncGenerator) and returns unused. It drains the generator's AsyncGeneratorQueue until it encounters an AsyncGeneratorRequest which holds a return completion. It performs the following steps when called:
The abstract operation CreateAsyncIteratorFromClosure takes arguments closure (an Abstract Closure with no parameters), generatorBrand (a String or empty), and generatorPrototype (an Object) and returns an AsyncGenerator. It performs the following steps when called:
AsyncFunctions are functions that are usually created by evaluating AsyncFunctionDeclarations, AsyncFunctionExpressions, AsyncMethods, and AsyncArrowFunctions. They may also be created by calling the %AsyncFunction% intrinsic.
The AsyncFunction constructor:
The last argument (if any) specifies the body (executable code) of an async function. Any preceding arguments specify formal parameters.
This function performs the following steps when called:
The AsyncFunction constructor:
The initial value of AsyncFunction.prototype is the AsyncFunction prototype object.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The AsyncFunction prototype object:
The initial value of AsyncFunction.prototype.constructor is %AsyncFunction%.
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The initial value of the @@toStringTag property is the String value "AsyncFunction".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
Every AsyncFunction instance is an ECMAScript function object and has the internal slots listed in Table 30. The value of the [[IsClassConstructor]] internal slot for all such instances is false. AsyncFunction instances are not constructors and do not have a [[Construct]] internal method. AsyncFunction instances do not have a prototype property as they are not constructible.
Each AsyncFunction instance has the following own properties:
The specification for the "length" property of Function instances given in 20.2.4.1 also applies to AsyncFunction instances.
The specification for the "name" property of Function instances given in 20.2.4.2 also applies to AsyncFunction instances.
The abstract operation AsyncFunctionStart takes arguments promiseCapability (a PromiseCapability Record) and asyncFunctionBody (a FunctionBody Parse Node or an ExpressionBody Parse Node) and returns unused. It performs the following steps when called:
The abstract operation AsyncBlockStart takes arguments promiseCapability (a PromiseCapability Record), asyncBody (a Parse Node), and asyncContext (an execution context) and returns unused. It performs the following steps when called:
The abstract operation Await takes argument value (an ECMAScript language value) and returns either a normal completion containing either an ECMAScript language value or empty, or a throw completion. It performs the following steps when called:
The Reflect object:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
This function performs the following steps when called:
The initial value of the @@toStringTag property is the String value "Reflect".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
The Proxy constructor:
This function performs the following steps when called:
The Proxy constructor:
This function creates a revocable Proxy object.
It performs the following steps when called:
A Module Namespace Object is a module namespace exotic object that provides runtime property-based access to a module's exported bindings. There is no constructor function for Module Namespace Objects. Instead, such an object is created for each module that is imported by an ImportDeclaration that contains a NameSpaceImport.
In addition to the properties specified in 10.4.6 each Module Namespace Object has the following own property:
The initial value of the @@toStringTag property is the String value "Module".
This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
The memory consistency model, or memory model, specifies the possible orderings of Shared Data Block events, arising via accessing TypedArray instances backed by a SharedArrayBuffer and via methods on the Atomics object. When the program has no data races (defined below), the ordering of events appears as sequentially consistent, i.e., as an interleaving of actions from each agent. When the program has data races, shared memory operations may appear sequentially inconsistent. For example, programs may exhibit causality-violating behaviour and other astonishments. These astonishments arise from compiler transforms and the design of CPUs (e.g., out-of-order execution and speculation). The memory model defines both the precise conditions under which a program exhibits sequentially consistent behaviour as well as the possible values read from data races. To wit, there is no undefined behaviour.
The memory model is defined as relational constraints on events introduced by abstract operations on SharedArrayBuffer or by methods on the Atomics object during an evaluation.
This section provides an axiomatic model on events introduced by the abstract operations on SharedArrayBuffers. It bears stressing that the model is not expressible algorithmically, unlike the rest of this specification. The nondeterministic introduction of events by abstract operations is the interface between the operational semantics of ECMAScript evaluation and the axiomatic semantics of the memory model. The semantics of these events is defined by considering graphs of all events in an evaluation. These are neither Static Semantics nor Runtime Semantics. There is no demonstrated algorithmic implementation, but instead a set of constraints that determine if a particular event graph is allowed or disallowed.
Shared memory accesses (reads and writes) are divided into two groups, atomic accesses and data accesses, defined below. Atomic accesses are sequentially consistent, i.e., there is a strict total ordering of events agreed upon by all agents in an agent cluster. Non-atomic accesses do not have a strict total ordering agreed upon by all agents, i.e., unordered.
No orderings weaker than sequentially consistent and stronger than unordered, such as release-acquire, are supported.
A Shared Data Block event is either a ReadSharedMemory, WriteSharedMemory, or ReadModifyWriteSharedMemory Record.
These events are introduced by abstract operations or by methods on the Atomics object.
Some operations may also introduce Synchronize events. A Synchronize event has no fields, and exists purely to directly constrain the permitted orderings of other events.
In addition to Shared Data Block and Synchronize events, there are host-specific events.
Let the range of a ReadSharedMemory, WriteSharedMemory, or ReadModifyWriteSharedMemory event be the Set of contiguous integers from its [[ByteIndex]] to [[ByteIndex]] + [[ElementSize]] - 1. Two events' ranges are equal when the events have the same [[Block]], and the ranges are element-wise equal. Two events' ranges are overlapping when the events have the same [[Block]], the ranges are not equal and their intersection is non-empty. Two events' ranges are disjoint when the events do not have the same [[Block]] or their ranges are neither equal nor overlapping.
Examples of host-specific synchronizing events that should be accounted for are: sending a SharedArrayBuffer from one agent to another (e.g., by postMessage in a browser), starting and stopping agents, and communicating within the agent cluster via channels other than shared memory. It is assumed those events are appended to agent-order during evaluation like the other SharedArrayBuffer events.
Events are ordered within candidate executions by the relations defined below.
An Agent Events Record is a Record with the following fields.
A Chosen Value Record is a Record with the following fields.
A candidate execution of the evaluation of an agent cluster is a Record with the following fields.
An empty candidate execution is a candidate execution Record whose fields are empty Lists and Relations.
The abstract operation EventSet takes argument execution (a candidate execution) and returns a Set of events. It performs the following steps when called:
The abstract operation SharedDataBlockEventSet takes argument execution (a candidate execution) and returns a Set of events. It performs the following steps when called:
The abstract operation HostEventSet takes argument execution (a candidate execution) and returns a Set of events. It performs the following steps when called:
The abstract operation ComposeWriteEventBytes takes arguments execution (a candidate execution), byteIndex (a non-negative integer), and Ws (a List of either WriteSharedMemory or ReadModifyWriteSharedMemory events) and returns a List of byte values. It performs the following steps when called:
The read-modify-write modification [[ModifyOp]] is given by the function properties on the Atomics object that introduce ReadModifyWriteSharedMemory events.
This abstract operation composes a List of write events into a List of byte values. It is used in the event semantics of ReadSharedMemory and ReadModifyWriteSharedMemory events.
The abstract operation ValueOfReadEvent takes arguments execution (a candidate execution) and R (a ReadSharedMemory or ReadModifyWriteSharedMemory event) and returns a List of byte values. It performs the following steps when called:
For a candidate execution execution, execution.[[AgentOrder]] is a Relation on events that satisfies the following.
Each agent introduces events in a per-agent strict total order during the evaluation. This is the union of those strict total orders.
For a candidate execution execution, execution.[[ReadsBytesFrom]] is a mathematical function mapping events in SharedDataBlockEventSet(execution) to Lists of events in SharedDataBlockEventSet(execution) that satisfies the following conditions.
For each ReadSharedMemory or ReadModifyWriteSharedMemory event R in SharedDataBlockEventSet(execution), execution.[[ReadsBytesFrom]](R) is a List of length R.[[ElementSize]] whose elements are WriteSharedMemory or ReadModifyWriteSharedMemory events Ws such that all of the following are true.
For a candidate execution execution, execution.[[ReadsFrom]] is the least Relation on events that satisfies the following.
For a candidate execution execution, execution.[[HostSynchronizesWith]] is a host-provided strict partial order on host-specific events that satisfies at least the following.
For two host-specific events E and D, E host-synchronizes-with D implies E happens-before D.
The host-synchronizes-with relation allows the host to provide additional synchronization mechanisms, such as postMessage between HTML workers.
For a candidate execution execution, execution.[[SynchronizesWith]] is the least Relation on events that satisfies the following.
Owing to convention, write events synchronizes-with read events, instead of read events synchronizes-with write events.
init events do not participate in synchronizes-with, and are instead constrained directly by happens-before.
Not all seq-cst events related by reads-from are related by synchronizes-with. Only events that also have equal ranges are related by synchronizes-with.
For Shared Data Block events R and W such that W synchronizes-with R, R may reads-from other writes than W.
For a candidate execution execution, execution.[[HappensBefore]] is the least Relation on events that satisfies the following.
Because happens-before is a superset of agent-order, candidate executions are consistent with the single-thread evaluation semantics of ECMAScript.
A candidate execution execution has valid chosen reads if the following algorithm returns true.
A candidate execution execution has coherent reads if the following algorithm returns true.
A candidate execution execution has tear free reads if the following algorithm returns true.
An event's [[NoTear]] field is true when that event was introduced via accessing an integer TypedArray, and false when introduced via accessing a floating point TypedArray or DataView.
Intuitively, this requirement says when a memory range is accessed in an aligned fashion via an integer TypedArray, a single write event on that range must "win" when in a data race with other write events with equal ranges. More precisely, this requirement says an aligned read event cannot read a value composed of bytes from multiple, different write events all with equal ranges. It is possible, however, for an aligned read event to read from multiple write events with overlapping ranges.
For a candidate execution execution, memory-order is a strict total order of all events in EventSet(execution) that satisfies the following.
For each pair (R, W) in execution.[[ReadsFrom]], there is no WriteSharedMemory or ReadModifyWriteSharedMemory event V in SharedDataBlockEventSet(execution) such that V.[[Order]] is seq-cst, the pairs (W, V) and (V, R) are in memory-order, and any of the following conditions are true.
This clause additionally constrains seq-cst events on equal ranges.
For each WriteSharedMemory or ReadModifyWriteSharedMemory event W in SharedDataBlockEventSet(execution), if W.[[Order]] is seq-cst, then it is not the case that there is an infinite number of ReadSharedMemory or ReadModifyWriteSharedMemory events in SharedDataBlockEventSet(execution) with equal range that is memory-order before W.
This clause together with the forward progress guarantee on agents ensure the liveness condition that seq-cst writes become visible to seq-cst reads with equal range in finite time.
A candidate execution has sequentially consistent atomics if a memory-order exists.
While memory-order includes all events in EventSet(execution), those that are not constrained by happens-before or synchronizes-with are allowed to occur anywhere in the order.
A candidate execution execution is a valid execution (or simply an execution) if all of the following are true.
All programs have at least one valid execution.
For an execution execution, two events E and D in SharedDataBlockEventSet(execution) are in a race if the following algorithm returns true.
For an execution execution, two events E and D in SharedDataBlockEventSet(execution) are in a data race if the following algorithm returns true.
An execution execution is data race free if there are no two events in SharedDataBlockEventSet(execution) that are in a data race.
A program is data race free if all its executions are data race free.
The memory model guarantees sequential consistency of all events for data race free programs.
The following are guidelines for ECMAScript programmers working with shared memory.
We recommend programs be kept data race free, i.e., make it so that it is impossible for there to be concurrent non-atomic operations on the same memory location. Data race free programs have interleaving semantics where each step in the evaluation semantics of each agent are interleaved with each other. For data race free programs, it is not necessary to understand the details of the memory model. The details are unlikely to build intuition that will help one to better write ECMAScript.
More generally, even if a program is not data race free it may have predictable behaviour, so long as atomic operations are not involved in any data races and the operations that race all have the same access size. The simplest way to arrange for atomics not to be involved in races is to ensure that different memory cells are used by atomic and non-atomic operations and that atomic accesses of different sizes are not used to access the same cells at the same time. Effectively, the program should treat shared memory as strongly typed as much as possible. One still cannot depend on the ordering and timing of non-atomic accesses that race, but if memory is treated as strongly typed the racing accesses will not "tear" (bits of their values will not be mixed).
The following are guidelines for ECMAScript implementers writing compiler transformations for programs using shared memory.
It is desirable to allow most program transformations that are valid in a single-agent setting in a multi-agent setting, to ensure that the performance of each agent in a multi-agent program is as good as it would be in a single-agent setting. Frequently these transformations are hard to judge. We outline some rules about program transformations that are intended to be taken as normative (in that they are implied by the memory model or stronger than what the memory model implies) but which are likely not exhaustive. These rules are intended to apply to program transformations that precede the introductions of the events that make up the agent-order.
Let an agent-order slice be the subset of the agent-order pertaining to a single agent.
Let possible read values of a read event be the set of all values of ValueOfReadEvent for that event across all valid executions.
Any transformation of an agent-order slice that is valid in the absence of shared memory is valid in the presence of shared memory, with the following exceptions.
Atomics are carved in stone: Program transformations must not cause the seq-cst events in an agent-order slice to be reordered with its unordered operations, nor its seq-cst operations to be reordered with each other, nor may a program transformation remove a seq-cst operation from the agent-order.
(In practice, the prohibition on reorderings forces a compiler to assume that every seq-cst operation is a synchronization and included in the final memory-order, which it would usually have to assume anyway in the absence of inter-agent program analysis. It also forces the compiler to assume that every call where the callee's effects on the memory-order are unknown may contain seq-cst operations.)
Reads must be stable: Any given shared memory read must only observe a single value in an execution.
(For example, if what is semantically a single read in the program is executed multiple times then the program is subsequently allowed to observe only one of the values read. A transformation known as rematerialization can violate this rule.)
Writes must be stable: All observable writes to shared memory must follow from program semantics in an execution.
(For example, a transformation may not introduce certain observable writes, such as by using read-modify-write operations on a larger location to write a smaller datum, writing a value to memory that the program could not have written, or writing a just-read value back to the location it was read from, if that location could have been overwritten by another agent after the read.)
Possible read values must be non-empty: Program transformations cannot cause the possible read values of a shared memory read to become empty.
(Counterintuitively, this rule in effect restricts transformations on writes, because writes have force in memory model insofar as to be read by read events. For example, writes may be moved and coalesced and sometimes reordered between two seq-cst operations, but the transformation may not remove every write that updates a location; some write must be preserved.)
Examples of transformations that remain valid are: merging multiple non-atomic reads from the same location, reordering non-atomic reads, introducing speculative non-atomic reads, merging multiple non-atomic writes to the same location, reordering non-atomic writes to different locations, and hoisting non-atomic reads out of loops even if that affects termination. Note in general that aliased TypedArrays make it hard to prove that locations are different.
The following are guidelines for ECMAScript implementers generating machine code for shared memory accesses.
For architectures with memory models no weaker than those of ARM or Power, non-atomic stores and loads may be compiled to bare stores and loads on the target architecture. Atomic stores and loads may be compiled down to instructions that guarantee sequential consistency. If no such instructions exist, memory barriers are to be employed, such as placing barriers on both sides of a bare store or load. Read-modify-write operations may be compiled to read-modify-write instructions on the target architecture, such as LOCK-prefixed instructions on x86, load-exclusive/store-exclusive instructions on ARM, and load-link/store-conditional instructions on Power.
Specifically, the memory model is intended to allow code generation as follows.
Naive code generation uses these patterns:
That mapping is correct so long as an atomic operation on an address range does not race with a non-atomic write or with an atomic operation of different size. However, that is all we need: the memory model effectively demotes the atomic operations involved in a race to non-atomic status. On the other hand, the naive mapping is quite strong: it allows atomic operations to be used as sequentially consistent fences, which the memory model does not actually guarantee.
Local improvements to those basic patterns are also allowed, subject to the constraints of the memory model. For example:

      When processing an instance of the production

PrimaryExpression[Yield, Await] : 
CoverParenthesizedExpressionAndArrowParameterList[?Yield, ?Await]

      the interpretation of CoverParenthesizedExpressionAndArrowParameterList is refined using the following grammar:
    
 

      When processing an instance of the production

CallExpression[Yield, Await] : 
CoverCallExpressionAndAsyncArrowHead[?Yield, ?Await]

      the interpretation of CoverCallExpressionAndAsyncArrowHead is refined using the following grammar:
    
 

      In certain circumstances when processing an instance of the production

AssignmentExpression[In, Yield, Await] : 
LeftHandSideExpression[?Yield, ?Await]
=
AssignmentExpression[?In, ?Yield, ?Await]

      the interpretation of LeftHandSideExpression is refined using the following grammar:
    
 

      When processing an instance of the production

ArrowParameters[Yield, Await] : 
CoverParenthesizedExpressionAndArrowParameterList[?Yield, ?Await]

      the interpretation of CoverParenthesizedExpressionAndArrowParameterList is refined using the following grammar:
    
 

      When processing an instance of the production

AsyncArrowFunction[In, Yield, Await] : 
CoverCallExpressionAndAsyncArrowHead[?Yield, ?Await]
[no LineTerminator here]
=>
AsyncConciseBody[?In]

      the interpretation of CoverCallExpressionAndAsyncArrowHead is refined using the following grammar:
    
 
All grammar symbols not explicitly defined by the StringNumericLiteral grammar have the definitions used in the Lexical Grammar for numeric literals.
Each \u HexTrailSurrogate for which the choice of associated u HexLeadSurrogate is ambiguous shall be associated with the nearest possible u HexLeadSurrogate that would otherwise have no corresponding \u HexTrailSurrogate.
 
The ECMAScript language syntax and semantics defined in this annex are required when the ECMAScript host is a web browser. The content of this annex is normative but optional if the ECMAScript host is not a web browser.
This annex describes various legacy features and other characteristics of web browser ECMAScript hosts. All of the language features and behaviours specified in this annex have one or more undesirable characteristics and in the absence of legacy usage would be removed from this specification. However, the usage of these features by large numbers of existing web pages means that web browsers must continue to support them. The specifications in this annex define the requirements for interoperable implementations of these legacy features.
These features are not considered part of the core ECMAScript language. Programmers should not use or assume the existence of these features and behaviours when writing new ECMAScript code. ECMAScript implementations are discouraged from implementing these features unless the implementation is part of a web browser or is required to run the same legacy ECMAScript code that web browsers encounter.
The syntax and semantics of 12.4 is extended as follows except that this extension is not allowed when parsing source text using the goal symbol Module:
Similar to a MultiLineComment that contains a line terminator code point, a SingleLineHTMLCloseComment is considered to be a LineTerminator for purposes of parsing by the syntactic grammar.
The syntax of 22.2.1 is modified and extended as follows. These changes introduce ambiguities that are broken by the ordering of grammar productions and by contextual information. When parsing using the following grammar, each alternative is considered only if previous production alternatives do not match.
This alternative pattern grammar and semantics only changes the syntax and semantics of BMP patterns. The following grammar extensions include productions parameterized with the [UnicodeMode] parameter. However, none of these extensions change the syntax of Unicode patterns recognized when parsing with the [UnicodeMode] parameter present on the goal symbol.
When the same left-hand sides occurs with both [+UnicodeMode] and [~UnicodeMode] guards it is to control the disambiguation priority.
The semantics of 22.2.1.1 is extended as follows:
Additionally, the rules for the following productions are modified with the addition of the highlighted text:
In the definitions of CountLeftCapturingParensWithin and CountLeftCapturingParensBefore, references to ‚Äú
Atom :: 
(
GroupSpecifieropt
Disjunction
)


 ‚Äù are to be interpreted as meaning ‚Äú
Atom :: 
(
GroupSpecifieropt
Disjunction
)


 ‚Äù or ‚Äú
ExtendedAtom :: 
(
GroupSpecifieropt
Disjunction
)


 ‚Äù.
The semantics of 22.2.1.5 is extended as follows:
The semantics of 22.2.1.6 is extended as follows:
The semantics of CompileSubpattern is extended as follows:
The rule for 
Term :: 
QuantifiableAssertion
Quantifier


 is the same as for 
Term :: 
Atom
Quantifier


 but with QuantifiableAssertion substituted for Atom.
The rule for 
Term :: 
ExtendedAtom
Quantifier


 is the same as for 
Term :: 
Atom
Quantifier


 but with ExtendedAtom substituted for Atom.
The rule for 
Term :: ExtendedAtom

 is the same as for 
Term :: Atom

 but with ExtendedAtom substituted for Atom.
CompileAssertion rules for the 
Assertion :: 
(?=
Disjunction
)


 and 
Assertion :: 
(?!
Disjunction
)


 productions are also used for the QuantifiableAssertion productions, but with QuantifiableAssertion substituted for Assertion.
CompileAtom rules for the Atom productions except for 
Atom :: PatternCharacter

 are also used for the ExtendedAtom productions, but with ExtendedAtom substituted for Atom. The following rules, with parameter direction, are also added:
The semantics of 22.2.2.9 is extended as follows:
The following two rules replace the corresponding rules of CompileToCharSet.
In addition, the following rules are added to CompileToCharSet.
The abstract operation CharacterRangeOrUnion takes arguments rer (a RegExp Record), A (a CharSet), and B (a CharSet) and returns a CharSet. It performs the following steps when called:
The semantics of 22.2.3.4 is extended as follows:
The abstract operation ParsePattern takes arguments patternText (a sequence of Unicode code points), u (a Boolean), and v (a Boolean). It performs the following steps when called:
When the ECMAScript host is a web browser the following additional properties of the standard built-in objects are defined.
The entries in Table 97 are added to Table 6.
This function is a property of the global object. It computes a new version of a String value in which certain code units have been replaced by a hexadecimal escape sequence.
When replacing a code unit of numeric value less than or equal to 0x00FF, a two-digit escape sequence of the form %xx is used. When replacing a code unit of numeric value strictly greater than 0x00FF, a four-digit escape sequence of the form %uxxxx is used.
It is the %escape% intrinsic object.
It performs the following steps when called:
The encoding is partly based on the encoding described in RFC 1738, but the entire encoding specified in this standard is described above without regard to the contents of RFC 1738. This encoding does not reflect changes to RFC 1738 made by RFC 3986.
This function is a property of the global object. It computes a new version of a String value in which each escape sequence of the sort that might be introduced by the escape function is replaced with the code unit that it represents.
It is the %unescape% intrinsic object.
It performs the following steps when called:
This method returns a substring of the result of converting the this value to a String, starting from index start and running for length code units (or through the end of the String if length is undefined). If start is negative, it is treated as sourceLength + start where sourceLength is the length of the String. The result is a String value, not a String object.
It performs the following steps when called:
This method is intentionally generic; it does not require that its this value be a String object. Therefore it can be transferred to other kinds of objects for use as a method.
This method performs the following steps when called:
The abstract operation CreateHTML takes arguments string (an ECMAScript language value), tag (a String), attribute (a String), and value (an ECMAScript language value) and returns either a normal completion containing a String or a throw completion. It performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
This method performs the following steps when called:
The property "trimStart" is preferred. The "trimLeft" property is provided principally for compatibility with old code. It is recommended that the "trimStart" property be used in new ECMAScript code.
The initial value of the "trimLeft" property is %String.prototype.trimStart%, defined in 22.1.3.34.
The property "trimEnd" is preferred. The "trimRight" property is provided principally for compatibility with old code. It is recommended that the "trimEnd" property be used in new ECMAScript code.
The initial value of the "trimRight" property is %String.prototype.trimEnd%, defined in 22.1.3.33.
The getFullYear method is preferred for nearly all purposes, because it avoids the ‚Äúyear 2000 problem.‚Äù
This method performs the following steps when called:
The setFullYear method is preferred for nearly all purposes, because it avoids the ‚Äúyear 2000 problem.‚Äù
This method performs the following steps when called:
The toUTCString method is preferred. This method is provided principally for compatibility with old code.
The initial value of the "toGMTString" property is %Date.prototype.toUTCString%, defined in 21.4.4.43.
This method performs the following steps when called:
This method completely reinitializes the this value RegExp with a new pattern and flags. An implementation may interpret use of this method as an assertion that the resulting RegExp object will be used multiple times and hence is a candidate for extra optimization.
Prior to ECMAScript 2015, the specification of LabelledStatement did not allow for the association of a statement label with a FunctionDeclaration. However, a labelled FunctionDeclaration was an allowable extension for non-strict code and most browser-hosted ECMAScript implementations supported that extension. In ECMAScript 2015 and later, the grammar production for LabelledStatement permits use of FunctionDeclaration as a LabelledItem but 14.13.1 includes an Early Error rule that produces a Syntax Error if that occurs. That rule is modified with the addition of the highlighted text:
The early error rules for WithStatement, IfStatement, and IterationStatement prevent these statements from containing a labelled FunctionDeclaration in non-strict code.
Prior to ECMAScript 2015, the ECMAScript specification did not define the occurrence of a FunctionDeclaration as an element of a Block statement's StatementList. However, support for that form of FunctionDeclaration was an allowable extension and most browser-hosted ECMAScript implementations permitted them. Unfortunately, the semantics of such declarations differ among those implementations. Because of these semantic differences, existing web ECMAScript source text that uses Block level function declarations is only portable among browser implementations if the usage only depends upon the semantic intersection of all of the browser implementations for such declarations. The following are the use cases that fall within that intersection semantics:
A function is declared and only referenced within a single block.
A function is declared and possibly used within a single Block but also referenced by an inner function definition that is not contained within that same Block.
A function is declared and possibly used within a single block but also referenced within subsequent blocks.
The first use case is interoperable with the semantics of Block level function declarations provided by ECMAScript 2015. Any pre-existing ECMAScript source text that employs that use case will operate using the Block level function declarations semantics defined by clauses 10, 14, and 15.
ECMAScript 2015 interoperability for the second and third use cases requires the following extensions to the clause 10, clause 15, clause 19.2.1 and clause 16.1.7 semantics.
If an ECMAScript implementation has a mechanism for reporting diagnostic warning messages, a warning should be produced when code contains a FunctionDeclaration for which these compatibility semantics are applied and introduce observable differences from non-compatibility semantics. For example, if a var binding is not introduced because its introduction would create an early error, a warning message should not be produced.
During FunctionDeclarationInstantiation the following steps are performed in place of step 29:
During GlobalDeclarationInstantiation the following steps are performed in place of step 12:
During EvalDeclarationInstantiation the following steps are performed in place of step 13:
The rules for the following production in 14.2.1 are modified with the addition of the highlighted text:
The rules for the following production in 14.12.1 are modified with the addition of the highlighted text:
During BlockDeclarationInstantiation the following steps are performed in place of step 3.a.ii.1:
During BlockDeclarationInstantiation the following steps are performed in place of step 3.b.iii:
The following augments the IfStatement production in 14.6:
This production only applies when parsing non-strict code. Source text matched by this production is processed as if each matching occurrence of FunctionDeclaration[?Yield, ?Await, ~Default] was the sole StatementListItem of a BlockStatement occupying that position in the source text. The semantics of such a synthetic BlockStatement includes the web legacy compatibility semantics specified in B.3.2.
The content of subclause 14.15.1 is replaced with the following:
The Block of a Catch clause may contain var declarations that bind a name that is also bound by the CatchParameter. At runtime, such bindings are instantiated in the VariableDeclarationEnvironment. They do not shadow the same-named bindings introduced by the CatchParameter and hence the Initializer for such var declarations will assign to the corresponding catch parameter rather than the var binding.
This modified behaviour also applies to var and function declarations introduced by direct eval calls contained within the Block of a Catch clause. This change is accomplished by modifying the algorithm of 19.2.1.3 as follows:
Step 3.d.i.2.a.i is replaced by:
Step 13.b.ii.4.a.i.i is replaced by:
The following augments the ForInOfStatement production in 14.7.5:
This production only applies when parsing non-strict code.
The static semantics of ContainsDuplicateLabels in 8.3.1 are augmented with the following:
The static semantics of ContainsUndefinedBreakTarget in 8.3.2 are augmented with the following:
The static semantics of ContainsUndefinedContinueTarget in 8.3.3 are augmented with the following:
The static semantics of IsDestructuring in 14.7.5.2 are augmented with the following:
The static semantics of VarDeclaredNames in 8.2.6 are augmented with the following:
The static semantics of VarScopedDeclarations in 8.2.7 are augmented with the following:
The runtime semantics of ForInOfLoopEvaluation in 14.7.5.5 are augmented with the following:
An [[IsHTMLDDA]] internal slot may exist on host-defined objects. Objects with an [[IsHTMLDDA]] internal slot behave like undefined in the ToBoolean and IsLooselyEqual abstract operations and when used as an operand for the typeof operator.
Objects with an [[IsHTMLDDA]] internal slot are never created by this specification. However, the document.all object in web browsers is a host-defined exotic object with this slot that exists for web compatibility purposes. There are no other known examples of this type of object and implementations should not create any with the exception of document.all.
The following step replaces step 3 of ToBoolean:
The following steps replace step 4 of IsLooselyEqual:
The following step replaces step 12 of the evaluation semantics for typeof:
The HostMakeJobCallback abstract operation allows hosts which are web browsers to specify non-default behaviour.
The HostEnsureCanAddPrivateElement abstract operation allows hosts which are web browsers to specify non-default behaviour.
The strict mode restriction and exceptions
See 4.2 for the definition of host.
HostCallJobCallback(...)
HostEnqueueFinalizationRegistryCleanupJob(...)
HostEnqueueGenericJob(...)
HostEnqueuePromiseJob(...)
HostEnqueueTimeoutJob(...)
HostEnsureCanCompileStrings(...)
HostFinalizeImportMeta(...)
HostGetImportMetaProperties(...)
HostGrowSharedArrayBuffer(...)
HostHasSourceTextAvailable(...)
HostLoadImportedModule(...)
HostMakeJobCallback(...)
HostPromiseRejectionTracker(...)
HostResizeArrayBuffer(...)
InitializeHostDefinedRealm(...)
[[HostDefined]] on Realm Records: See Table 24.
[[HostDefined]] on Script Records: See Table 40.
[[HostDefined]] on Module Records: See Table 41.
[[HostDefined]] on JobCallback Records: See Table 28.
[[HostSynchronizesWith]] on Candidate Executions: See Table 96.
[[IsHTMLDDA]]: See B.3.6.
The global object: See clause 19.
Preparation steps before, and cleanup steps after, invocation of Job Abstract Closures. See 9.5.
Any of the essential internal methods in Table 4 for any exotic object not specified within this specification.
Any built-in objects and methods not defined within this specification, except as restricted in 17.1.
9.1.1.4.15-9.1.1.4.18 Edition 5 and 5.1 used a property existence test to determine whether a global object property corresponding to a new global declaration already existed. ECMAScript 2015 uses an own property existence test. This corresponds to what has been most commonly implemented by web browsers.
10.4.2.1: The 5th Edition moved the capture of the current array length prior to the integer conversion of the array index or new length value. However, the captured length value could become invalid if the conversion process has the side-effect of changing the array length. ECMAScript 2015 specifies that the current array length must be captured after the possible occurrence of such side-effects.
21.4.1.31: Previous editions permitted the TimeClip abstract operation to return either +0ùîΩ or -0ùîΩ as the representation of a 0 time value. ECMAScript 2015 specifies that +0ùîΩ always returned. This means that for ECMAScript 2015 the time value of a Date is never observably -0ùîΩ and methods that return time values never return -0ùîΩ.
21.4.1.32: If a UTC offset representation is not present, the local time zone is used. Edition 5.1 incorrectly stated that a missing time zone should be interpreted as "z".
21.4.4.36: If the year cannot be represented using the Date Time String Format specified in 21.4.1.32 a RangeError exception is thrown. Previous editions did not specify the behaviour for that case.
21.4.4.41: Previous editions did not specify the value returned by Date.prototype.toString when the time value is NaN. ECMAScript 2015 specifies the result to be the String value "Invalid Date".
22.2.4.1, 22.2.6.13.1: Any LineTerminator code points in the value of the "source" property of a RegExp instance must be expressed using an escape sequence. Edition 5.1 only required the escaping of /.
22.2.6.8, 22.2.6.11: In previous editions, the specifications for String.prototype.match and String.prototype.replace was incorrect for cases where the pattern argument was a RegExp value whose global flag is set. The previous specifications stated that for each attempt to match the pattern, if lastIndex did not change, it should be incremented by 1. The correct behaviour is that lastIndex should be incremented by 1 only if the pattern matched the empty String.
23.1.3.30: Previous editions did not specify how a NaN value returned by a comparefn was interpreted by Array.prototype.sort. ECMAScript 2015 specifies that such as value is treated as if +0ùîΩ was returned from the comparefn. ECMAScript 2015 also specifies that ToNumber is applied to the result returned by a comparefn. In previous editions, the effect of a comparefn result that is not a Number value was implementation-defined. In practice, implementations call ToNumber.
6.2.5: In ECMAScript 2015, Function calls are not allowed to return a Reference Record.
7.1.4.1: In ECMAScript 2015, ToNumber applied to a String value now recognizes and converts BinaryIntegerLiteral and OctalIntegerLiteral numeric strings. In previous editions such strings were converted to NaN.
9.3: In ECMAScript 2018, Template objects are canonicalized based on Parse Node (source location), instead of across all occurrences of that template literal or tagged template in a Realm in previous editions.
12.2: In ECMAScript 2016, Unicode 8.0.0 or higher is mandated, as opposed to ECMAScript 2015 which mandated Unicode 5.1. In particular, this caused U+180E MONGOLIAN VOWEL SEPARATOR, which was in the Space_Separator (Zs) category and thus treated as whitespace in ECMAScript 2015, to be moved to the Format (Cf) category (as of Unicode 6.3.0). This causes whitespace-sensitive methods to behave differently. For example, "\u180E".trim().length was 0 in previous editions, but 1 in ECMAScript 2016 and later. Additionally, ECMAScript 2017 mandated always using the latest version of the Unicode Standard.
12.7: In ECMAScript 2015, the valid code points for an IdentifierName are specified in terms of the Unicode properties ‚ÄúID_Start‚Äù and ‚ÄúID_Continue‚Äù. In previous editions, the valid IdentifierName or Identifier code points were specified by enumerating various Unicode code point categories.
12.10.1: In ECMAScript 2015, Automatic Semicolon Insertion adds a semicolon at the end of a do-while statement if the semicolon is missing. This change aligns the specification with the actual behaviour of most existing implementations.
13.2.5.1: In ECMAScript 2015, it is no longer an early error to have duplicate property names in Object Initializers.
13.15.1: In ECMAScript 2015, strict mode code containing an assignment to an immutable binding such as the function name of a FunctionExpression does not produce an early error. Instead it produces a runtime error.
14.2: In ECMAScript 2015, a StatementList beginning with the token let followed by the input elements LineTerminator then Identifier is the start of a LexicalDeclaration. In previous editions, automatic semicolon insertion would always insert a semicolon before the Identifier input element.
14.5: In ECMAScript 2015, a StatementListItem beginning with the token let followed by the token [ is the start of a LexicalDeclaration. In previous editions such a sequence would be the start of an ExpressionStatement.
14.6.2: In ECMAScript 2015, the normal result of an IfStatement is never the value empty. If no Statement part is evaluated or if the evaluated Statement part produces a normal completion containing empty, the result of the IfStatement is undefined.
14.7: In ECMAScript 2015, if the ( token of a for statement is immediately followed by the token sequence let [ then the let is treated as the start of a LexicalDeclaration. In previous editions such a token sequence would be the start of an Expression.
14.7: In ECMAScript 2015, if the ( token of a for-in statement is immediately followed by the token sequence let [ then the let is treated as the start of a ForDeclaration. In previous editions such a token sequence would be the start of an LeftHandSideExpression.
14.7: Prior to ECMAScript 2015, an initialization expression could appear as part of the VariableDeclaration that precedes the in keyword. In ECMAScript 2015, the ForBinding in that same position does not allow the occurrence of such an initializer. In ECMAScript 2017, such an initializer is permitted only in non-strict code.
14.7: In ECMAScript 2015, the result of evaluating an IterationStatement is never a normal completion whose [[Value]] is empty. If the Statement part of an IterationStatement is not evaluated or if the final evaluation of the Statement part produces a normal completion whose [[Value]] is empty, the result of evaluating the IterationStatement is a normal completion whose [[Value]] is undefined.
14.11.2: In ECMAScript 2015, the result of evaluating a WithStatement is never a normal completion whose [[Value]] is empty. If evaluation of the Statement part of a WithStatement produces a normal completion whose [[Value]] is empty, the result of evaluating the WithStatement is a normal completion whose [[Value]] is undefined.
14.12.4: In ECMAScript 2015, the result of evaluating a SwitchStatement is never a normal completion whose [[Value]] is empty. If evaluation of the CaseBlock part of a SwitchStatement produces a normal completion whose [[Value]] is empty, the result of evaluating the SwitchStatement is a normal completion whose [[Value]] is undefined.
14.15: In ECMAScript 2015, it is an early error for a Catch clause to contain a var declaration for the same Identifier that appears as the Catch clause parameter. In previous editions, such a variable declaration would be instantiated in the enclosing variable environment but the declaration's Initializer value would be assigned to the Catch parameter.
14.15, 19.2.1.3: In ECMAScript 2015, a runtime SyntaxError is thrown if a Catch clause evaluates a non-strict direct eval whose eval code includes a var or FunctionDeclaration declaration that binds the same Identifier that appears as the Catch clause parameter.
14.15.3: In ECMAScript 2015, the result of a TryStatement is never the value empty. If the Block part of a TryStatement evaluates to a normal completion containing empty, the result of the TryStatement is undefined. If the Block part of a TryStatement evaluates to a throw completion and it has a Catch part that evaluates to a normal completion containing empty, the result of the TryStatement is undefined if there is no Finally clause or if its Finally clause evaluates to an empty normal completion.
15.4.5 In ECMAScript 2015, the function objects that are created as the values of the [[Get]] or [[Set]] attribute of accessor properties in an ObjectLiteral are not constructor functions and they do not have a "prototype" own property. In the previous edition, they were constructors and had a "prototype" property.
20.1.2.6: In ECMAScript 2015, if the argument to Object.freeze is not an object it is treated as if it was a non-extensible ordinary object with no own properties. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.1.2.8: In ECMAScript 2015, if the argument to Object.getOwnPropertyDescriptor is not an object an attempt is made to coerce the argument using ToObject. If the coercion is successful the result is used in place of the original argument value. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.1.2.10: In ECMAScript 2015, if the argument to Object.getOwnPropertyNames is not an object an attempt is made to coerce the argument using ToObject. If the coercion is successful the result is used in place of the original argument value. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.1.2.12: In ECMAScript 2015, if the argument to Object.getPrototypeOf is not an object an attempt is made to coerce the argument using ToObject. If the coercion is successful the result is used in place of the original argument value. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.1.2.16: In ECMAScript 2015, if the argument to Object.isExtensible is not an object it is treated as if it was a non-extensible ordinary object with no own properties. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.1.2.17: In ECMAScript 2015, if the argument to Object.isFrozen is not an object it is treated as if it was a non-extensible ordinary object with no own properties. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.1.2.18: In ECMAScript 2015, if the argument to Object.isSealed is not an object it is treated as if it was a non-extensible ordinary object with no own properties. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.1.2.19: In ECMAScript 2015, if the argument to Object.keys is not an object an attempt is made to coerce the argument using ToObject. If the coercion is successful the result is used in place of the original argument value. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.1.2.20: In ECMAScript 2015, if the argument to Object.preventExtensions is not an object it is treated as if it was a non-extensible ordinary object with no own properties. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.1.2.22: In ECMAScript 2015, if the argument to Object.seal is not an object it is treated as if it was a non-extensible ordinary object with no own properties. In the previous edition, a non-object argument always causes a TypeError to be thrown.
20.2.3.2: In ECMAScript 2015, the [[Prototype]] internal slot of a bound function is set to the [[GetPrototypeOf]] value of its target function. In the previous edition, [[Prototype]] was always set to %Function.prototype%.
20.2.4.1: In ECMAScript 2015, the "length" property of function instances is configurable. In previous editions it was non-configurable.
20.5.6.2: In ECMAScript 2015, the [[Prototype]] internal slot of a NativeError constructor is the Error constructor. In previous editions it was the Function prototype object.
21.4.4 In ECMAScript 2015, the Date prototype object is not a Date instance. In previous editions it was a Date instance whose TimeValue was NaN.
22.1.3.12 In ECMAScript 2015, the String.prototype.localeCompare function must treat Strings that are canonically equivalent according to the Unicode Standard as being identical. In previous editions implementations were permitted to ignore canonical equivalence and could instead use a bit-wise comparison.
22.1.3.28 and 22.1.3.30 In ECMAScript 2015, lowercase/upper conversion processing operates on code points. In previous editions such the conversion processing was only applied to individual code units. The only affected code points are those in the Deseret block of Unicode.
22.1.3.32 In ECMAScript 2015, the String.prototype.trim method is defined to recognize white space code points that may exist outside of the Unicode BMP. However, as of Unicode 7 no such code points are defined. In previous editions such code points would not have been recognized as white space.
22.2.4.1 In ECMAScript 2015, If the pattern argument is a RegExp instance and the flags argument is not undefined, a new RegExp instance is created just like pattern except that pattern's flags are replaced by the argument flags. In previous editions a TypeError exception was thrown when pattern was a RegExp instance and flags was not undefined.
22.2.6 In ECMAScript 2015, the RegExp prototype object is not a RegExp instance. In previous editions it was a RegExp instance whose pattern is the empty String.
22.2.6 In ECMAScript 2015, "source", "global", "ignoreCase", and "multiline" are accessor properties defined on the RegExp prototype object. In previous editions they were data properties defined on RegExp instances.
25.4.15: In ECMAScript 2019, Atomics.wake has been renamed to Atomics.notify to prevent confusion with Atomics.wait.
27.1.4.4, 27.6.3.6: In ECMAScript 2019, the number of Jobs enqueued by await was reduced, which could create an observable difference in resolution order between a then() call and an await expression.
This specification is authored on GitHub in a plaintext source format called Ecmarkup. Ecmarkup is an HTML and Markdown dialect that provides a framework and toolset for authoring ECMAScript specifications in plaintext and processing the specification into a full-featured HTML rendering that follows the editorial conventions for this document. Ecmarkup builds on and integrates a number of other formats and technologies including Grammarkdown for defining syntax and Ecmarkdown for authoring algorithm steps. PDF renderings of this specification are produced by printing the HTML rendering to a PDF.
Prior editions of this specification were authored using Word‚Äîthe Ecmarkup source text that formed the basis of this edition was produced by converting the ECMAScript 2015 Word document to Ecmarkup using an automated conversion tool.
There are no normative changes between IEEE 754-2008 and IEEE 754-2019 that affect the ECMA-262 specification.
Ecma International
Rue du Rhone 114
CH-1204 Geneva
Tel: +41 22 849 6000
Fax: +41 22 849 6001
Web: https://ecma-international.org/
¬© 2024 Ecma International
This draft document may be copied and furnished to others, and derivative works that comment on or otherwise explain it or assist in its implementation may be prepared, copied, published, and distributed, in whole or in part, without restriction of any kind, provided that the above copyright notice and this section are included on all such copies and derivative works. However, this document itself may not be modified in any way, including by removing the copyright notice or references to Ecma International, except as needed for the purpose of developing any document or deliverable produced by Ecma International.
This disclaimer is valid only prior to final version of this document. After approval all rights on the standard are reserved by Ecma International.
The limited permissions are granted through the standardization phase and will not be revoked by Ecma International or its successors or assigns during this time.
This document and the information contained herein is provided on an "AS IS" basis and ECMA INTERNATIONAL DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTY THAT THE USE OF THE INFORMATION HEREIN WILL NOT INFRINGE ANY OWNERSHIP RIGHTS OR ANY IMPLIED WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
All Software contained in this document ("Software") is protected by copyright and is being made available under the "BSD License", included below. This Software may be subject to third party rights (rights from parties other than Ecma International), including patent rights, and no licenses under such third party rights are granted under this license even if the third party concerned is a member of Ecma International. SEE THE ECMA CODE OF CONDUCT IN PATENT MATTERS AVAILABLE AT https://ecma-international.org/memento/codeofconduct.htm FOR INFORMATION REGARDING THE LICENSING OF PATENT CLAIMS THAT ARE REQUIRED TO IMPLEMENT ECMA INTERNATIONAL STANDARDS.
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
THIS SOFTWARE IS PROVIDED BY THE ECMA INTERNATIONAL "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL ECMA INTERNATIONAL BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

This specification is developed on GitHub with the help of the ECMAScript community. There are a number of ways to contribute to the development of this specification:
Refer to the colophon for more information on how this document is created.

    This specification's source can be found at https://github.com/tc39/ecma402.
  

    The ECMAScript 2024 Internationalization API Specification (ECMA-402 11th Edition), provides key language sensitive functionality as a complement to the ECMAScript 2024 Language Specification (ECMA-262 15th Edition or successor). Its functionality has been selected from that of well-established internationalization APIs such as those of the Internationalization Components for Unicode (ICU) library (https://unicode-org.github.io/icu-docs/), of the .NET framework, or of the Java platform.
  

    The 1st Edition API was developed by an ad-hoc group established by Ecma TC39 in September 2010 based on a proposal by Neboj≈°a ƒÜiriƒá and Jungshik Shin.
  

    The 2nd Edition API was adopted by the General Assembly of June 2015, as a complement to the ECMAScript 6th Edition.
  

    The 3rd Edition API was the first edition released under Ecma TC39's new yearly release cadence and open development process. A plain-text source document was built from the ECMA-402 source document to serve as the base for further development entirely on GitHub. Over the year of this standard's development, dozens of pull requests and issues were filed representing several of bug fixes, editorial fixes and other improvements. Additionally, numerous software tools were developed to aid in this effort including Ecmarkup, Ecmarkdown, and Grammarkdown.
  

    Dozens of individuals representing many organizations have made very significant contributions within Ecma TC39 to the development of this edition and to the prior editions. In addition, a vibrant community has emerged supporting TC39's ECMAScript efforts. This community has reviewed numerous drafts, filed dozens of bug reports, performed implementation experiments, contributed test suites, and educated the world-wide developer community about ECMAScript Internationalization. Unfortunately, it is impossible to identify and acknowledge every person and organization who has contributed to this effort.
  

    Norbert Lindenberg
    ECMA-402, 1st Edition Project Editor
  

    Rick Waldron
    ECMA-402, 2nd Edition Project Editor
  

    Caridy Pati√±o
    ECMA-402, 3rd, 4th and 5th Editions Project Editor
  

    Caridy Pati√±o, Daniel Ehrenberg, Leo Balter
    ECMA-402, 6th Edition Project Editors
  

    Leo Balter, Valerie Young, Isaac Durazo
    ECMA-402, 7th Edition Project Editors
  

    Leo Balter, Richard Gibson
    ECMA-402, 8th Edition Project Editors
  

    Leo Balter, Richard Gibson, Ujjwal Sharma
    ECMA-402, 9th Edition Project Editors
  

    Richard Gibson, Ujjwal Sharma
    ECMA-402, 10th Edition Project Editors
  

    Richard Gibson, Ujjwal Sharma
    ECMA-402, 11th Edition Project Editors
  

    This Standard defines the application programming interface for ECMAScript objects that support programs that need to adapt to the linguistic and cultural conventions used by different human languages and countries.
  

    A conforming implementation of this specification must conform to the ECMAScript 2024 Language Specification (ECMA-262 15th Edition, or successor), and must provide and support all the objects, properties, functions, and program semantics described in this specification. Nothing in this specification is intended to allow behaviour that is otherwise prohibited by ECMA-262, and any such conflict should be considered an editorial error rather than an override of constraints from ECMA-262.
  

    A conforming implementation is permitted to provide additional objects, properties, and functions beyond those described in this specification. In particular, a conforming implementation is permitted to provide properties not described in this specification, and values for those properties, for objects that are described herein. A conforming implementation is not permitted to add optional arguments to the functions defined in this specification.
  

    A conforming implementation is permitted to accept additional values, and then have implementation-defined behaviour instead of throwing a RangeError, for the following properties of options arguments:
  

    The following referenced documents are required for the application of this document. For dated references, only the edition cited applies. For undated references, the latest edition of the referenced document (including any amendments) applies.
  

    ECMAScript 2024 Language Specification (ECMA-262 15th Edition, or successor).
https://www.ecma-international.org/publications/standards/Ecma-262.htm


    This section contains a non-normative overview of the ECMAScript 2024 Internationalization API Specification.
  

      Internationalization of software means designing it such that it supports or can be easily adapted to support the needs of users speaking different languages and having different cultural expectations, and enables worldwide communication between them. Localization then is the actual adaptation to a specific language and culture. Globalization of software is commonly understood to be the combination of internationalization and localization. Globalization starts at the lowest level by using a text representation that supports all languages in the world, and using standard identifiers to identify languages, countries, time zones, and other relevant parameters. It continues with using a user interface language and data presentation that the user understands, and finally often requires product-specific adaptations to the user's language, culture, and environment.
    

      The ECMAScript 2024 Language Specification lays the foundation by using Unicode for text representation and by providing a few language-sensitive functions, but gives applications little control over the behaviour of these functions. This specification builds on that foundation by providing a set of customizable language-sensitive functionality. The API is useful even for applications that themselves are not internationalized, as even applications targeting only one language and one region need to properly support that one language and region. However, the API also enables applications that support multiple languages and regions, even concurrently, as may be needed in server environments.
    

      This specification is designed to complement the ECMAScript 2024 Language Specification by providing key language-sensitive functionality. The API can be added to an implementation of the ECMAScript 2024 Language Specification (ECMA-262 15th Edition, or successor) in whole or in part. This specification introduces new language values observable to ECMAScript code (such as the value of a [[FallbackSymbol]] internal slot and the set of values transitively reachable from %Intl% by property access), and also refines the definition of some functions specified in ECMA-262 (as described below). Neither category prohibits behaviour that is otherwise permitted for values and interfaces defined in ECMA-262, in order to support adoption of this specification by any implementation of ECMA-262.
    

      This specification provides several key pieces of language-sensitive functionality that are required in most applications: String comparison (collation), number formatting, date and time formatting, relative time formatting, display names, list formatting, locale selection and operation, pluralization rules, case conversion, and text segmentation. While the ECMAScript 2024 Language Specification provides functions for this basic functionality (on Array.prototype: toLocaleString; on String.prototype: localeCompare, toLocaleLowerCase, toLocaleUpperCase; on Number.prototype: toLocaleString; on Date.prototype: toLocaleString, toLocaleDateString, and toLocaleTimeString), their actual behaviour is left largely implemenation-defined. This specification provides additional functionality, control over the language and over details of the behaviour to be used, and a more complete specification of required functionality.
    
Applications can use the API in two ways:

      The Intl object is used to package all functionality defined in this specification in order to avoid name collisions.
    

      Every Intl constructor should behave as if defined by a class, throwing a TypeError exception when called as a function (without NewTarget). For backwards compatibility with past editions, this does not apply to %Intl.Collator%, %Intl.DateTimeFormat%, or %Intl.NumberFormat%, each of which construct and return a new object when called as a function.
    

      Due to the nature of internationalization, this specification has to leave several details implementation dependent:
    

        ECMA 402 describes the schema of the data used by its functions. The
        data contained inside is implementation-dependent, and expected to
        change over time and vary between implementations. The variation is
        visible by programmers, and it is possible to construct programs which
        will depend on a particular output. However, this specification
        attempts to describe reasonable constraints which will allow
        well-written programs to function across implementations.
        Implementations are encouraged to continue their efforts to harmonize
        linguistic data.
      

    This standard uses a subset of the notational conventions of the ECMAScript 2024 Language Specification (ECMA-262 15th Edition), as es2024:
  

    As an extension to the Record Specification Type, the notation ‚Äú[[<name>]]‚Äù denotes a field whose name is given by the variable name, which must have a String value. For example, if a variable s has the value "a", then [[<s>]] denotes the field [[a]].
  

    This specification uses blocks demarcated as Normative Optional to denote the sense of Annex B in ECMA 262. That is, normative optional sections are required when the ECMAScript host is a web browser. The content of the section is normative but optional if the ECMAScript host is not a web browser.
  

      The following table extends the Well-Known Intrinsic Objects table defined in es2024, 6.1.7.4.
    

    This clause describes the String values used in this specification to identify locales, currencies, time zones, measurement units, numbering systems, collations, and calendars.
  

      The String values used to identify locales, currencies, scripts, and time zones are interpreted in an ASCII-case-insensitive manner, treating the code units 0x0041 through 0x005A (corresponding to Unicode characters LATIN CAPITAL LETTER A through LATIN CAPITAL LETTER Z) as equivalent to the corresponding code units 0x0061 through 0x007A (corresponding to Unicode characters LATIN SMALL LETTER A through LATIN SMALL LETTER Z), both inclusive. No other case folding equivalences are applied.
    

      The ASCII-uppercase of a String value S is the String value derived from S by replacing each occurrence of an ASCII lowercase letter code unit (0x0061 through 0x007A, inclusive) with the corresponding ASCII uppercase letter code unit (0x0041 through 0x005A, inclusive) while preserving all other code units.
    

      The ASCII-lowercase of a String value S is the String value derived from S by replacing each occurrence of an ASCII uppercase letter code unit (0x0041 through 0x005A, inclusive) with the corresponding ASCII lowercase letter code unit (0x0061 through 0x007A, inclusive) while preserving all other code units.
    

      A String value A is an ASCII-case-insensitive match for String value B if the ASCII-uppercase of A is exactly the same sequence of code units as the ASCII-uppercase of B. A sequence of Unicode code points A is an ASCII-case-insensitive match for B if B is an ASCII-case-insensitive match for ! CodePointsToString(A).
    

      This specification identifies locales using Unicode BCP 47 locale identifiers as defined by Unicode Technical Standard #35 Part 1 Core, Section 3.3 BCP 47 Conformance, and its algorithms refer to Unicode locale nonterminals defined in the grammars of Section 3 Unicode Language and Locale Identifiers.
      Each such identifier can also be referred to as a language tag, and is in fact a valid language tag as that term is used in BCP 47.
      A locale identifier in canonical form as specified in Unicode Technical Standard #35 Part 1 Core, Section 3.2.1 Canonical Unicode Locale Identifiers is referred to as a "Unicode canonicalized locale identifier".
    

      Locale identifiers consist of case-insensitive Unicode Basic Latin alphanumeric subtags separated by "-" (U+002D HYPHEN-MINUS) characters, with single-character subtags referred to as "singleton subtags".
      Unicode Technical Standard #35 Part 1 Core, Section 3.6 Unicode BCP 47 U Extension subtag sequences are used extensively, and the term "Unicode locale extension sequence" describes the longest substring of a language tag that can be matched by the unicode_locale_extensions Unicode locale nonterminal and is not part of a "-x-‚Ä¶" private use subtag sequence. It starts with "-u-" and includes all immediately following subtags that are not singleton subtags, along with their preceding "-" separators. For example, the Unicode locale extension sequence of "en-US-u-fw-mon-x-u-ex-foobar" is "-u-fw-mon".
    

      All structurally valid language tags are appropriate for use with the APIs defined by this specification, but implementations are not required to use Unicode Common Locale Data Repository (CLDR) data for validating them; the set of locales and thus language tags that an implementation supports with adequate localizations is implementation-defined. Intl constructors map requested language tags to locales supported by their respective implementations.
    
The abstract operation IsStructurallyValidLanguageTag takes argument locale (a String) and returns a Boolean. It determines whether locale is a syntactically well-formed language tag. It does not consider whether locale conveys any meaningful semantics, nor does it differentiate between aliased subtags and their preferred replacement subtags or require canonical casing or subtag ordering. It performs the following steps when called:
The abstract operation CanonicalizeUnicodeLocaleId takes argument locale (a language tag) and returns a Unicode canonicalized locale identifier. It returns the canonical and case-regularized form of the locale. It performs the following steps when called:
The implementation-defined abstract operation DefaultLocale takes no arguments and returns a Unicode canonicalized locale identifier. The returned String value represents the structurally valid (6.2.1) and canonicalized (6.2.2) language tag for the host environment's current locale.

      This specification identifies currencies using 3-letter currency codes as defined by ISO 4217. Their canonical form is uppercase.
    

      All well-formed 3-letter ISO 4217 currency codes are allowed. However, the set of combinations of currency code and language tag for which localized currency symbols are available is implementation dependent. Where a localized currency symbol is not available, the ISO 4217 currency code is used for formatting.
    
The abstract operation IsWellFormedCurrencyCode takes argument currency (a String) and returns a Boolean. It verifies that the currency argument represents a well-formed 3-letter ISO currency code. It performs the following steps when called:
The implementation-defined abstract operation AvailableCanonicalCurrencies takes no arguments and returns a List of Strings. The returned List is ordered as if an Array of the same values had been sorted using %Array.prototype.sort% using undefined as comparefn, and contains unique, well-formed, and upper case canonicalized 3-letter ISO 4217 currency codes, identifying the currencies for which the implementation provides the functionality of Intl.DisplayNames and Intl.NumberFormat objects.

      This specification identifies time zones using the Zone and Link names of the IANA Time Zone Database. Their canonical form is the corresponding Zone name in the casing used in the IANA Time Zone Database except as specifically overridden by CanonicalizeTimeZoneName.
    

      A conforming implementation must recognize "UTC" and all other Zone and Link names (and only such names), and use best available current and historical information about their offsets from UTC and their daylight saving time rules in calculations. However, the set of combinations of time zone name and language tag for which localized time zone names are available is implementation dependent.
    
The abstract operation IsValidTimeZoneName takes argument timeZone (a String) and returns a Boolean. It verifies that the timeZone argument represents a valid Zone or Link name of the IANA Time Zone Database. It performs the following steps when called:
The abstract operation CanonicalizeTimeZoneName takes argument timeZone (a String value that is a valid time zone name as verified by IsValidTimeZoneName) and returns a String. It returns the canonical and case-regularized form of timeZone. It performs the following steps when called:
The implementation-defined abstract operation AvailableCanonicalTimeZones takes no arguments and returns a List of Strings. The returned List is a sorted List of supported Zone and Link names in the IANA Time Zone Database. It performs the following steps when called:

      This specification identifies measurement units using a core unit identifier (or equivalently core unit ID) as defined by Unicode Technical Standard #35 Part 2 General, Section 6.2 Unit Identifiers. Their canonical form is a string containing only Unicode Basic Latin lowercase letters (U+0061 LATIN SMALL LETTER A through U+007A LATIN SMALL LETTER Z) with zero or more medial hyphens (U+002D HYPHEN-MINUS).
    

      Only a limited set of core unit identifiers are sanctioned.
      Attempting to use an unsanctioned core unit identifier results in a RangeError.
    
The abstract operation IsWellFormedUnitIdentifier takes argument unitIdentifier (a String) and returns a Boolean. It verifies that the unitIdentifier argument represents a well-formed core unit identifier that is either a sanctioned single unit or a complex unit formed by division of two sanctioned single units. It performs the following steps when called:
The abstract operation IsSanctionedSingleUnitIdentifier takes argument unitIdentifier (a String) and returns a Boolean. It verifies that the unitIdentifier argument is among the single unit identifiers sanctioned in the current version of this specification, which are a subset of the Common Locale Data Repository release 38 unit validity data; the list may grow over time. As discussed in Unicode Technical Standard #35 Part 2 General, Section 6.2 Unit Identifiers, a single unit identifier is a core unit identifier that is not composed of multiplication or division of other unit identifiers. It performs the following steps when called:
The abstract operation AvailableCanonicalUnits takes no arguments and returns a List of Strings. The returned List is ordered as if an Array of the same values had been sorted using %Array.prototype.sort% using undefined as comparefn, and consists of the unique values of simple unit identifiers listed in every row of Table 2, except the header row.

      This specification identifies numbering systems using a numbering system identifier corresponding with the name referenced by Unicode Technical Standard #35 Part 3 Numbers, Section 1 Numbering Systems. Their canonical form is a string containing only Unicode Basic Latin lowercase letters (U+0061 LATIN SMALL LETTER A through U+007A LATIN SMALL LETTER Z).
    
The implementation-defined abstract operation AvailableCanonicalNumberingSystems takes no arguments and returns a List of Strings. The returned List is ordered as if an Array of the same values had been sorted using %Array.prototype.sort% using undefined as comparefn, and contains unique canonical numbering systems identifiers identifying the numbering systems for which the implementation provides the functionality of Intl.DateTimeFormat, Intl.NumberFormat, and Intl.RelativeTimeFormat objects. The list must include the Numbering System value of every row of Table 14, except the header row.

      This specification identifies collations using a collation type as defined by Unicode Technical Standard #35 Part 5 Collation, Section 3.1 Collation Types. Their canonical form is a string containing only Unicode Basic Latin lowercase letters (U+0061 LATIN SMALL LETTER A through U+007A LATIN SMALL LETTER Z) with zero or more medial hyphens (U+002D HYPHEN-MINUS).
    
The implementation-defined abstract operation AvailableCanonicalCollations takes no arguments and returns a List of Strings. The returned List is ordered as if an Array of the same values had been sorted using %Array.prototype.sort% using undefined as comparefn, and contains unique canonical collation types identifying the collations for which the implementation provides the functionality of Intl.Collator objects.

      This specification identifies calendars using a calendar type as defined by Unicode Technical Standard #35 Part 4 Dates, Section 2 Calendar Elements. Their canonical form is a string containing only Unicode Basic Latin lowercase letters (U+0061 LATIN SMALL LETTER A through U+007A LATIN SMALL LETTER Z) with zero or more medial hyphens (U+002D HYPHEN-MINUS).
    
The implementation-defined abstract operation AvailableCanonicalCalendars takes no arguments and returns a List of Strings. The returned List is ordered as if an Array of the same values had been sorted using %Array.prototype.sort% using undefined as comparefn, and contains unique canonical calendar types identifying the calendars for which the implementation provides the functionality of Intl.DateTimeFormat objects. The list must include "iso8601".

    Unless specified otherwise in this document, the objects, functions, and constructors described in this standard are subject to the generic requirements and restrictions specified for standard built-in ECMAScript objects in the ECMAScript 2024 Language Specification (ECMA-262 15th Edition, or successor), clause 18.
  

    The Intl object is the %Intl% intrinsic object and the initial value of the "Intl" property of the global object. The Intl object is a single ordinary object.
  

    The value of the [[Prototype]] internal slot of the Intl object is the intrinsic object %Object.prototype%.
  

    The Intl object is not a function object. It does not have a [[Construct]] internal method; it is not possible to use the Intl object as a constructor with the new operator. The Intl object does not have a [[Call]] internal method; it is not possible to invoke the Intl object as a function.
  

    The Intl object has an internal slot, [[FallbackSymbol]], which is a new %Symbol% in the current realm with the [[Description]] "IntlLegacyConstructedSymbol".
  

        The initial value of the @@toStringTag property is the String value "Intl".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      

      With the exception of Intl.Locale, each of the following constructors is a service constructor that creates objects providing locale-sensitive services.
    

        See 10.
      

        See 11.
      

        See 12.
      

        See 13.
      

        See 14.
      

        See 15.
      

        See 16.
      

        See 17.
      

        See 18.
      

        When the getCanonicalLocales method is called with argument locales, the following steps are taken:
      

      When the supportedValuesOf method is called with argument key , the following steps are taken:
      

Service constructors use a common pattern to negotiate the requests represented by their locales and options arguments against the actual capabilities of their implementations. That common behaviour is explained here in terms of internal slots describing the capabilities and abstract operations using these internal slots.
  

      Each service constructor has the following internal slots:
    

      Where the following abstract operations take an availableLocales argument, it must be an [[AvailableLocales]] List as specified in 9.1.
    
The abstract operation CanonicalizeLocaleList takes argument locales (an ECMAScript language value) and returns either a normal completion containing a List of Unicode canonicalized locale identifiers or a throw completion. It performs the following steps when called:
The abstract operation BestAvailableLocale takes arguments availableLocales (a List of Unicode canonicalized locale identifiers) and locale (a Unicode canonicalized locale identifier) and returns a Unicode canonicalized locale identifier or undefined. It compares locale against the locales in availableLocales and returns either the longest non-empty prefix of locale that is an element of availableLocales, or undefined if there is no such element. It uses the fallback mechanism of RFC 4647, section 3.4. It performs the following steps when called:
The abstract operation LookupMatcher takes arguments availableLocales (a List of Unicode canonicalized locale identifiers) and requestedLocales (a List of Unicode canonicalized locale identifiers) and returns a Record with fields [[locale]] (a Unicode canonicalized locale identifier) and [[extension]] (a Unicode locale extension sequence or empty). It compares requestedLocales, which must be a List as returned by CanonicalizeLocaleList, against the locales in availableLocales and determines the best available language to meet the request. It performs the following steps when called:
The implementation-defined abstract operation BestFitMatcher takes arguments availableLocales (a List of Unicode canonicalized locale identifiers) and requestedLocales (a List of Unicode canonicalized locale identifiers) and returns a Record with fields [[locale]] (a Unicode canonicalized locale identifier) and [[extension]] (a Unicode locale extension sequence or empty). It compares requestedLocales, which must be a List as returned by CanonicalizeLocaleList, against the locales in availableLocales and determines the best available language to meet the request. The algorithm is implementation dependent, but should produce results that a typical user of the requested locales would perceive as at least as good as those produced by the LookupMatcher abstract operation. Options specified through Unicode locale extension sequences must be ignored by the algorithm. Information about such subsequences is returned separately. The abstract operation returns a record with a [[locale]] field, whose value is the language tag of the selected locale, which must be an element of availableLocales. If the language tag of the request locale that led to the selected locale contained a Unicode locale extension sequence, then the returned record's [[extension]] field is set to the substring of the Unicode locale extension sequence within the request locale language tag. Otherwise the [[extension]] field is set to empty.
The abstract operation UnicodeExtensionComponents takes argument extension (a Unicode locale extension sequence) and returns a Record with fields [[Attributes]] and [[Keywords]]. It deconstructs extension into a List of unique attributes and a List of keywords with unique keys. Any repeated appearance of an attribute or keyword key after the first is ignored. It performs the following steps when called:
The abstract operation InsertUnicodeExtensionAndCanonicalize takes arguments locale (a Unicode canonicalized locale identifier) and extension (a Unicode locale extension sequence) and returns a Unicode canonicalized locale identifier. It incorporates extension into locale and returns the canonicalized result. It performs the following steps when called:
The abstract operation ResolveLocale takes arguments availableLocales (a List of Unicode canonicalized locale identifiers), requestedLocales (a List of Unicode canonicalized locale identifiers), options (a Record), relevantExtensionKeys (a List of Strings), and localeData (a Record) and returns a Record. It compares a BCP 47 language priority list requestedLocales against the locales in availableLocales and determines the best available language to meet the request. It performs the following steps when called:
The abstract operation LookupSupportedLocales takes arguments availableLocales (a List of Unicode canonicalized locale identifiers) and requestedLocales (a List of Unicode canonicalized locale identifiers) and returns a List of Unicode canonicalized locale identifiers. It returns the subset of the provided BCP 47 language priority list requestedLocales for which availableLocales has a matching locale when using the BCP 47 Lookup algorithm. Locales appear in the same order in the returned list as in requestedLocales. It performs the following steps when called:
The implementation-defined abstract operation BestFitSupportedLocales takes arguments availableLocales (a List of Unicode canonicalized locale identifiers) and requestedLocales (a List of Unicode canonicalized locale identifiers) and returns a List of Unicode canonicalized locale identifiers. It returns the subset of the provided BCP 47 language priority list requestedLocales for which availableLocales has a matching locale when using the Best Fit Matcher algorithm. Locales appear in the same order in the returned list as in requestedLocales.
The abstract operation SupportedLocales takes arguments availableLocales (a List of Unicode canonicalized locale identifiers), requestedLocales (a List of Unicode canonicalized locale identifiers), and options (an ECMAScript language value) and returns either a normal completion containing a List of Unicode canonicalized locale identifiers or a throw completion. It returns the subset of the provided BCP 47 language priority list requestedLocales for which availableLocales has a matching locale. Two algorithms are available to match the locales: the Lookup algorithm described in RFC 4647 section 3.4, and an implementation dependent best-fit algorithm. Locales appear in the same order in the returned list as in requestedLocales. It performs the following steps when called:
The abstract operation GetOptionsObject takes argument options (an ECMAScript language value) and returns either a normal completion containing an Object or a throw completion. 
          It returns an Object suitable for use with GetOption, either options itself or a default empty Object.
          It throws a TypeError if options is not undefined and not an Object.
         It performs the following steps when called:
The abstract operation CoerceOptionsToObject takes argument options (an ECMAScript language value) and returns either a normal completion containing an Object or a throw completion. 
          It coerces options into an Object suitable for use with GetOption, defaulting to an empty Object.
          Because it coerces non-null primitive values into objects, its use is discouraged for new functionality in favour of GetOptionsObject.
         It performs the following steps when called:
The abstract operation GetOption takes arguments options (an Object), property (a property key), type (boolean or string), values (empty or a List of ECMAScript language values), and default (required or an ECMAScript language value) and returns either a normal completion containing an ECMAScript language value or a throw completion. It extracts the value of the specified property of options, converts it to the required type, checks whether it is allowed by values if values is not empty, and substitutes default if the value is undefined. It performs the following steps when called:
The abstract operation GetBooleanOrStringNumberFormatOption takes arguments options (an Object), property (a property key), stringValues (a List of Strings), and fallback (an ECMAScript language value) and returns either a normal completion containing either a Boolean, String, or fallback, or a throw completion. It extracts the value of the property named property from the provided options object. It returns fallback if that value is undefined, true if that value is true, false if that value coerces to false, and otherwise coerces it to a String and returns the result if it is allowed by stringValues. It performs the following steps when called:
The abstract operation DefaultNumberOption takes arguments value (an ECMAScript language value), minimum (an integer), maximum (an integer), and fallback (an integer or undefined) and returns either a normal completion containing either an integer or undefined, or a throw completion. It converts value to an integer, checks whether it is in the allowed range, and fills in a fallback value if necessary. It performs the following steps when called:
The abstract operation GetNumberOption takes arguments options (an Object), property (a String), minimum (an integer), maximum (an integer), and fallback (an integer or undefined) and returns either a normal completion containing either an integer or undefined, or a throw completion. It extracts the value of the property named property from the provided options object, converts it to an integer, checks whether it is in the allowed range, and fills in a fallback value if necessary. It performs the following steps when called:
The abstract operation PartitionPattern takes argument pattern (a String) and returns a List of Records with fields [[Type]] (a String) and [[Value]] (a String or undefined). 
          The [[Value]] field will be a String value if [[Type]] is "literal", and undefined otherwise.
          The syntax of the abstract pattern strings is an implementation detail and is not exposed to users of ECMA-402.
         It performs the following steps when called:

      The Intl.Collator constructor is the %Intl.Collator% intrinsic object and a standard built-in property of the Intl object. Behaviour common to all service constructor properties of the Intl object is specified in 9.1.
    

        When the Intl.Collator function is called with optional arguments locales and options, the following steps are taken:
      
The abstract operation InitializeCollator takes arguments collator (an Intl.Collator), locales (an ECMAScript language value), and options (an ECMAScript language value) and returns either a normal completion containing collator or a throw completion. It initializes collator as a Collator object. It performs the following steps when called:

      The Intl.Collator constructor has the following properties:
    

        The value of Intl.Collator.prototype is %Intl.Collator.prototype%.
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
      

        When the supportedLocalesOf method is called with arguments locales and options, the following steps are taken:
      

        The value of the [[AvailableLocales]] internal slot is implementation-defined within the constraints described in 9.1. The value of the [[RelevantExtensionKeys]] internal slot is a List that must include the element "co", may include any or all of the elements "kf" and "kn", and must not include any other elements.
      

        The values of the [[SortLocaleData]] and [[SearchLocaleData]] internal slots are implementation-defined within the constraints described in 9.1 and the following additional constraints, for all locale values locale:
      

      The Intl.Collator prototype object is itself an ordinary object. %Intl.Collator.prototype% is not an Intl.Collator instance and does not have an [[InitializedCollator]] internal slot or any of the other internal slots of Intl.Collator instance objects.
    

        The initial value of Intl.Collator.prototype.constructor is %Intl.Collator%.
      

        The initial value of the @@toStringTag property is the String value "Intl.Collator".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      

        This named accessor property returns a function that compares two strings according to the sort order of this Collator object.
      

        Intl.Collator.prototype.compare is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
      
A Collator compare function is an anonymous built-in function that has a [[Collator]] internal slot.
When a Collator compare function F is called with arguments x and y, the following steps are taken:
The "length" property of a Collator compare function is 2ùîΩ.
The implementation-defined abstract operation CompareStrings takes arguments collator (an Intl.Collator), x (a String), and y (a String) and returns a Number, but not NaN. 
            The returned Number represents the result of an implementation-defined locale-sensitive String comparison of x with y.
            The result is intended to correspond with a sort order of String values according to the effective locale and collation options of collator, and will be negative when x is ordered before y, positive when x is ordered after y, and zero in all other cases (representing no relative ordering between x and y).
            String values must be interpreted as UTF-16 code unit sequences as described in es2024, 6.1.4, and a surrogate pair (a code unit in the range 0xD800 to 0xDBFF followed by a code unit in the range 0xDC00 to 0xDFFF) within a string must be interpreted as the corresponding code point.
          

          Behaviour as described below depends upon locale-sensitive identification of the sequence of collation elements for a string, in particular "base letters", and different base letters always compare as unequal (causing the strings containing them to also compare as unequal). Results of comparing variations of the same base letter with different case, diacritic marks, or potentially other aspects further depends upon collator.[[Sensitivity]] as follows:
        

          If collator.[[IgnorePunctuation]] is true, then punctuation is ignored (e.g., strings that differ only in punctuation compare as equal).
        

          For the interpretation of options settable through locale extension keys, see Unicode Technical Standard #35 Part 1 Core, Section 3.6.1 Key and Type Definitions.
        

          The actual return values are implementation-defined to permit encoding additional information in them, but this operation for any given collator, when considered as a function of x and y, is required to be a consistent comparator defining a total ordering on the set of all Strings. This operation is also required to recognize and honour canonical equivalence according to the Unicode Standard, including returning +0ùîΩ when comparing distinguishable Strings that are canonically equivalent.
        

        This function provides access to the locale and options computed during initialization of the object.
      

      Intl.Collator instances are ordinary objects that inherit properties from %Intl.Collator.prototype%.
    

      Intl.Collator instances have an [[InitializedCollator]] internal slot.
    

      Intl.Collator instances also have several internal slots that are computed by the constructor:
    

      Intl.Collator instances also have the following internal slots if the key corresponding to the name of the internal slot in Table 4 is included in the [[RelevantExtensionKeys]] internal slot of Intl.Collator:
    

      Finally, Intl.Collator instances have a [[BoundCompare]] internal slot that caches the function returned by the compare accessor (10.3.3).
    

      The Intl.DateTimeFormat constructor is the %Intl.DateTimeFormat% intrinsic object and a standard built-in property of the Intl object. Behaviour common to all service constructor properties of the Intl object is specified in 9.1.
    

        When the Intl.DateTimeFormat function is called with optional arguments locales and options, the following steps are taken:
      
The abstract operation ChainDateTimeFormat takes arguments dateTimeFormat (an Intl.DateTimeFormat), newTarget (an ECMAScript language value), and this (an ECMAScript language value) and returns either a normal completion containing an Object or a throw completion. It performs the following steps when called:
The abstract operation CreateDateTimeFormat takes arguments newTarget (a constructor), locales (an ECMAScript language value), options (an ECMAScript language value), required (date, time, or any), and defaults (date, time, or all) and returns either a normal completion containing a DateTimeFormat object or a throw completion. It performs the following steps when called:
The abstract operation FormatOffsetTimeZoneIdentifier takes argument offsetMinutes (an integer) and returns a String. 
          It formats a UTC offset, in minutes, into a UTC offset string formatted like ¬±HH:MM.
         It performs the following steps when called:

      The Intl.DateTimeFormat constructor has the following properties:
    

        The value of Intl.DateTimeFormat.prototype is %Intl.DateTimeFormat.prototype%.
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
      

        When the supportedLocalesOf method is called with arguments locales and options, the following steps are taken:
      

        The value of the [[AvailableLocales]] internal slot is implementation-defined within the constraints described in 9.1.
      

        The value of the [[RelevantExtensionKeys]] internal slot is ¬´ "ca", "hc", "nu" ¬ª.
      

        The value of the [[LocaleData]] internal slot is implementation-defined within the constraints described in 9.1 and the following additional constraints, for all locale values locale:
      

      The Intl.DateTimeFormat prototype object is itself an ordinary object. %Intl.DateTimeFormat.prototype% is not an Intl.DateTimeFormat instance and does not have an [[InitializedDateTimeFormat]] internal slot or any of the other internal slots of Intl.DateTimeFormat instance objects.
    

        The initial value of Intl.DateTimeFormat.prototype.constructor is %Intl.DateTimeFormat%.
      

        The initial value of the @@toStringTag property is the String value "Intl.DateTimeFormat".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      

        Intl.DateTimeFormat.prototype.format is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
      

        When the formatToParts method is called with an argument date, the following steps are taken:
      

        When the formatRange method is called with arguments startDate and endDate, the following steps are taken:
      

        When the formatRangeToParts method is called with arguments startDate and endDate, the following steps are taken:
      

        This function provides access to the locale and options computed during initialization of the object.
      

        For web compatibility reasons, if the property "hourCycle" is set, the "hour12" property should be set to true when "hourCycle" is "h11" or "h12", or to false when "hourCycle" is "h23" or "h24".
      

      Intl.DateTimeFormat instances are ordinary objects that inherit properties from %Intl.DateTimeFormat.prototype%.
    

      Intl.DateTimeFormat instances have an [[InitializedDateTimeFormat]] internal slot.
    

      Intl.DateTimeFormat instances also have several internal slots that are computed by the constructor:
    

      Finally, Intl.DateTimeFormat instances have a [[BoundFormat]] internal slot that caches the function returned by the format accessor (11.3.3).
    

      Several DateTimeFormat algorithms use values from the following table, which provides internal slots, property names and allowable values for the components of date and time formats:
    
The abstract operation DateTimeStyleFormat takes arguments dateStyle ("full", "long", "medium", "short", or undefined), timeStyle ("full", "long", "medium", "short", or undefined), and styles (a Record) and returns a Record. styles is a record from %Intl.DateTimeFormat%.[[LocaleData]].[[<locale>]].[[styles]].[[<calendar>]] for some locale locale and calendar calendar. It returns the appropriate format record for date time formatting based on the parameters. It performs the following steps when called:
The abstract operation BasicFormatMatcher takes arguments options (a Record) and formats (a List of Records) and returns a Record. It performs the following steps when called:
The implementation-defined abstract operation BestFitFormatMatcher takes arguments options (a Record) and formats (a List of Records) and returns a Record. It returns a set of component representations that a typical user of the selected locale would perceive as at least as good as the one returned by BasicFormatMatcher.
A DateTime format function is an anonymous built-in function that has a [[DateTimeFormat]] internal slot.
When a DateTime format function F is called with optional argument date, the following steps are taken:

        The "length" property of a DateTime format function is 1ùîΩ.
      
The abstract operation FormatDateTimePattern takes arguments dateTimeFormat (an Intl.DateTimeFormat), patternParts (a List of Records as returned by PartitionPattern), x (a Number), and rangeFormatOptions (a range pattern Record as used in [[rangePattern]], or undefined) and returns either a normal completion containing a List of Records with fields [[Type]] (a String) and [[Value]] (a String), or a throw completion. It interprets x as a time value as specified in es2024, 21.4.1.1, and creates the corresponding parts according pattern and to the effective locale and the formatting options of dateTimeFormat and rangeFormatOptions. It performs the following steps when called:
The abstract operation PartitionDateTimePattern takes arguments dateTimeFormat (an Intl.DateTimeFormat) and x (a Number) and returns either a normal completion containing a List of Records with fields [[Type]] (a String) and [[Value]] (a String), or a throw completion. It interprets x as a time value as specified in es2024, 21.4.1.1, and creates the corresponding parts according to the effective locale and the formatting options of dateTimeFormat. It performs the following steps when called:
The abstract operation FormatDateTime takes arguments dateTimeFormat (an Intl.DateTimeFormat) and x (a Number) and returns either a normal completion containing a String or a throw completion. It performs the following steps when called:
The abstract operation FormatDateTimeToParts takes arguments dateTimeFormat (an Intl.DateTimeFormat) and x (a Number) and returns either a normal completion containing an Array or a throw completion. It performs the following steps when called:
The abstract operation PartitionDateTimeRangePattern takes arguments dateTimeFormat (an Intl.DateTimeFormat), x (a Number), and y (a Number) and returns either a normal completion containing a List of Records with fields [[Type]] (a String), [[Value]] (a String), and [[Source]] (a String), or a throw completion. It interprets x and y as time values as specified in es2024, 21.4.1.1, and creates the corresponding parts according to the effective locale and the formatting options of dateTimeFormat. It performs the following steps when called:
The abstract operation FormatDateTimeRange takes arguments dateTimeFormat (an Intl.DateTimeFormat), x (a Number), and y (a Number) and returns either a normal completion containing a String or a throw completion. It performs the following steps when called:
The abstract operation FormatDateTimeRangeToParts takes arguments dateTimeFormat (an Intl.DateTimeFormat), x (a Number), and y (a Number) and returns either a normal completion containing an Array or a throw completion. It performs the following steps when called:
The implementation-defined abstract operation ToLocalTime takes arguments epochNs (a BigInt), calendar (a String), and timeZoneIdentifier (a String) and returns a ToLocalTime Record. It performs the following steps when called:

        Each ToLocalTime Record has the fields defined in Table 8.
      
The abstract operation UnwrapDateTimeFormat takes argument dtf (an ECMAScript language value) and returns either a normal completion containing an ECMAScript language value or a throw completion. 
          It returns the DateTimeFormat instance of its input object, which is either the value itself or a value associated with it by %Intl.DateTimeFormat% according to the normative optional constructor mode of 4.3 Note 1.
         It performs the following steps when called:

      The DisplayNames constructor is the %Intl.DisplayNames% intrinsic object and a standard built-in property of the Intl object. Behaviour common to all service constructor properties of the Intl object is specified in 9.1.
    

        When the Intl.DisplayNames function is called with arguments locales and options, the following steps are taken:
      

      The Intl.DisplayNames constructor has the following properties:
    

        The value of Intl.DisplayNames.prototype is %Intl.DisplayNames.prototype%.
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
      

        When the supportedLocalesOf method is called with arguments locales and options, the following steps are taken:
      

        The value of the [[AvailableLocales]] internal slot is implementation-defined within the constraints described in 9.1.
      

        The value of the [[RelevantExtensionKeys]] internal slot is ¬´ ¬ª.
      

        The value of the [[LocaleData]] internal slot is implementation-defined within the constraints described in 9.1 and the following additional constraints:
      

      The Intl.DisplayNames prototype object is itself an ordinary object. %Intl.DisplayNames.prototype% is not an Intl.DisplayNames instance and does not have an [[InitializedDisplayNames]] internal slot or any of the other internal slots of Intl.DisplayNames instance objects.
    

        The initial value of Intl.DisplayNames.prototype.constructor is %Intl.DisplayNames%.
      

        The initial value of the @@toStringTag property is the String value "Intl.DisplayNames".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      

        When the Intl.DisplayNames.prototype.of is called with an argument code, the following steps are taken:
      

        This function provides access to the locale and options computed during initialization of the object.
      

      Intl.DisplayNames instances are ordinary objects that inherit properties from %Intl.DisplayNames.prototype%.
    

      Intl.DisplayNames instances have an [[InitializedDisplayNames]] internal slot.
    

      Intl.DisplayNames instances also have several internal slots that are computed by the constructor:
    
The abstract operation CanonicalCodeForDisplayNames takes arguments type (a String) and code (a String) and returns either a normal completion containing a String or a throw completion. It verifies that code represents a well-formed code according to type and returns the case-regularized form of code. It performs the following steps when called:
The abstract operation IsValidDateTimeFieldCode takes argument field (a String) and returns a Boolean. It verifies that the field argument represents a valid date time field code. It performs the following steps when called:

      The ListFormat constructor is the %Intl.ListFormat% intrinsic object and a standard built-in property of the Intl object. Behaviour common to all service constructor properties of the Intl object is specified in 9.1.
    

        When the Intl.ListFormat function is called with optional arguments locales and options, the following steps are taken:
      

      The Intl.ListFormat constructor has the following properties:
    

        The value of Intl.ListFormat.prototype is %Intl.ListFormat.prototype%.
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
      

        When the supportedLocalesOf method is called with arguments locales and options, the following steps are taken:
      

        The value of the [[AvailableLocales]] internal slot is implementation-defined within the constraints described in 9.1.
      

        The value of the [[RelevantExtensionKeys]] internal slot is ¬´ ¬ª.
      

        The value of the [[LocaleData]] internal slot is implementation-defined within the constraints described in 9.1 and the following additional constraints, for each locale value locale in %Intl.ListFormat%.[[AvailableLocales]]:
      

      The Intl.ListFormat prototype object is itself an ordinary object. %Intl.ListFormat.prototype% is not an Intl.ListFormat instance and does not have an [[InitializedListFormat]] internal slot or any of the other internal slots of Intl.ListFormat instance objects.
    

        The initial value of Intl.ListFormat.prototype.constructor is %Intl.ListFormat%.
      

        The initial value of the @@toStringTag property is the String value "Intl.ListFormat".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      

        When the format method is called with an argument list, the following steps are taken:
      

        When the formatToParts method is called with an argument list, the following steps are taken:
      

        This function provides access to the locale and options computed during initialization of the object.
      

      Intl.ListFormat instances inherit properties from %Intl.ListFormat.prototype%.
    

      Intl.ListFormat instances have an [[InitializedListFormat]] internal slot.
    

      Intl.ListFormat instances also have several internal slots that are computed by the constructor:
    
The abstract operation DeconstructPattern takes arguments pattern (a String) and placeables (a Record) and returns a List. 
          It deconstructs the pattern string into a list of parts.
          
placeables is a Record whose keys are placeables tokens used in the pattern string, and values are parts records (as from PartitionPattern) which will be used in the result List to represent the token part.
            Example:
          
Input:
  DeconstructPattern("AA{xx}BB{yy}CC", {
    [[xx]]: {[[Type]]: "hour", [[Value]]: "15"},
    [[yy]]: {[[Type]]: "minute", [[Value]]: "06"}
  })

Output (List of parts records):
  ¬´
    {[[Type]]: "literal", [[Value]]: "AA"},
    {[[Type]]: "hour", [[Value]]: "15"},
    {[[Type]]: "literal", [[Value]]: "BB"},
    {[[Type]]: "minute", [[Value]]: "06"},
    {[[Type]]: "literal", [[Value]]: "CC"}
  ¬ª
          
         It performs the following steps when called:

placeables is a Record whose keys are placeables tokens used in the pattern string, and values are parts records (as from PartitionPattern) which will be used in the result List to represent the token part.
            Example:
          
The abstract operation CreatePartsFromList takes arguments listFormat (an Intl.ListFormat) and list (a List of Strings) and returns a List of Records with fields [[Type]] ("element" or "literal") and [[Value]] (a String). It creates the corresponding list of parts according to the effective locale and the formatting options of listFormat. It performs the following steps when called:
The abstract operation FormatList takes arguments listFormat (an Intl.ListFormat) and list (a List of Strings) and returns a String. It performs the following steps when called:
The abstract operation FormatListToParts takes arguments listFormat (an Intl.ListFormat) and list (a List of Strings) and returns an Array. It performs the following steps when called:
The abstract operation StringListFromIterable takes argument iterable (an ECMAScript language value) and returns either a normal completion containing a List of Strings or a throw completion. It performs the following steps when called:
This algorithm raises exceptions when it encounters values that are not Strings, because there is no obvious locale-aware coercion for arbitrary values.

      The Locale constructor is the %Intl.Locale% intrinsic object and a standard built-in property of the Intl object.
    

        When the Intl.Locale function is called with an argument tag and an optional argument options, the following steps are taken:
      
The abstract operation ApplyOptionsToTag takes arguments tag (a String) and options (an Object) and returns either a normal completion containing a Unicode canonicalized locale identifier or a throw completion. It performs the following steps when called:
The abstract operation ApplyUnicodeExtensionToTag takes arguments tag (a String), options (a Record), and relevantExtensionKeys (a List of Strings) and returns a Record. It performs the following steps when called:

      The Intl.Locale constructor has the following properties:
    

        The value of Intl.Locale.prototype is %Intl.Locale.prototype%.
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
      

        The value of the [[RelevantExtensionKeys]] internal slot is ¬´ "ca", "co", "hc", "kf", "kn", "nu" ¬ª. If %Intl.Collator%.[[RelevantExtensionKeys]] does not contain "kf", then remove "kf" from %Intl.Locale%.[[RelevantExtensionKeys]]. If %Intl.Collator%.[[RelevantExtensionKeys]] does not contain "kn", then remove "kn" from %Intl.Locale%.[[RelevantExtensionKeys]].
      

      The Intl.Locale prototype object is itself an ordinary object. %Intl.Locale.prototype% is not an Intl.Locale instance and does not have an [[InitializedLocale]] internal slot or any of the other internal slots of Intl.Locale instance objects.
    

        The initial value of Intl.Locale.prototype.constructor is %Intl.Locale%.
      

        The initial value of the @@toStringTag property is the String value "Intl.Locale".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      
Intl.Locale.prototype.baseName is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
Intl.Locale.prototype.calendar is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
This property only exists if %Intl.Locale%.[[RelevantExtensionKeys]] contains "kf".
Intl.Locale.prototype.caseFirst is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
Intl.Locale.prototype.collation is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
Intl.Locale.prototype.hourCycle is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
This property only exists if %Intl.Locale%.[[RelevantExtensionKeys]] contains "kn".
Intl.Locale.prototype.numeric is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
Intl.Locale.prototype.numberingSystem is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
Intl.Locale.prototype.language is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
Intl.Locale.prototype.script is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
Intl.Locale.prototype.region is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:

      Intl.Locale instances are ordinary objects that inherit properties from %Intl.Locale.prototype%.
    

      Intl.Locale instances have an [[InitializedLocale]] internal slot.
    

      Intl.Locale instances also have several internal slots that are computed by the constructor:
    
The abstract operation GetLocaleLanguage takes argument locale (a String) and returns a String. It performs the following steps when called:
The abstract operation GetLocaleScript takes argument locale (a String) and returns a String or undefined. It performs the following steps when called:
The abstract operation GetLocaleRegion takes argument locale (a String) and returns a String or undefined. It performs the following steps when called:
The abstract operation GetLocaleVariants takes argument locale (a String) and returns a String or undefined. It performs the following steps when called:

      The NumberFormat constructor is the %Intl.NumberFormat% intrinsic object and a standard built-in property of the Intl object. Behaviour common to all service constructor properties of the Intl object is specified in 9.1.
    

        When the Intl.NumberFormat function is called with optional arguments locales and options, the following steps are taken:
      
The abstract operation ChainNumberFormat takes arguments numberFormat (an Intl.NumberFormat), newTarget (an ECMAScript language value), and this (an ECMAScript language value) and returns either a normal completion containing an Object or a throw completion. It performs the following steps when called:
The abstract operation InitializeNumberFormat takes arguments numberFormat (an Intl.NumberFormat), locales (an ECMAScript language value), and options (an ECMAScript language value) and returns either a normal completion containing unused or a throw completion. It initializes numberFormat as a NumberFormat object. It performs the following steps when called:
The abstract operation SetNumberFormatDigitOptions takes arguments intlObj (an Object), options (an Object), mnfdDefault (an integer), mxfdDefault (an integer), and notation (a String) and returns either a normal completion containing unused or a throw completion. It populates the internal slots of intlObj that affect locale-independent number rounding (see 15.5.3). It performs the following steps when called:
The abstract operation SetNumberFormatUnitOptions takes arguments intlObj (an Intl.NumberFormat) and options (an Object) and returns either a normal completion containing unused or a throw completion. It resolves the user-specified options relating to units onto intlObj. It performs the following steps when called:

      The Intl.NumberFormat constructor has the following properties:
    

        The value of Intl.NumberFormat.prototype is %Intl.NumberFormat.prototype%.
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
      

        When the supportedLocalesOf method is called with arguments locales and options, the following steps are taken:
      

        The value of the [[AvailableLocales]] internal slot is implementation-defined within the constraints described in 9.1.
      

        The value of the [[RelevantExtensionKeys]] internal slot is ¬´ "nu" ¬ª.
      

        The value of the [[LocaleData]] internal slot is implementation-defined within the constraints described in 9.1 and the following additional constraints:
      

      The Intl.NumberFormat prototype object is itself an ordinary object. %Intl.NumberFormat.prototype% is not an Intl.NumberFormat instance and does not have an [[InitializedNumberFormat]] internal slot or any of the other internal slots of Intl.NumberFormat instance objects.
    

        The initial value of Intl.NumberFormat.prototype.constructor is %Intl.NumberFormat%.
      

        The initial value of the @@toStringTag property is the String value "Intl.NumberFormat".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      

        Intl.NumberFormat.prototype.format is an accessor property whose set accessor function is undefined. Its get accessor function performs the following steps:
      

        When the formatToParts method is called with an optional argument value, the following steps are taken:
      

        When the formatRange method is called with arguments start and end, the following steps are taken:
      

        When the formatRangeToParts method is called with arguments start and end, the following steps are taken:
      

        This function provides access to the locale and options computed during initialization of the object.
      

      Intl.NumberFormat instances are ordinary objects that inherit properties from %Intl.NumberFormat.prototype%.
    

      Intl.NumberFormat instances have an [[InitializedNumberFormat]] internal slot.
    

      Intl.NumberFormat instances also have several internal slots that are computed by the constructor:
    

      Finally, Intl.NumberFormat instances have a [[BoundFormat]] internal slot that caches the function returned by the format accessor (15.3.3).
    
The abstract operation CurrencyDigits takes argument currency (a String) and returns a non-negative integer. It performs the following steps when called:
A Number format function is an anonymous built-in function that has a [[NumberFormat]] internal slot.
When a Number format function F is called with optional argument value, the following steps are taken:

        The "length" property of a Number format function is 1ùîΩ.
      
The abstract operation FormatNumericToString takes arguments intlObject (an Object) and x (a mathematical value or negative-zero) and returns a Record with fields [[RoundedNumber]] (a mathematical value or negative-zero) and [[FormattedString]] (a String). It rounds x to an Intl mathematical value according to the internal slots of intlObject. The [[RoundedNumber]] field contains the rounded result value and the [[FormattedString]] field contains a String value representation of that result formatted according to the internal slots of intlObject. It performs the following steps when called:
The abstract operation PartitionNumberPattern takes arguments numberFormat (an object initialized as a NumberFormat) and x (an Intl mathematical value) and returns a List of Records with fields [[Type]] (a String) and [[Value]] (a String). It creates the parts representing the mathematical value of x according to the effective locale and the formatting options of numberFormat. It performs the following steps when called:
The abstract operation PartitionNotationSubPattern takes arguments numberFormat (an Intl.NumberFormat), x (an Intl mathematical value), n (a String), and exponent (an integer) and returns a List of Records with fields [[Type]] (a String) and [[Value]] (a String). 
          x is an Intl mathematical value after rounding is applied and n is an intermediate formatted string.
          It creates the corresponding parts for the number and notation according to the effective locale and the formatting options of numberFormat.
         It performs the following steps when called:
The abstract operation FormatNumeric takes arguments numberFormat (an Intl.NumberFormat) and x (an Intl mathematical value) and returns a String. It performs the following steps when called:
The abstract operation FormatNumericToParts takes arguments numberFormat (an Intl.NumberFormat) and x (an Intl mathematical value) and returns an Array. It performs the following steps when called:
The abstract operation ToRawPrecision takes arguments x (non-negative mathematical value), minPrecision (an integer in the inclusive interval from 1 to 21), maxPrecision (an integer in the inclusive interval from 1 to 21), and unsignedRoundingMode (a specification type from the Unsigned Rounding Mode column of Table 15, or undefined) and returns a Record with fields [[FormattedString]] (a String), [[RoundedNumber]] (a mathematical value), [[IntegerDigitsCount]] (an integer), and [[RoundingMagnitude]] (an integer).

            It involves solving the following equation, which returns a valid mathematical value given integer inputs:
          
It performs the following steps when called:
The abstract operation ToRawFixed takes arguments x (non-negative mathematical value), minFraction (an integer in the inclusive interval from 0 to 100), maxFraction (an integer in the inclusive interval from 0 to 100), roundingIncrement (an integer), and unsignedRoundingMode (a specification type from the Unsigned Rounding Mode column of Table 15, or undefined) and returns a Record with fields [[FormattedString]] (a String), [[RoundedNumber]] (a mathematical value), [[IntegerDigitsCount]] (an integer), and [[RoundingMagnitude]] (an integer).

            It involves solving the following equation, which returns a valid mathematical value given integer inputs:
          
It performs the following steps when called:
The abstract operation UnwrapNumberFormat takes argument nf (an ECMAScript language value) and returns either a normal completion containing an ECMAScript language value or a throw completion. 
          It returns the NumberFormat instance of its input object, which is either the value itself or a value associated with it by %Intl.NumberFormat% according to the normative optional constructor mode of 4.3 Note 1.
         It performs the following steps when called:
The abstract operation GetNumberFormatPattern takes arguments numberFormat (an Intl.NumberFormat) and x (an Intl mathematical value) and returns a String. 
          It considers the resolved unit-related options in the number format object along with the final scaled and rounded number being formatted (an Intl mathematical value) and returns a pattern, a String value as described in 15.2.3.
         It performs the following steps when called:
The abstract operation GetNotationSubPattern takes arguments numberFormat (an Intl.NumberFormat) and exponent (an integer) and returns a String. 
          It considers the resolved notation and exponent, and returns a String value for the notation sub pattern as described in 15.2.3.
         It performs the following steps when called:
The abstract operation ComputeExponent takes arguments numberFormat (an Intl.NumberFormat) and x (a mathematical value) and returns an integer. 
          It computes an exponent (power of ten) by which to scale x according to the number formatting settings.
          It handles cases such as 999 rounding up to 1000, requiring a different exponent.
         It performs the following steps when called:
The abstract operation ComputeExponentForMagnitude takes arguments numberFormat (an Intl.NumberFormat) and magnitude (an integer) and returns an integer. 
          It computes an exponent by which to scale a number of the given magnitude (power of ten of the most significant digit) according to the locale and the desired notation (scientific, engineering, or compact).
         It performs the following steps when called:
The syntax-directed operation StringIntlMV takes no arguments.
The conversion of a StringNumericLiteral to a Number value is similar overall to the determination of the NumericValue of a NumericLiteral (see 12.9.3), but some of the details are different.
It is defined piecewise over the following productions:
The abstract operation ToIntlMathematicalValue takes argument value (an ECMAScript language value) and returns either a normal completion containing an Intl mathematical value or a throw completion. 
          It returns value converted to an Intl mathematical value, which is a mathematical value together with positive-infinity, negative-infinity, not-a-number, and negative-zero.
          This abstract operation is similar to 7.1.3, but a mathematical value can be returned instead of a Number or BigInt, so that exact decimal values can be represented.
         It performs the following steps when called:
The abstract operation GetUnsignedRoundingMode takes arguments roundingMode (a String) and sign (negative or positive) and returns a specification type from the Unsigned Rounding Mode column of Table 15. It returns the rounding mode that should be applied to the absolute value of a number to produce the same result as if roundingMode, one of the String values in the Identifier column of Table 13, were applied to the signed value of the number (negative if sign is negative, or positive otherwise). It performs the following steps when called:
The abstract operation ApplyUnsignedRoundingMode takes arguments x (a mathematical value), r1 (a mathematical value), r2 (a mathematical value), and unsignedRoundingMode (a specification type from the Unsigned Rounding Mode column of Table 15, or undefined) and returns a mathematical value. It considers x, bracketed below by r1 and above by r2, and returns either r1 or r2 according to unsignedRoundingMode. It performs the following steps when called:
The abstract operation PartitionNumberRangePattern takes arguments numberFormat (an Intl.NumberFormat), x (an Intl mathematical value), and y (an Intl mathematical value) and returns either a normal completion containing a List of Records with fields [[Type]] (a String), [[Value]] (a String), and [[Source]] (a String), or a throw completion. It creates the parts for a localized number range according to x, y, and the formatting options of numberFormat. It performs the following steps when called:
The abstract operation FormatApproximately takes arguments numberFormat (an Intl.NumberFormat) and result (a List of Records with fields [[Type]] (a String) and [[Value]] (a String)) and returns a List of Records with fields [[Type]] (a String) and [[Value]] (a String). It modifies result, which must be a List of Record values as described in PartitionNumberPattern, by adding a new Record for the approximately sign, which may depend on numberFormat. It performs the following steps when called:
The implementation-defined abstract operation CollapseNumberRange takes argument result (a List of Records with fields [[Type]] (a String), [[Value]] (a String), and [[Source]] (a String)) and returns a List of Records with fields [[Type]] (a String), [[Value]] (a String), and [[Source]] (a String). It modifies result (which must be a List of Records as constructed within PartitionNumberRangePattern) by removing redundant information and resolving internal inconsistency, and returns the resulting List. The algorithm is implementation dependent, but must not introduce ambiguity that would cause the result of Intl.NumberFormat.prototype.formatRange ( start, end ) with arguments List ¬´ start1, end1 ¬ª to equal the result with arguments List ¬´ start2, end2 ¬ª if the results for those same arguments Lists would not be equal with a trivial implementation of CollapseNumberRange that always returns result unmodified.

        For example, an implementation may remove the Record representing a currency symbol after a range separator to convert a results List representing "$3‚Äì$5" into one representing "$3‚Äì5".
      

        An implementation may also modify Record [[Value]] fields for grammatical correctness; for example, converting a results List representing "0.5 miles‚Äì1 mile" into one representing "0.5‚Äì1 miles".
      

        Returning result unmodified is guaranteed to be a correct implementation of CollapseNumberRange.
      
The abstract operation FormatNumericRange takes arguments numberFormat (an Intl.NumberFormat), x (an Intl mathematical value), and y (an Intl mathematical value) and returns either a normal completion containing a String or a throw completion. It performs the following steps when called:
The abstract operation FormatNumericRangeToParts takes arguments numberFormat (an Intl.NumberFormat), x (an Intl mathematical value), and y (an Intl mathematical value) and returns either a normal completion containing an Array or a throw completion. It performs the following steps when called:

      The PluralRules constructor is the %Intl.PluralRules% intrinsic object and a standard built-in property of the Intl object. Behaviour common to all service constructor properties of the Intl object is specified in 9.1.
    

        When the Intl.PluralRules function is called with optional arguments locales and options, the following steps are taken:
      
The abstract operation InitializePluralRules takes arguments pluralRules (an Intl.PluralRules), locales (an ECMAScript language value), and options (an ECMAScript language value) and returns either a normal completion containing pluralRules or a throw completion. It initializes pluralRules as a PluralRules object. It performs the following steps when called:

      The Intl.PluralRules constructor has the following properties:
    

        The value of Intl.PluralRules.prototype is %Intl.PluralRules.prototype%.
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
      

        When the supportedLocalesOf method is called with arguments locales and options, the following steps are taken:
      

        The value of the [[AvailableLocales]] internal slot is implementation-defined within the constraints described in 9.1.
      

        The value of the [[RelevantExtensionKeys]] internal slot is ¬´ ¬ª.
      

        The value of the [[LocaleData]] internal slot is implementation-defined within the constraints described in 9.1.
      

      The Intl.PluralRules prototype object is itself an ordinary object. %Intl.PluralRules.prototype% is not an Intl.PluralRules instance and does not have an [[InitializedPluralRules]] internal slot or any of the other internal slots of Intl.PluralRules instance objects.
    

        The initial value of Intl.PluralRules.prototype.constructor is %Intl.PluralRules%.
      

        The initial value of the @@toStringTag property is the String value "Intl.PluralRules".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      

        When the select method is called with an argument value, the following steps are taken:
      

        When the selectRange method is called with arguments start and end, the following steps are taken:
      

        This function provides access to the locale and options computed during initialization of the object.
      

      Intl.PluralRules instances are ordinary objects that inherit properties from %Intl.PluralRules.prototype%.
    

      Intl.PluralRules instances have an [[InitializedPluralRules]] internal slot.
    

      Intl.PluralRules instances also have several internal slots that are computed by the constructor:
    
The abstract operation GetOperands takes argument s (a decimal String) and returns a Plural Rules Operands Record. It extracts numeric features from s that correspond with the operands of Unicode Technical Standard #35 Part 3 Numbers, Section 5.1.1 Operands. It performs the following steps when called:

        Each Plural Rules Operands Record has the fields defined in Table 17.
      
The implementation-defined abstract operation PluralRuleSelect takes arguments locale (a String), type ("cardinal" or "ordinal"), n (a finite Number), and operands (a Plural Rules Operands Record derived from formatting n) and returns "zero", "one", "two", "few", "many", or "other". The returned String best categorizes the operands representation of n according to the rules for locale and type.
The abstract operation ResolvePlural takes arguments pluralRules (an Intl.PluralRules) and n (a Number) and returns a Record with fields [[PluralCategory]] ("zero", "one", "two", "few", "many", or "other") and [[FormattedString]] (a String). The returned Record contains two string-valued fields describing n according to the effective locale and the options of pluralRules: [[PluralCategory]] characterizing its plural category, and [[FormattedString]] containing its formatted representation. It performs the following steps when called:
The implementation-defined abstract operation PluralRuleSelectRange takes arguments locale (a String), type ("cardinal" or "ordinal"), xp ("zero", "one", "two", "few", "many", or "other"), and yp ("zero", "one", "two", "few", "many", or "other") and returns "zero", "one", "two", "few", "many", or "other". It performs an implementation-dependent algorithm to map the plural category String values xp and yp, respectively characterizing the start and end of a range, to a resolved String value for the plural form of the range as a whole denoted by type for the corresponding locale, or the String value "other".
The abstract operation ResolvePluralRange takes arguments pluralRules (an Intl.PluralRules), x (a Number), and y (a Number) and returns either a normal completion containing either "zero", "one", "two", "few", "many", or "other", or a throw completion. The returned String value represents the plural form of the range starting from x and ending at y according to the effective locale and the options of pluralRules. It performs the following steps when called:

      The RelativeTimeFormat constructor is the %Intl.RelativeTimeFormat% intrinsic object and a standard built-in property of the Intl object. Behaviour common to all service constructor properties of the Intl object is specified in 9.1.
    

        When the Intl.RelativeTimeFormat function is called with optional arguments locales and options, the following steps are taken:
      
The abstract operation InitializeRelativeTimeFormat takes arguments relativeTimeFormat (an Intl.RelativeTimeFormat), locales (an ECMAScript language value), and options (an ECMAScript language value) and returns either a normal completion containing relativeTimeFormat or a throw completion. It initializes relativeTimeFormat as a RelativeTimeFormat object. It performs the following steps when called:

      The Intl.RelativeTimeFormat constructor has the following properties:
    

        The value of Intl.RelativeTimeFormat.prototype is %Intl.RelativeTimeFormat.prototype%.
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
      

        When the supportedLocalesOf method is called with arguments locales and options, the following steps are taken:
      

        The value of the [[AvailableLocales]] internal slot is implementation-defined within the constraints described in 9.1.
      

        The value of the [[RelevantExtensionKeys]] internal slot is ¬´ "nu" ¬ª.
      

        The value of the [[LocaleData]] internal slot is implementation-defined within the constraints described in 9.1 and the following additional constraints, for all locale values locale:
      

      The Intl.RelativeTimeFormat prototype object is itself an ordinary object. %Intl.RelativeTimeFormat.prototype% is not an Intl.RelativeTimeFormat instance and does not have an [[InitializedRelativeTimeFormat]] internal slot or any of the other internal slots of Intl.RelativeTimeFormat instance objects.
    

        The initial value of Intl.RelativeTimeFormat.prototype.constructor is %Intl.RelativeTimeFormat%.
      

        The initial value of the @@toStringTag property is the String value "Intl.RelativeTimeFormat".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      

        When the format method is called with arguments value and unit, the following steps are taken:
      

        When the formatToParts method is called with arguments value and unit, the following steps are taken:
      

        This function provides access to the locale and options computed during initialization of the object.
      

      Intl.RelativeTimeFormat instances are ordinary objects that inherit properties from %Intl.RelativeTimeFormat.prototype%.
    

      Intl.RelativeTimeFormat instances have an [[InitializedRelativeTimeFormat]] internal slot.
    

      Intl.RelativeTimeFormat instances also have several internal slots that are computed by the constructor:
    
The abstract operation SingularRelativeTimeUnit takes argument unit (a String) and returns either a normal completion containing a String or a throw completion. It performs the following steps when called:
The abstract operation PartitionRelativeTimePattern takes arguments relativeTimeFormat (an Intl.RelativeTimeFormat), value (a Number), and unit (a String) and returns either a normal completion containing a List of Records with fields [[Type]] (a String), [[Value]] (a String), and [[Unit]] (a String or empty), or a throw completion. The returned List represents value according to the effective locale and the formatting options of relativeTimeFormat. It performs the following steps when called:
The abstract operation MakePartsList takes arguments pattern (a pattern String), unit (a String), and parts (a List of Records representing a formatted Number) and returns a List of Records with fields [[Type]] (a String), [[Value]] (a String), and [[Unit]] (a String or empty). It performs the following steps when called:
The abstract operation FormatRelativeTime takes arguments relativeTimeFormat (an Intl.RelativeTimeFormat), value (a Number), and unit (a String) and returns either a normal completion containing a String or a throw completion. It performs the following steps when called:
The abstract operation FormatRelativeTimeToParts takes arguments relativeTimeFormat (an Intl.RelativeTimeFormat), value (a Number), and unit (a String) and returns either a normal completion containing an Array or a throw completion. It performs the following steps when called:

      The Segmenter constructor is the %Intl.Segmenter% intrinsic object and a standard built-in property of the Intl object. Behaviour common to all service constructor properties of the Intl object is specified in 9.1.
    

        When the Intl.Segmenter function is called with optional arguments locales and options, the following steps are taken:
      

      The Intl.Segmenter constructor has the following properties:
    

        The value of Intl.Segmenter.prototype is %Intl.Segmenter.prototype%.
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: false }.
      

        When the supportedLocalesOf method is called with arguments locales and options, the following steps are taken:
      

        The value of the [[AvailableLocales]] internal slot is implementation-defined within the constraints described in 9.1.
      

        The value of the [[RelevantExtensionKeys]] internal slot is ¬´ ¬ª.
      

        The value of the [[LocaleData]] internal slot is implementation-defined within the constraints described in 9.1.
      

      The Intl.Segmenter prototype object is itself an ordinary object. %Intl.Segmenter.prototype% is not an Intl.Segmenter instance and does not have an [[InitializedSegmenter]] internal slot or any of the other internal slots of Intl.Segmenter instance objects.
    

        The initial value of Intl.Segmenter.prototype.constructor is %Intl.Segmenter%.
      

        The initial value of the @@toStringTag property is the String value "Intl.Segmenter".
      

        This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
      

        The Intl.Segmenter.prototype.segment method is called on an Intl.Segmenter instance with argument string to create a Segments instance for the string using the locale and options of the Intl.Segmenter instance. The following steps are taken:
      

        This function provides access to the locale and options computed during initialization of the object.
      

      Intl.Segmenter instances are ordinary objects that inherit properties from %Intl.Segmenter.prototype%.
    

      Intl.Segmenter instances have an [[InitializedSegmenter]] internal slot.
    

      Intl.Segmenter instances also have internal slots that are computed by the constructor:
    

      A Segments instance is an object that represents the segments of a specific string, subject to the locale and options of its constructing Intl.Segmenter instance.
    
The abstract operation CreateSegmentsObject takes arguments segmenter (an Intl.Segmenter) and string (a String) and returns a Segments instance. The Segments instance references segmenter and string. It performs the following steps when called:
The %IntlSegmentsPrototype% object:
The containing method is called on a Segments instance with argument index to return a Segment Data object describing the segment in the string including the code unit at the specified index according to the locale and options of the Segments instance's constructing Intl.Segmenter instance. The following steps are taken:
The @@iterator method is called on a Segments instance to create a Segment Iterator over its string using the locale and options of its constructing Intl.Segmenter instance. The following steps are taken:

        Segments instances are ordinary objects that inherit properties from %IntlSegmentsPrototype%.
      

        Segments instances have a [[SegmentsSegmenter]] internal slot that references the constructing Intl.Segmenter instance.
      

        Segments instances have a [[SegmentsString]] internal slot that references the String value whose segments they expose.
      

      A Segment Iterator is an object that represents a particular iteration over the segments of a specific string.
    
The abstract operation CreateSegmentIterator takes arguments segmenter (an Intl.Segmenter) and string (a String) and returns a Segment Iterator. The Segment Iterator iterates over string using the locale and options of segmenter. It performs the following steps when called:
The %IntlSegmentIteratorPrototype% object:
The next method is called on a Segment Iterator instance to advance it forward one segment and return an IteratorResult object either describing the new segment or declaring iteration done. The following steps are taken:

          The initial value of the @@toStringTag property is the String value "Segmenter String Iterator".
        

          This property has the attributes { [[Writable]]: false, [[Enumerable]]: false, [[Configurable]]: true }.
        
Segment Iterator instances are ordinary objects that inherit properties from %SegmentIteratorPrototype%. Segment Iterator instances are initially created with the internal slots described in Table 20.

      A Segment Data object is an object that represents a particular segment from a string.
    
The abstract operation CreateSegmentDataObject takes arguments segmenter (an Intl.Segmenter), string (a String), startIndex (a non-negative integer), and endIndex (a non-negative integer) and returns a Segment Data object. The Segment Data object describes the segment within string from segmenter that is bounded by the indices startIndex and endIndex. It performs the following steps when called:
The abstract operation FindBoundary takes arguments segmenter (an Intl.Segmenter), string (a String), startIndex (a non-negative integer), and direction (before or after) and returns a non-negative integer or +‚àû. It finds a segmentation boundary between two code units in string in the specified direction from the code unit at index startIndex according to the locale and options of segmenter and returns the immediately following code unit index (which will be infinite if no such boundary exists). It performs the following steps when called:

    The ECMAScript Language Specification, edition 10 or successor, describes several locale-sensitive functions. An ECMAScript implementation that implements this specification shall implement these functions as described here.
  

        This definition supersedes the definition provided in es2024, 22.1.3.12.
      

        When the localeCompare method is called with argument that and optional arguments locales, and options, the following steps are taken:
      

        This definition supersedes the definition provided in es2024, 22.1.3.26.
      

        This function interprets a String value as a sequence of code points, as described in es2024, 6.1.4. The following steps are taken:
      
The abstract operation TransformCase takes arguments S (a String), locales (an ECMAScript language value), and targetCase (lower or upper). It interprets S as a sequence of UTF-16 encoded code points, as described in es2024, 6.1.4, and returns the result of implementation- and locale-dependent (ILD) transformation into targetCase as a new String value. It performs the following steps when called:

          Code point mappings may be derived according to a tailored version of the Default Case Conversion Algorithms of the Unicode Standard. Implementations may use locale-sensitive tailoring defined in the file SpecialCasing.txt of the Unicode Character Database and/or CLDR and/or any other custom tailoring. Regardless of tailoring, a conforming implementation's case transformation algorithm must always yield the same result given the same input code points, locale, and target case.
        

        This definition supersedes the definition provided in es2024, 22.1.3.27.
      

        This function interprets a String value as a sequence of code points, as described in es2024, 6.1.4. The following steps are taken:
      

      The following definition(s) refer to the abstract operation thisNumberValue as defined in es2024, 21.1.3.
    

        This definition supersedes the definition provided in es2024, 21.1.3.4.
      

        When the toLocaleString method is called with optional arguments locales and options, the following steps are taken:
      

      The following definition(s) refer to the abstract operation thisBigIntValue as defined in es2024, 21.2.3.
    

        This definition supersedes the definition provided in es2024, 21.2.3.2.
      

        When the toLocaleString method is called with optional arguments locales and options, the following steps are taken:
      

      The following definition(s) refer to the abstract operation thisTimeValue as defined in es2024, 21.4.4.
    

        This definition supersedes the definition provided in es2024, 21.4.4.39.
      

        When the toLocaleString method is called with optional arguments locales and options, the following steps are taken:
      

        This definition supersedes the definition provided in es2024, 21.4.4.38.
      

        When the toLocaleDateString method is called with optional arguments locales and options, the following steps are taken:
      

        This definition supersedes the definition provided in es2024, 21.4.4.40.
      

        When the toLocaleTimeString method is called with optional arguments locales and options, the following steps are taken:
      

        This definition supersedes the definition provided in es2024, 23.1.3.32.
      

        When the toLocaleString method is called with optional arguments locales and options, the following steps are taken:
      

    The following aspects of this specification are implementation dependent:
  
This specification is authored on GitHub in a plaintext source format called Ecmarkup. Ecmarkup is an HTML and Markdown dialect that provides a framework and toolset for authoring ECMAScript specifications in plaintext and processing the specification into a full-featured HTML rendering that follows the editorial conventions for this document. Ecmarkup builds on and integrates a number of other formats and technologies including Grammarkdown for defining syntax and Ecmarkdown for authoring algorithm steps. PDF renderings of this specification are produced by printing the HTML rendering to a PDF.
Prior editions of this specification were authored using Word‚Äîthe Ecmarkup source text that formed the basis of this edition was produced by converting the ECMAScript 2015 Word document to Ecmarkup using an automated conversion tool.
Ecma International
Rue du Rhone 114
CH-1204 Geneva
Tel: +41 22 849 6000
Fax: +41 22 849 6001
Web: https://ecma-international.org/
¬© 2024 Ecma International
This draft document may be copied and furnished to others, and derivative works that comment on or otherwise explain it or assist in its implementation may be prepared, copied, published, and distributed, in whole or in part, without restriction of any kind, provided that the above copyright notice and this section are included on all such copies and derivative works. However, this document itself may not be modified in any way, including by removing the copyright notice or references to Ecma International, except as needed for the purpose of developing any document or deliverable produced by Ecma International.
This disclaimer is valid only prior to final version of this document. After approval all rights on the standard are reserved by Ecma International.
The limited permissions are granted through the standardization phase and will not be revoked by Ecma International or its successors or assigns during this time.
This document and the information contained herein is provided on an "AS IS" basis and ECMA INTERNATIONAL DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTY THAT THE USE OF THE INFORMATION HEREIN WILL NOT INFRINGE ANY OWNERSHIP RIGHTS OR ANY IMPLIED WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
All Software contained in this document ("Software") is protected by copyright and is being made available under the "BSD License", included below. This Software may be subject to third party rights (rights from parties other than Ecma International), including patent rights, and no licenses under such third party rights are granted under this license even if the third party concerned is a member of Ecma International. SEE THE ECMA CODE OF CONDUCT IN PATENT MATTERS AVAILABLE AT https://ecma-international.org/memento/codeofconduct.htm FOR INFORMATION REGARDING THE LICENSING OF PATENT CLAIMS THAT ARE REQUIRED TO IMPLEMENT ECMA INTERNATIONAL STANDARDS.
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
THIS SOFTWARE IS PROVIDED BY THE ECMA INTERNATIONAL "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL ECMA INTERNATIONAL BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          

        Tracking ECMAScript Proposals
      

        Tracking ECMAScript Proposals
      
The Ecma TC39 committee is responsible for evolving the ECMAScript programming language and authoring the specification. The committee operates by consensus and has discretion to alter the specification as it sees fit. However, the general process for making changes to the specification is as follows.

Stages
Changes to the language are developed by way of a process which provides guidelines for evolving an addition from an idea to a fully specified feature, complete with acceptance tests and multiple implementations. There are six stages: a strawperson stage and five ‚Äúmaturity‚Äù stages. The TC39 committee must approve acceptance for each stage.

Proposals at stage 1 and beyond should be owned by the TC39 committee. Upon proposal acceptance, any externally-owned repositories should be transferred by following the onboarding instructions.


ECMAScript Proposal Stages


Stage
      Status
      Entrance Criteria
      Purpose
    


0
    This is a new proposal. It is not currently being considered by the committee.
    None. New proposals are assigned this stage by their authors outside of the usual advancement process.
    
      Ideation and exploration. Define a problem space in which the committee and the champions can focus their efforts.
      
Make the case for an improvement
        Describe the shape of some possible solutions
        Identify potential challenges
        Research how the problem is dealt with using available facilities today
        Research how the problem has been solved by other languages or in the library ecosystem
      


1
    This proposal is under consideration. The committee expects to devote time to examining the identified problem space, the full breadth of possible solutions, and cross-cutting concerns. 
    

Identified a champion or champion group who will advance the addition
        Prose outlining the problem or need and the general shape of a solution
        Discussion of key algorithms, abstractions and semantics
        Identification of potential cross-cutting concerns and implementation challenges/complexity
        A publicly available repository for the proposal that captures the above requirements
      


      Designing a solution.
      
Make the case for a particular solution or solution space
        Resolve any cross-cutting concerns
      



2
    The committee has chosen a preferred solution or solution space, but the design is a draft and may still change significantly. The committee expects the feature to be developed and eventually included in the standard, but due to reasons that may not be apparent at this stage, the feature may never be included in the standard.
    

Proposal document describes all high-level APIs and syntax
        Illustrative examples of usage
        Initial spec text including all major semantics, syntax, and APIs. Placeholders, TODOs, and editorial issues are acceptable
      


      Refining the solution.
      
Work out minor details such as ordering of observable effects, handling of invalid inputs, API names, etc.
        Receive and address spec text reviews from the assigned reviewers and the appropriate editor group
        Produce experimental implementations such as loosely-correct (not for production use) polyfills to aid in validating the design and exploring the details
        Investigate integration with relevant host APIs, if necessary
      



2.7
    The proposal is approved in principle and undergoing validation. The solution is complete and no further work is possible without feedback from tests, implementations, or usage. No changes to the proposal will be requested by the committee aside from those elicited through testing, implementation, or usage experience.
    

Complete spec text: all semantics, syntax, and APIs are completely described
        Assigned reviewers have signed off on the current spec text
        Relevant editor group has signed off on the current spec text
      

      Testing and validation.
      
Validate the design of the feature through the development of a rigorous and comprehensive test suite
        Develop spec-compliant prototypes to validate implementability, as necessary, or aid in test development
      



3
    The proposal has been recommended for implementation. No changes to the proposal are expected, but some necessary changes may still occur due to web incompatibilities or feedback from production-grade implementations.
    

The feature has sufficient testing and appropriate pre-implementation experience
      

Gaining implementation experience and discovering any web compatibility or integration issues.
  

4
    The proposed feature is complete and ready to be included in the standard. No further changes will be made to the proposal.
    

Two compatible implementations which pass the test262 acceptance tests
        Significant in-the-field experience with shipping implementations, such as that provided by two independent VMs
        A pull request has been sent to tc39/ecma262 or tc39/ecma402, as appropriate, with the integrated spec text
        The relevant editor group has signed off on the pull request
      

Integration into the draft specification and eventual inclusion in a yearly standard publication.
  

Input into the process
Ideas for evolving the ECMAScript language are accepted in any form. Any discussion, idea, or proposal for a change or addition which has not been submitted as a formal proposal is considered to be a ‚Äústrawperson‚Äù (stage 0) and has no acceptance requirements. Such submissions must either come from TC39 delegates or from non-delegates who have registered via Ecma International.

Spec revisions and scheduling
TC39 intends to submit a specification to the ECMA General Assembly for ratification in July of each year. The following is an approximate timeline for producing a new spec revision:

February 1: Candidate Draft is produced.
February - March: 60 day royalty-free opt-out period.
March TC39 Meeting: stage 4 proposals are incorporated, final semantics are approved, and the new spec version is branched from main. Only editorial changes are accepted from this point forward.
April-June: ECMA CC and ECMA GA review period.
July: Approval of new standard by the ECMA General Assembly

Status of in-process additions
TC39 will maintain a list of in-process additions, along with the current maturity stage of each, on its GitHub.

Spec text
At stages 2 and later, the semantics, API and syntax of an addition must be described as edits to the latest published ECMAScript standard, using the same language and conventions. The quality of the spec text expected at each stage is described above.

Reviewers
Anyone can be a reviewer and submit feedback on an in-process addition. The committee should identify designated reviewers for acceptance during the ‚Äúdraft‚Äù (stage 2) maturity stage. These reviewers must give their sign-off before a proposal enters the ‚Äúcandidate‚Äù (stage 3) maturity stage. Designated reviewers should not be authors of the spec text for the addition and should have expertise applicable to the subject matter. Designated reviewers must be chosen by the committee, not by the proposal's champion.

When reviewers are designated, a target meeting for Stage 3 should be identified. Initial reviewer feedback should be given to the champions two weeks before that meeting to allow for a back-and-forth ahead of the meeting. The target Stage 3 meeting may be delayed by a champion outside of the meeting at a later time if it is not ready.

Calls for implementation and feedback
When an addition is accepted at stage 3, the committee is signifying that it believes design work is complete and further refinement will require implementation experience, significant usage, and external feedback.

Tips for achieving consensus
During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

Changes to the language are developed by way of a process which provides guidelines for evolving an addition from an idea to a fully specified feature, complete with acceptance tests and multiple implementations. There are six stages: a strawperson stage and five ‚Äúmaturity‚Äù stages. The TC39 committee must approve acceptance for each stage.

Proposals at stage 1 and beyond should be owned by the TC39 committee. Upon proposal acceptance, any externally-owned repositories should be transferred by following the onboarding instructions.


ECMAScript Proposal Stages


Stage
      Status
      Entrance Criteria
      Purpose
    


0
    This is a new proposal. It is not currently being considered by the committee.
    None. New proposals are assigned this stage by their authors outside of the usual advancement process.
    
      Ideation and exploration. Define a problem space in which the committee and the champions can focus their efforts.
      
Make the case for an improvement
        Describe the shape of some possible solutions
        Identify potential challenges
        Research how the problem is dealt with using available facilities today
        Research how the problem has been solved by other languages or in the library ecosystem
      


1
    This proposal is under consideration. The committee expects to devote time to examining the identified problem space, the full breadth of possible solutions, and cross-cutting concerns. 
    

Identified a champion or champion group who will advance the addition
        Prose outlining the problem or need and the general shape of a solution
        Discussion of key algorithms, abstractions and semantics
        Identification of potential cross-cutting concerns and implementation challenges/complexity
        A publicly available repository for the proposal that captures the above requirements
      


      Designing a solution.
      
Make the case for a particular solution or solution space
        Resolve any cross-cutting concerns
      



2
    The committee has chosen a preferred solution or solution space, but the design is a draft and may still change significantly. The committee expects the feature to be developed and eventually included in the standard, but due to reasons that may not be apparent at this stage, the feature may never be included in the standard.
    

Proposal document describes all high-level APIs and syntax
        Illustrative examples of usage
        Initial spec text including all major semantics, syntax, and APIs. Placeholders, TODOs, and editorial issues are acceptable
      


      Refining the solution.
      
Work out minor details such as ordering of observable effects, handling of invalid inputs, API names, etc.
        Receive and address spec text reviews from the assigned reviewers and the appropriate editor group
        Produce experimental implementations such as loosely-correct (not for production use) polyfills to aid in validating the design and exploring the details
        Investigate integration with relevant host APIs, if necessary
      



2.7
    The proposal is approved in principle and undergoing validation. The solution is complete and no further work is possible without feedback from tests, implementations, or usage. No changes to the proposal will be requested by the committee aside from those elicited through testing, implementation, or usage experience.
    

Complete spec text: all semantics, syntax, and APIs are completely described
        Assigned reviewers have signed off on the current spec text
        Relevant editor group has signed off on the current spec text
      

      Testing and validation.
      
Validate the design of the feature through the development of a rigorous and comprehensive test suite
        Develop spec-compliant prototypes to validate implementability, as necessary, or aid in test development
      



3
    The proposal has been recommended for implementation. No changes to the proposal are expected, but some necessary changes may still occur due to web incompatibilities or feedback from production-grade implementations.
    

The feature has sufficient testing and appropriate pre-implementation experience
      

Gaining implementation experience and discovering any web compatibility or integration issues.
  

4
    The proposed feature is complete and ready to be included in the standard. No further changes will be made to the proposal.
    

Two compatible implementations which pass the test262 acceptance tests
        Significant in-the-field experience with shipping implementations, such as that provided by two independent VMs
        A pull request has been sent to tc39/ecma262 or tc39/ecma402, as appropriate, with the integrated spec text
        The relevant editor group has signed off on the pull request
      

Integration into the draft specification and eventual inclusion in a yearly standard publication.
  

Input into the process
Ideas for evolving the ECMAScript language are accepted in any form. Any discussion, idea, or proposal for a change or addition which has not been submitted as a formal proposal is considered to be a ‚Äústrawperson‚Äù (stage 0) and has no acceptance requirements. Such submissions must either come from TC39 delegates or from non-delegates who have registered via Ecma International.

Spec revisions and scheduling
TC39 intends to submit a specification to the ECMA General Assembly for ratification in July of each year. The following is an approximate timeline for producing a new spec revision:

February 1: Candidate Draft is produced.
February - March: 60 day royalty-free opt-out period.
March TC39 Meeting: stage 4 proposals are incorporated, final semantics are approved, and the new spec version is branched from main. Only editorial changes are accepted from this point forward.
April-June: ECMA CC and ECMA GA review period.
July: Approval of new standard by the ECMA General Assembly

Status of in-process additions
TC39 will maintain a list of in-process additions, along with the current maturity stage of each, on its GitHub.

Spec text
At stages 2 and later, the semantics, API and syntax of an addition must be described as edits to the latest published ECMAScript standard, using the same language and conventions. The quality of the spec text expected at each stage is described above.

Reviewers
Anyone can be a reviewer and submit feedback on an in-process addition. The committee should identify designated reviewers for acceptance during the ‚Äúdraft‚Äù (stage 2) maturity stage. These reviewers must give their sign-off before a proposal enters the ‚Äúcandidate‚Äù (stage 3) maturity stage. Designated reviewers should not be authors of the spec text for the addition and should have expertise applicable to the subject matter. Designated reviewers must be chosen by the committee, not by the proposal's champion.

When reviewers are designated, a target meeting for Stage 3 should be identified. Initial reviewer feedback should be given to the champions two weeks before that meeting to allow for a back-and-forth ahead of the meeting. The target Stage 3 meeting may be delayed by a champion outside of the meeting at a later time if it is not ready.

Calls for implementation and feedback
When an addition is accepted at stage 3, the committee is signifying that it believes design work is complete and further refinement will require implementation experience, significant usage, and external feedback.

Tips for achieving consensus
During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

Proposals at stage 1 and beyond should be owned by the TC39 committee. Upon proposal acceptance, any externally-owned repositories should be transferred by following the onboarding instructions.


ECMAScript Proposal Stages


Stage
      Status
      Entrance Criteria
      Purpose
    


0
    This is a new proposal. It is not currently being considered by the committee.
    None. New proposals are assigned this stage by their authors outside of the usual advancement process.
    
      Ideation and exploration. Define a problem space in which the committee and the champions can focus their efforts.
      
Make the case for an improvement
        Describe the shape of some possible solutions
        Identify potential challenges
        Research how the problem is dealt with using available facilities today
        Research how the problem has been solved by other languages or in the library ecosystem
      


1
    This proposal is under consideration. The committee expects to devote time to examining the identified problem space, the full breadth of possible solutions, and cross-cutting concerns. 
    

Identified a champion or champion group who will advance the addition
        Prose outlining the problem or need and the general shape of a solution
        Discussion of key algorithms, abstractions and semantics
        Identification of potential cross-cutting concerns and implementation challenges/complexity
        A publicly available repository for the proposal that captures the above requirements
      


      Designing a solution.
      
Make the case for a particular solution or solution space
        Resolve any cross-cutting concerns
      



2
    The committee has chosen a preferred solution or solution space, but the design is a draft and may still change significantly. The committee expects the feature to be developed and eventually included in the standard, but due to reasons that may not be apparent at this stage, the feature may never be included in the standard.
    

Proposal document describes all high-level APIs and syntax
        Illustrative examples of usage
        Initial spec text including all major semantics, syntax, and APIs. Placeholders, TODOs, and editorial issues are acceptable
      


      Refining the solution.
      
Work out minor details such as ordering of observable effects, handling of invalid inputs, API names, etc.
        Receive and address spec text reviews from the assigned reviewers and the appropriate editor group
        Produce experimental implementations such as loosely-correct (not for production use) polyfills to aid in validating the design and exploring the details
        Investigate integration with relevant host APIs, if necessary
      



2.7
    The proposal is approved in principle and undergoing validation. The solution is complete and no further work is possible without feedback from tests, implementations, or usage. No changes to the proposal will be requested by the committee aside from those elicited through testing, implementation, or usage experience.
    

Complete spec text: all semantics, syntax, and APIs are completely described
        Assigned reviewers have signed off on the current spec text
        Relevant editor group has signed off on the current spec text
      

      Testing and validation.
      
Validate the design of the feature through the development of a rigorous and comprehensive test suite
        Develop spec-compliant prototypes to validate implementability, as necessary, or aid in test development
      



3
    The proposal has been recommended for implementation. No changes to the proposal are expected, but some necessary changes may still occur due to web incompatibilities or feedback from production-grade implementations.
    

The feature has sufficient testing and appropriate pre-implementation experience
      

Gaining implementation experience and discovering any web compatibility or integration issues.
  

4
    The proposed feature is complete and ready to be included in the standard. No further changes will be made to the proposal.
    

Two compatible implementations which pass the test262 acceptance tests
        Significant in-the-field experience with shipping implementations, such as that provided by two independent VMs
        A pull request has been sent to tc39/ecma262 or tc39/ecma402, as appropriate, with the integrated spec text
        The relevant editor group has signed off on the pull request
      

Integration into the draft specification and eventual inclusion in a yearly standard publication.
  

Input into the process
Ideas for evolving the ECMAScript language are accepted in any form. Any discussion, idea, or proposal for a change or addition which has not been submitted as a formal proposal is considered to be a ‚Äústrawperson‚Äù (stage 0) and has no acceptance requirements. Such submissions must either come from TC39 delegates or from non-delegates who have registered via Ecma International.

Spec revisions and scheduling
TC39 intends to submit a specification to the ECMA General Assembly for ratification in July of each year. The following is an approximate timeline for producing a new spec revision:

February 1: Candidate Draft is produced.
February - March: 60 day royalty-free opt-out period.
March TC39 Meeting: stage 4 proposals are incorporated, final semantics are approved, and the new spec version is branched from main. Only editorial changes are accepted from this point forward.
April-June: ECMA CC and ECMA GA review period.
July: Approval of new standard by the ECMA General Assembly

Status of in-process additions
TC39 will maintain a list of in-process additions, along with the current maturity stage of each, on its GitHub.

Spec text
At stages 2 and later, the semantics, API and syntax of an addition must be described as edits to the latest published ECMAScript standard, using the same language and conventions. The quality of the spec text expected at each stage is described above.

Reviewers
Anyone can be a reviewer and submit feedback on an in-process addition. The committee should identify designated reviewers for acceptance during the ‚Äúdraft‚Äù (stage 2) maturity stage. These reviewers must give their sign-off before a proposal enters the ‚Äúcandidate‚Äù (stage 3) maturity stage. Designated reviewers should not be authors of the spec text for the addition and should have expertise applicable to the subject matter. Designated reviewers must be chosen by the committee, not by the proposal's champion.

When reviewers are designated, a target meeting for Stage 3 should be identified. Initial reviewer feedback should be given to the champions two weeks before that meeting to allow for a back-and-forth ahead of the meeting. The target Stage 3 meeting may be delayed by a champion outside of the meeting at a later time if it is not ready.

Calls for implementation and feedback
When an addition is accepted at stage 3, the committee is signifying that it believes design work is complete and further refinement will require implementation experience, significant usage, and external feedback.

Tips for achieving consensus
During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

Ideas for evolving the ECMAScript language are accepted in any form. Any discussion, idea, or proposal for a change or addition which has not been submitted as a formal proposal is considered to be a ‚Äústrawperson‚Äù (stage 0) and has no acceptance requirements. Such submissions must either come from TC39 delegates or from non-delegates who have registered via Ecma International.

Spec revisions and scheduling
TC39 intends to submit a specification to the ECMA General Assembly for ratification in July of each year. The following is an approximate timeline for producing a new spec revision:

February 1: Candidate Draft is produced.
February - March: 60 day royalty-free opt-out period.
March TC39 Meeting: stage 4 proposals are incorporated, final semantics are approved, and the new spec version is branched from main. Only editorial changes are accepted from this point forward.
April-June: ECMA CC and ECMA GA review period.
July: Approval of new standard by the ECMA General Assembly

Status of in-process additions
TC39 will maintain a list of in-process additions, along with the current maturity stage of each, on its GitHub.

Spec text
At stages 2 and later, the semantics, API and syntax of an addition must be described as edits to the latest published ECMAScript standard, using the same language and conventions. The quality of the spec text expected at each stage is described above.

Reviewers
Anyone can be a reviewer and submit feedback on an in-process addition. The committee should identify designated reviewers for acceptance during the ‚Äúdraft‚Äù (stage 2) maturity stage. These reviewers must give their sign-off before a proposal enters the ‚Äúcandidate‚Äù (stage 3) maturity stage. Designated reviewers should not be authors of the spec text for the addition and should have expertise applicable to the subject matter. Designated reviewers must be chosen by the committee, not by the proposal's champion.

When reviewers are designated, a target meeting for Stage 3 should be identified. Initial reviewer feedback should be given to the champions two weeks before that meeting to allow for a back-and-forth ahead of the meeting. The target Stage 3 meeting may be delayed by a champion outside of the meeting at a later time if it is not ready.

Calls for implementation and feedback
When an addition is accepted at stage 3, the committee is signifying that it believes design work is complete and further refinement will require implementation experience, significant usage, and external feedback.

Tips for achieving consensus
During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

TC39 intends to submit a specification to the ECMA General Assembly for ratification in July of each year. The following is an approximate timeline for producing a new spec revision:
TC39 will maintain a list of in-process additions, along with the current maturity stage of each, on its GitHub.

Spec text
At stages 2 and later, the semantics, API and syntax of an addition must be described as edits to the latest published ECMAScript standard, using the same language and conventions. The quality of the spec text expected at each stage is described above.

Reviewers
Anyone can be a reviewer and submit feedback on an in-process addition. The committee should identify designated reviewers for acceptance during the ‚Äúdraft‚Äù (stage 2) maturity stage. These reviewers must give their sign-off before a proposal enters the ‚Äúcandidate‚Äù (stage 3) maturity stage. Designated reviewers should not be authors of the spec text for the addition and should have expertise applicable to the subject matter. Designated reviewers must be chosen by the committee, not by the proposal's champion.

When reviewers are designated, a target meeting for Stage 3 should be identified. Initial reviewer feedback should be given to the champions two weeks before that meeting to allow for a back-and-forth ahead of the meeting. The target Stage 3 meeting may be delayed by a champion outside of the meeting at a later time if it is not ready.

Calls for implementation and feedback
When an addition is accepted at stage 3, the committee is signifying that it believes design work is complete and further refinement will require implementation experience, significant usage, and external feedback.

Tips for achieving consensus
During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

At stages 2 and later, the semantics, API and syntax of an addition must be described as edits to the latest published ECMAScript standard, using the same language and conventions. The quality of the spec text expected at each stage is described above.

Reviewers
Anyone can be a reviewer and submit feedback on an in-process addition. The committee should identify designated reviewers for acceptance during the ‚Äúdraft‚Äù (stage 2) maturity stage. These reviewers must give their sign-off before a proposal enters the ‚Äúcandidate‚Äù (stage 3) maturity stage. Designated reviewers should not be authors of the spec text for the addition and should have expertise applicable to the subject matter. Designated reviewers must be chosen by the committee, not by the proposal's champion.

When reviewers are designated, a target meeting for Stage 3 should be identified. Initial reviewer feedback should be given to the champions two weeks before that meeting to allow for a back-and-forth ahead of the meeting. The target Stage 3 meeting may be delayed by a champion outside of the meeting at a later time if it is not ready.

Calls for implementation and feedback
When an addition is accepted at stage 3, the committee is signifying that it believes design work is complete and further refinement will require implementation experience, significant usage, and external feedback.

Tips for achieving consensus
During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

Anyone can be a reviewer and submit feedback on an in-process addition. The committee should identify designated reviewers for acceptance during the ‚Äúdraft‚Äù (stage 2) maturity stage. These reviewers must give their sign-off before a proposal enters the ‚Äúcandidate‚Äù (stage 3) maturity stage. Designated reviewers should not be authors of the spec text for the addition and should have expertise applicable to the subject matter. Designated reviewers must be chosen by the committee, not by the proposal's champion.

When reviewers are designated, a target meeting for Stage 3 should be identified. Initial reviewer feedback should be given to the champions two weeks before that meeting to allow for a back-and-forth ahead of the meeting. The target Stage 3 meeting may be delayed by a champion outside of the meeting at a later time if it is not ready.

Calls for implementation and feedback
When an addition is accepted at stage 3, the committee is signifying that it believes design work is complete and further refinement will require implementation experience, significant usage, and external feedback.

Tips for achieving consensus
During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

When reviewers are designated, a target meeting for Stage 3 should be identified. Initial reviewer feedback should be given to the champions two weeks before that meeting to allow for a back-and-forth ahead of the meeting. The target Stage 3 meeting may be delayed by a champion outside of the meeting at a later time if it is not ready.

Calls for implementation and feedback
When an addition is accepted at stage 3, the committee is signifying that it believes design work is complete and further refinement will require implementation experience, significant usage, and external feedback.

Tips for achieving consensus
During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

When an addition is accepted at stage 3, the committee is signifying that it believes design work is complete and further refinement will require implementation experience, significant usage, and external feedback.

Tips for achieving consensus
During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

During the discussion of a proposal any aspect may be discussed. Consensus is given as an indicator of the current stage. Delegates should openly give feedback on proposals, and especially for a proposal for stage advancement where the concern is relevant to the stage. Delegates should raise their concerns early and asynchronously, in order to help the champion resolve any issues.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

A delegate may pose a constraint as necessary for advancement. A constraint refers to an desired property of the proposal, accompanied by a rationale. We encourage this to also be done asynchronously in issues, and in incubator calls, as well as in plenary. In this situation, the delegate should expect to work with the champion and other delegates during earlier stages to incorporate their constraint into the solution, and to consider different possible tradeoffs. In general, the earlier a constraint is raised, the better.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

Frequently, many different conflicting constraints are posited about proposals, and the committee collectively may make tradeoffs selecting a particular design even though it compromises one or more constraints.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

Given that consensus on Stage 3 means "the solution is complete" (i.e., all open design issues have been resolved including anticipated implementation and ecosystem compatibility issues), all TC39 participants should validate the design of proposals they care about before granting Stage 3 consensus. Stage 3 proposals which have fulfilled the acceptance criteria for Stage 4 may not be withheld from advancement unless the issue raised is related to implementation experience or identifies a problem or information which has not previously been discussed by the committee. The intention is to allow implementers to invest in implementations, and maintain the significance of stage 3 in the process.

In cases where the committee does not come to consensus
The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

The committee may come to a point where consensus is not reached in committee regarding the feature. In this case, the committee must record a good description of why a proposal did not advance. This should be done both in the meeting notes and within an issue in the proposal's tracker, but not limited to those. This allows us to understand issues in the proposal and similar proposals in a coherent way.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

There are two forms that this sometimes takes, the first is the violation of a constraint and the other is colloquially known as a block. Other forms exist but are not discussed directly here.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

In the first case, delegates may consider that the violation of a constraint is sufficiently serious reason to withhold their consensus for stage advancement. The dissenting delegate(s) and the champion(s) should work together accordingly to resolve the issue.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

Not all issues with proposals are easily solvable. Some issues are too fundamental and serious, requiring a significant rework of the proposal, or may be unsolvable. In these situations, if consensus is withheld, it might be referred to colloquially as a "block". The proposal will require substantial work to address the concern, may need to be rethought all together, or may not have enough justification to pursue at this time.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

When possible, it is preferable to raise an actionable constraint. The committee does not have an established concept of a rejected proposal--it is always possible for the champion to make changes and come back to ask for consensus.

Conditional Advancement
A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

A delegate may also request additional time to consider the proposal, if a topic they had not considered comes up during discussion. In this case, the delegate should give the champion some actionable request for how to facilitate the analysis (e.g., the champion could walk through the proposal with the delegate offline). In practice, this work should be done during the plenary, or before the next meeting. A delegate may also request additional time to consider the proposal, if it was added to the agenda after the deadline for proposal advancement.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

The committee may resolve to conditionally advance a proposal to address a particular well-understood condition offline, e.g., making a particular small specification change concrete, among a group of interested people who have an idea of the solution. Conditional advancement is time-limited, giving the person raising the concern time to discuss with the champions and authors about their concerns. If a proposal has a conditional advancement, an issue must be opened on the proposal‚Äôs repository. If the issue is resolved, the proposal automatically reaches the next stage without further discussion by the committee. If the issue cannot be resolved, the proposal does not advance.

Withdrawing Proposals, Reverting to Earlier Stages, and Adopting Proposals
At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

At any point in the process, a proposal champion may propose that a proposal be downgraded to an earlier stage or withdrawn. Consensus of the committee is necessary for these transitions. The proposal to make this change must be accompanied by a reason why it is appropriate, e.g., a significant issue that may have not been considered, or identified, before.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

If the proposal champion is not available or no longer interested in a proposal, then another committee delegate may volunteer to champion the proposal. From that point on, this other delegate takes over champion duties, and can propose to advance, downgrade, or withdraw the proposal.

Scope of Responsibility for Champions
Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

Champions (or, frequently "champion groups" of several members) are authors and editors of proposals. The champion is responsible for the evolution of the proposal from Stage 0 through Stage 4, at which point maintenance transfers to the editor group. Champions have admin permissions in the proposal repository and can freely make changes within this repository. Periodically, champions may bring their proposal to TC39 to ask for consensus on stage advancement.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

When asking for advancement, the champion is expected to make the whole proposal accessible for review by the committee, by explaining its contents, providing supporting documentation, etc. Material changes should be presented explicitly.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

Although there is no requirement to do so, it is often beneficial for champions to keep the committee updated with periodic status updates explaining major changes. These status updates do not require consensus; consensus is only required for stage advancement. A significant design change may require that the committee has a chance to re-evaluate if the proposal is in the appropriate stage.


Test262 tests
During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

During stage 2.7, test262 tests should be authored and submitted via pull request. Once it has been appropriately reviewed, it should be merged to aid implementors in providing the feedback expected during this stage.

Eliding the process
The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

The committee may elide the process based on the scope of a change under consideration as it sees fit.

Role of the editors
In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.

In-process additions will likely have spec text which is authored by a champion or a committee member other than the editors although in some cases one or more of the editors may also be a champion with responsibility for specific features. The editors are responsible for the overall structure and coherence of the ECMAScript specification. It is also the role of the editors to provide guidance and feedback to spec text authors so that as an addition matures, the quality and completeness of its specification improves. It is also the role of the editors to integrate additions which have been accepted as ‚Äúfinished‚Äù (stage 4) into a new revision of the specification.





Java is a high-level, class-based, object-oriented programming language that is designed to have as few implementation dependencies as possible. It is a general-purpose programming language intended to let programmers write once, run anywhere (WORA),[16] meaning that compiled Java code can run on all platforms that support Java without the need to recompile.[17] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture. The syntax of Java is similar to C and C++, but has fewer low-level facilities than either of them. The Java runtime provides dynamic capabilities (such as reflection and runtime code modification) that are typically not available in traditional compiled languages. 

Java gained popularity shortly after its release, and has been a very popular programming language since then.[18] Java was the third most popular programming language in 2022[update] according to GitHub[19] and it is ranked fourth on TIOBE index as of January 2024[update].[20] Although still widely popular, there has been a gradual decline in use of Java in recent years with other languages using JVM gaining popularity.[21]

Java was originally developed by James Gosling at Sun Microsystems. It was released in May 1995 as a core component of Sun's Java platform. The original and reference implementation Java compilers, virtual machines, and class libraries were originally released by Sun under proprietary licenses. As of May 2007, in compliance with the specifications of the Java Community Process, Sun had relicensed most of its Java technologies under the GPL-2.0-only license. Oracle offers its own HotSpot Java Virtual Machine, however the official reference implementation is the OpenJDK JVM which is free open-source software and used by most developers and is the default JVM for almost all Linux distributions.

As of September 2023[update], Java 21 is the latest version, which is also a long-term support (LTS) version. Java 8, 11, and 17 are previous LTS versions still officially supported.

James Gosling, Mike Sheridan, and Patrick Naughton initiated the Java language project in June 1991.[22] Java was originally designed for interactive television, but it was too advanced for the digital cable television industry at the time.[23] The language was initially called Oak after an oak tree that stood outside Gosling's office. Later the project went by the name Green and was finally renamed Java, from Java coffee, a type of coffee from Indonesia.[24] Gosling designed Java with a C/C++-style syntax that system and application programmers would find familiar.[25]

Sun Microsystems released the first public implementation as Java 1.0 in 1996.[26] It promised write once, run anywhere (WORA) functionality, providing no-cost run-times on popular platforms. Fairly secure and featuring configurable security, it allowed network- and file-access restrictions. Major web browsers soon incorporated the ability to run Java applets within web pages, and Java quickly became popular. The Java 1.0 compiler was re-written in Java by Arthur van Hoff to comply strictly with the Java 1.0 language specification.[27] With the advent of Java 2 (released initially as J2SE 1.2 in December 1998 ‚Äì  1999), new versions had multiple configurations built for different types of platforms. J2EE included technologies and APIs for enterprise applications typically run in server environments, while J2ME featured APIs optimized for mobile applications. The desktop version was renamed J2SE. In 2006, for marketing purposes, Sun renamed new J2 versions as Java EE, Java ME, and Java SE, respectively.

In 1997, Sun Microsystems approached the ISO/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process.[28][29][30] Java remains a de facto standard, controlled through the Java Community Process.[31] At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.

On November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software (FOSS), under the terms of the GPL-2.0-only license. On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.[32]

Sun's vice-president Rich Green said that Sun's ideal role with regard to Java was as an evangelist.[33] Following Oracle Corporation's acquisition of Sun Microsystems in 2009‚Äì10, Oracle has described itself as the steward of Java technology with a relentless commitment to fostering a community of participation and transparency.[34] This did not prevent Oracle from filing a lawsuit against Google shortly after that for using Java inside the Android SDK (see the Android section).

On April 2, 2010, James Gosling resigned from Oracle.[35]

In January 2016, Oracle announced that Java run-time environments based on JDK 9 will discontinue the browser plugin.[36]

Java software runs on everything from laptops to data centers, game consoles to scientific supercomputers.[37]

Oracle (and others) highly recommend uninstalling outdated and unsupported versions of Java, due to unresolved security issues in older versions.[38]

There were five primary goals in the creation of the Java language:[17]

As of September 2023[update], Java 8, 11, 17 and 21 are supported as Long-Term Support (LTS) versions.[39]

Oracle released the last zero-cost public update for the legacy version Java 8 LTS in January 2019 for commercial use, although it will otherwise still support Java 8 with public updates for personal use indefinitely. Other vendors have begun to offer zero-cost builds of OpenJDK 18 and 8, 11 and 17 that are still receiving security and other upgrades.[citation needed]

Major release versions of Java, along with their release dates:

Sun has defined and supports four editions of Java targeting different application environments and segmented many of its APIs so that they belong to one of the platforms. The platforms are:

The classes in the Java APIs are organized into separate groups called packages. Each package contains a set of related interfaces, classes, subpackages and exceptions.

Sun also provided an edition called Personal Java that has been superseded by later, standards-based Java ME configuration-profile pairings.

One design goal of Java is portability, which means that programs written for the Java platform must run similarly on any combination of hardware and operating system with adequate run time support. This is achieved by compiling the Java language code to an intermediate representation called Java bytecode, instead of directly to architecture-specific machine code. Java bytecode instructions are analogous to machine code, but they are intended to be executed by a virtual machine (VM) written specifically for the host hardware. End-users commonly use a Java Runtime Environment (JRE) installed on their device for standalone Java applications or a web browser for Java applets.

Standard libraries provide a generic way to access host-specific features such as graphics, threading, and networking.

The use of universal bytecode makes porting simple. However, the overhead of interpreting bytecode into machine instructions made interpreted programs almost always run more slowly than native executables. Just-in-time (JIT) compilers that compile byte-codes to machine code during runtime were introduced from an early stage. Java's Hotspot compiler is actually two compilers in one; and with GraalVM (included in e.g. Java 11, but removed as of Java 16) allowing tiered compilation.[48] Java itself is platform-independent and is adapted to the particular platform it is to run on by a Java virtual machine (JVM), which translates the Java bytecode into the platform's machine language.[49]

Programs written in Java have a reputation for being slower and requiring more memory than those written in C++.[50][51] However, Java programs' execution speed improved significantly with the introduction of just-in-time compilation in 1997/1998 for Java 1.1,[52] the addition of language features supporting better code analysis (such as inner classes, the StringBuilder class, optional assertions, etc.), and optimizations in the Java virtual machine, such as HotSpot becoming Sun's default JVM in 2000. With Java 1.5, the performance was improved with the addition of the java.util.concurrent package, including lock-free implementations of the ConcurrentMaps and other multi-core collections, and it was improved further with Java 1.6.

Some platforms offer direct hardware support for Java; there are micro controllers that can run Java bytecode in hardware instead of a software Java virtual machine,[53] and some ARM-based processors could have hardware support for executing Java bytecode through their Jazelle option, though support has mostly been dropped in current implementations of ARM.

Java uses an automatic garbage collector to manage memory in the object lifecycle. The programmer determines when objects are created, and the Java runtime is responsible for recovering the memory once objects are no longer in use. Once no references to an object remain, the unreachable memory becomes eligible to be freed automatically by the garbage collector. Something similar to a memory leak may still occur if a programmer's code holds a reference to an object that is no longer needed, typically when objects that are no longer needed are stored in containers that are still in use.[54] If methods for a non-existent object are called, a null pointer exception is thrown.[55][56]

One of the ideas behind Java's automatic memory management model is that programmers can be spared the burden of having to perform manual memory management. In some languages, memory for the creation of objects is implicitly allocated on the stack or explicitly allocated and deallocated from the heap. In the latter case, the responsibility of managing memory resides with the programmer. If the program does not deallocate an object, a memory leak occurs.[54] If the program attempts to access or deallocate memory that has already been deallocated, the result is undefined and difficult to predict, and the program is likely to become unstable or crash. This can be partially remedied by the use of smart pointers, but these add overhead and complexity. Garbage collection does not prevent logical memory leaks, i.e. those where the memory is still referenced but never used.[54]

Garbage collection may happen at any time. Ideally, it will occur when a program is idle. It is guaranteed to be triggered if there is insufficient free memory on the heap to allocate a new object; this can cause a program to stall momentarily. Explicit memory management is not possible in Java.

Java does not support C/C++ style pointer arithmetic, where object addresses can be arithmetically manipulated (e.g. by adding or subtracting an offset). This allows the garbage collector to relocate referenced objects and ensures type safety and security.

As in C++ and some other object-oriented languages, variables of Java's primitive data types are either stored directly in fields (for objects) or on the stack (for methods) rather than on the heap, as is commonly true for non-primitive data types (but see escape analysis). This was a conscious decision by Java's designers for performance reasons.

Java contains multiple types of garbage collectors. Since Java 9, HotSpot uses the Garbage First Garbage Collector (G1GC) as the default.[57] However, there are also several other garbage collectors that can be used to manage the heap. For most applications in Java, G1GC is sufficient. Previously, the Parallel Garbage Collector was used in Java 8.

Having solved the memory management problem does not relieve the programmer of the burden of handling properly other kinds of resources, like network or database connections, file handles, etc., especially in the presence of exceptions.

The syntax of Java is largely influenced by C++ and C. Unlike C++, which combines the syntax for structured, generic, and object-oriented programming, Java was built almost exclusively as an object-oriented language.[17] All code is written inside classes, and every data item is an object, with the exception of the primitive data types, (i.e. integers, floating-point numbers, boolean values, and characters), which are not objects for performance reasons. Java reuses some popular aspects of C++ (such as the printf method).

Unlike C++, Java does not support operator overloading[58] or multiple inheritance for classes, though multiple inheritance is supported for interfaces.[59]

Java uses comments similar to those of C++. There are three different styles of comments: a single line style marked with two slashes (//), a multiple line style opened with /* and closed with */, and the Javadoc commenting style opened with /** and closed with */. The Javadoc style of commenting allows the user to run the Javadoc executable to create documentation for the program and can be read by some integrated development environments (IDEs) such as Eclipse to allow developers to access documentation within the IDE.

Java applets are programs embedded in other applications, typically in a Web page displayed in a web browser. The Java applet API is now deprecated since Java 9 in 2017.[60][61]

Java servlet technology provides Web developers with a simple, consistent mechanism for extending the functionality of a Web server and for accessing existing business systems. Servlets are server-side Java EE components that generate responses to requests from clients. Most of the time, this means generating HTML pages in response to HTTP requests, although there are a number of other standard servlet classes available, for example for WebSocket communication.

The Java servlet API has to some extent been superseded (but still used under the hood) by two standard Java technologies for web services:

Typical implementations of these APIs on Application Servers or Servlet Containers use a standard servlet for handling all interactions with the HTTP requests and responses that delegate to the web service methods for the actual business logic.

JavaServer Pages (JSP) are server-side Java EE components that generate responses, typically HTML pages, to HTTP requests from clients. JSPs embed Java code in an HTML page by using the special delimiters <% and %>. A JSP is compiled to a Java servlet, a Java application in its own right, the first time it is accessed. After that, the generated servlet creates the response.[62]

Swing is a graphical user interface library for the Java SE platform. It is possible to specify a different look and feel through the pluggable look and feel system of Swing. Clones of Windows, GTK+, and Motif are supplied by Sun. Apple also provides an Aqua look and feel for macOS. Where prior implementations of these looks and feels may have been considered lacking, Swing in Java SE 6 addresses this problem by using more native GUI widget drawing routines of the underlying platforms.[63]

JavaFX is a software platform for creating and delivering desktop applications, as well as rich web applications that can run across a wide variety of devices. JavaFX is intended to replace Swing as the standard GUI library for Java SE, but since JDK 11 JavaFX has not been in the core JDK and instead in a separate module.[64] JavaFX has support for desktop computers and web browsers on Microsoft Windows, Linux, and macOS. JavaFX does not have support for native OS look and feels.[65]

In 2004, generics were added to the Java language, as part of J2SE 5.0. Prior to the introduction of generics, each variable declaration had to be of a specific type. For container classes, for example, this is a problem because there is no easy way to create a container that accepts only specific types of objects. Either the container operates on all subtypes of a class or interface, usually Object, or a different container class has to be created for each contained class. Generics allow compile-time type checking without having to create many container classes, each containing almost identical code. In addition to enabling more efficient code, certain runtime exceptions are prevented from occurring, by issuing compile-time errors. If Java prevented all runtime type errors (ClassCastExceptions) from occurring, it would be type safe.

In 2016, the type system of Java was proven unsound in that it is possible to use generics to construct classes and methods that allow assignment of an instance one class to a variable of another unrelated class. Such code is accepted by the compiler, but fails at run time with a class cast exception.[66]

Criticisms directed at Java include the implementation of generics,[67] speed,[50] the handling of unsigned numbers,[68] the implementation of floating-point arithmetic,[69] and a history of security vulnerabilities in the primary Java VM implementation HotSpot.[70]

The Java Class Library is the standard library, developed to support application development in Java. It is controlled by Oracle in cooperation with others through the Java Community Process program.[71] Companies or individuals participating in this process can influence the design and development of the APIs. This process has been a subject of controversy during the 2010s.[72] The class library contains features such as:

Javadoc is a comprehensive documentation system, created by Sun Microsystems. It provides developers with an organized system for documenting their code. Javadoc comments have an extra asterisk at the beginning, i.e. the delimiters are /** and */, whereas the normal multi-line comments in Java are delimited by /* and */, and single-line comments start with //.[77]

Oracle Corporation owns the official implementation of the Java SE platform, due to its acquisition of Sun Microsystems on January 27, 2010. This implementation is based on the original implementation of Java by Sun. The Oracle implementation is available for Windows, macOS, Linux, and Solaris. Because Java lacks any formal standardization recognized by Ecma International, ISO/IEC, ANSI, or other third-party standards organizations, the Oracle implementation is the de facto standard.

The Oracle implementation is packaged into two different distributions: The Java Runtime Environment (JRE) which contains the parts of the Java SE platform required to run Java programs and is intended for end users, and the Java Development Kit (JDK), which is intended for software developers and includes development tools such as the Java compiler, Javadoc, Jar, and a debugger. Oracle has also released GraalVM, a high performance Java dynamic compiler and interpreter.

OpenJDK is another Java SE implementation that is licensed under the GNU GPL. The implementation started when Sun began releasing the Java source code under the GPL. As of Java SE 7, OpenJDK is the official Java reference implementation.

The goal of Java is to make all implementations of Java compatible. Historically, Sun's trademark license for usage of the Java brand insists that all implementations be compatible. This resulted in a legal dispute with Microsoft after Sun claimed that the Microsoft implementation did not support Java remote method invocation (RMI) or Java Native Interface (JNI) and had added platform-specific features of their own. Sun sued in 1997, and, in 2001, won a settlement of US$20 million, as well as a court order enforcing the terms of the license from Sun.[78] As a result, Microsoft no longer ships Java with Windows.

Platform-independent Java is essential to Java EE, and an even more rigorous validation is required to certify an implementation. This environment enables portable server-side applications.

The Java programming language requires the presence of a software platform in order for compiled programs to be executed.

Oracle supplies the Java platform for use with Java. The Android SDK is an alternative software platform, used primarily for developing Android applications with its own GUI system.

The Java language is a key pillar in Android, an open source mobile operating system. Although Android, built on the Linux kernel, is written largely in C, the Android SDK uses the Java language as the basis for Android applications but does not use any of its standard GUI, SE, ME or other established Java standards.[79] The bytecode language supported by the Android SDK is incompatible with Java bytecode and runs on its own virtual machine, optimized for low-memory devices such as smartphones and tablet computers. Depending on the Android version, the bytecode is either interpreted by the Dalvik virtual machine or compiled into native code by the Android Runtime.

Android does not provide the full Java SE standard library, although the Android SDK does include an independent implementation of a large subset of it. It supports Java 6 and some Java 7 features, offering an implementation compatible with the standard library (Apache Harmony).

The use of Java-related technology in Android led to a legal dispute between Oracle and Google. On May 7, 2012, a San Francisco jury found that if APIs could be copyrighted, then Google had infringed Oracle's copyrights by the use of Java in Android devices.[80] District Judge William Alsup ruled on May 31, 2012, that APIs cannot be copyrighted,[81] but this was reversed by the United States Court of Appeals for the Federal Circuit in May 2014.[82] On May 26, 2016, the district court decided in favor of Google, ruling the copyright infringement of the Java API in Android constitutes fair use.[83] In March 2018, this ruling was overturned by the Appeals Court, which sent down the case of determining the damages to federal court in San Francisco.[84]
Google filed a petition for writ of certiorari with the Supreme Court of the United States in January 2019 to challenge the two rulings that were made by the Appeals Court in Oracle's favor.[85] On April 5, 2021, the Court ruled 6‚Äì2 in Google's favor, that its use of Java APIs should be considered fair use. However, the court refused to rule on the copyrightability of APIs, choosing instead to determine their ruling by considering Java's API copyrightable "purely for argument's sake."[86]

Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
The JavaScript Guide shows you how to use JavaScript and gives an overview of the language. If you need exhaustive information about a language feature, have a look at the JavaScript reference.
This Guide is divided into the following chapters.
Overview: Introduction
Overview: Grammar and types
Overview: Control flow and error handling
Overview: Loops and iteration
Overview: Functions
Overview: Expressions and operators
Overview: Numbers and dates
Overview: Text formatting
Overview: Indexed collections
Overview: Keyed collections
Overview: Working with objects
Overview: Using classes
Overview: Promises
Overview: Typed arrays
Overview: Iterators and generators
Overview: Meta programming
Overview: JavaScript modules
This page was last modified on May 1, 2023 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
Welcome to our front-end web developer learning pathway!
Here we provide you with a structured course that will teach you all you need to know to become a front-end web developer. Work through each section, learning new skills (or improving existing ones) as you go along. Each section includes exercises and assessments to test your understanding before you move forward.
The subjects covered are:
You can work through sections in order, but each one is also self-contained. For example, if you already know HTML, you can skip ahead to the CSS section.
You don't need any previous knowledge to start this course. All you need is a computer that can run modern web browsers, an internet connection, and a willingness to learn.
If you are not sure if front-end web development is for you, and/or you want a gentle introduction before starting a longer and more complete course, work through our Getting started with the web module first.
We have tried to make learning front-end web development as comfortable as possible, but you will probably still get stuck because you don't understand something, or some code is just not working.
Don't panic. We all get stuck, whether we are beginner or professional web developers. The Learning and getting help article provides you with a series of tips for looking up information and helping yourself. If you are still stuck, feel free to post a question on our Discourse forums.
Let's get started. Good luck!
Time to complete: 1.5‚Äì2 hours
Nothing except basic computer literacy.
There are no assessments in this part of the course. But make sure you don't skip. It is important to get you set up and ready to do work for exercises later on in the course.
Time to complete: 35‚Äì50 hours
A basic web development environment.
The assessments in each module are designed to test your knowledge of the subject matter. Completing the assessments confirms that you are ready to move on to the next module.
Time to complete: 90‚Äì120 hours
It is recommended that you have basic HTML knowledge before starting to learn CSS. You should at least study Introduction to HTML first.
The assessments in each module are designed to test your knowledge of the subject matter. Completing the assessments confirms that you are ready to move on to the next module.
Time to complete: 135‚Äì185 hours
It is recommended that you have basic HTML knowledge before starting to learn JavaScript. You should at least study Introduction to HTML first.
The assessments in each module are designed to test your knowledge of the subject matter. Completing the assessments confirms that you are ready to move on to the next module.
Time to complete: 40‚Äì50 hours
Forms require HTML, CSS, and JavaScript knowledge. Given the complexity of working with forms, it is a dedicated topic.
The assessments in each module are designed to test your knowledge of the subject matter. Completing the assessments confirms that you are ready to move on to the next module.
Time to complete: 45‚Äì55 hours
It is good to know HTML, CSS, and JavaScript before working through this section. Many of the techniques and best practices touch on multiple technologies.
The assessments in each module are designed to test your knowledge of the subject matter. Completing the assessments confirms that you are ready to move on to the next module.
Time to complete: 55‚Äì90 hours
It is good to know HTML, CSS, and JavaScript before working through this section, as the tools discussed work alongside many of these technologies.
There are no specific assessment articles in this set of modules. The case study tutorials at the end of the second and third modules prepare you for grasping the essentials of modern tooling.
This page was last modified on Jan 1, 2024 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
JavaScript (JS) is a lightweight interpreted (or just-in-time compiled) programming language with first-class functions. While it is most well-known as the scripting language for Web pages, many non-browser environments also use it, such as Node.js, Apache CouchDB and Adobe Acrobat. JavaScript is a prototype-based, multi-paradigm, single-threaded, dynamic language, supporting object-oriented, imperative, and declarative (e.g. functional programming) styles.
JavaScript's dynamic capabilities include runtime object construction, variable parameter lists, function variables, dynamic script creation (via eval), object introspection (via for...in and Object utilities), and source-code recovery (JavaScript functions store their source text and can be retrieved through toString()).
This section is dedicated to the JavaScript language itself, and not the parts that are specific to Web pages or other host environments. For information about APIs that are specific to Web pages, please see Web APIs and DOM.
The standards for JavaScript are the ECMAScript Language Specification (ECMA-262) and the ECMAScript Internationalization API specification (ECMA-402). As soon as one browser implements a feature, we try to document it. This means that cases where some proposals for new ECMAScript features have already been implemented in browsers, documentation and examples in MDN articles may use some of those new features. Most of the time, this happens between the stages 3 and 4, and is usually before the spec is officially published.
Do not confuse JavaScript with the Java programming language ‚Äî JavaScript is not "Interpreted Java". Both "Java" and "JavaScript" are trademarks or registered trademarks of Oracle in the U.S. and other countries. However, the two programming languages have very different syntax, semantics, and use.
JavaScript documentation of core language features (pure ECMAScript, for the most part) includes the following:
For more information about JavaScript specifications and related technologies, see JavaScript technologies overview.
Looking to become a front-end web developer?

    We have put together a course that includes all the essential information you need to
    work towards your goal.
  
Get started
Learn how to program in JavaScript with guides and tutorials.
Head over to our Learning Area JavaScript topic if you want to learn JavaScript but have no previous experience with JavaScript or programming. The complete modules available there are as follows:
Answers some fundamental questions such as "what is JavaScript?", "what does it look like?", and "what can it do?", along with discussing key JavaScript features such as variables, strings, numbers, and arrays.
Continues our coverage of JavaScript's key fundamental features, turning our attention to commonly-encountered types of code blocks such as conditional statements, loops, functions, and events.
The object-oriented nature of JavaScript is important to understand if you want to go further with your knowledge of the language and write more efficient code, therefore we've provided this module to help you.
Discusses asynchronous JavaScript, why it is important, and how it can be used to effectively handle potential blocking operations such as fetching resources from a server.
Explores what APIs are, and how to use some of the most common APIs you'll come across often in your development work.
A much more detailed guide to the JavaScript language, aimed at those with previous programming experience either in JavaScript or another language.
JavaScript frameworks are an essential part of modern front-end web development, providing developers with proven tools for building scalable, interactive web applications. This module gives you some fundamental background knowledge about how client-side frameworks work and how they fit into your toolset, before moving on to a series of tutorials covering some of today's most popular ones.
An overview of the basic syntax and semantics of JavaScript for those coming from other programming languages to get up to speed.
Overview of available data structures in JavaScript.
JavaScript provides three different value comparison operations: strict equality using ===, loose equality using ==, and the Object.is() method.
How different methods that visit a group of object properties one-by-one handle the enumerability and ownership of properties.
A closure is the combination of a function and the lexical environment within which that function was declared.
Explanation of the widely misunderstood and underestimated prototype-based inheritance.
Memory life cycle and garbage collection in JavaScript.
JavaScript has a runtime model based on an "event loop".
Browse the complete JavaScript reference documentation.
Get to know standard built-in objects Array, Boolean, Date, Error, Function, JSON, Math, Number, Object, RegExp, String, Map, Set, WeakMap, WeakSet, and others.
Learn more about the behavior of JavaScript's operators instanceof, typeof, new, this, the operator precedence, and more.
Learn how do-while, for-in, for-of, try-catch, let, var, const, if-else, switch, and more JavaScript statements and keywords work.
Learn how to work with JavaScript's functions to develop your applications.
JavaScript classes are the most appropriate way to do object-oriented programming.
This page was last modified on Sep 25, 2023 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
In this module, we take a look at asynchronous JavaScript, why it is important, and how it can be used to effectively handle potential blocking operations, such as fetching resources from a server.

    We have put together a course that includes all the essential information you need to
    work towards your goal.
  
Get started
Asynchronous JavaScript is a fairly advanced topic, and you are advised to work through JavaScript first steps and JavaScript building blocks modules before attempting this.
Note: If you are working on a computer/tablet/other device where you don't have the ability to create your own files, you can try out (most of) the code examples in an online coding program such as JSBin or Glitch.
In this article, we'll learn about synchronous and asynchronous programming, why we often need to use asynchronous techniques, and the problems related to the way asynchronous functions have historically been implemented in JavaScript.
Here we'll introduce promises and show how to use promise-based APIs. We'll also introduce the async and await keywords.
This article will outline how to implement your own promise-based API.
Workers enable you to run certain tasks in a separate thread to keep your main code responsive. In this article, we'll rewrite a long-running synchronous function to use a worker.
The assessment asks you to use promises to play a set of animations in a particular sequence.
This page was last modified on Jul 3, 2023 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
JavaScript (JS) is a lightweight interpreted (or just-in-time compiled) programming language with first-class functions. While it is most well-known as the scripting language for Web pages, many non-browser environments also use it, such as Node.js, Apache CouchDB and Adobe Acrobat. JavaScript is a prototype-based, multi-paradigm, single-threaded, dynamic language, supporting object-oriented, imperative, and declarative (e.g. functional programming) styles.
JavaScript's dynamic capabilities include runtime object construction, variable parameter lists, function variables, dynamic script creation (via eval), object introspection (via for...in and Object utilities), and source-code recovery (JavaScript functions store their source text and can be retrieved through toString()).
This section is dedicated to the JavaScript language itself, and not the parts that are specific to Web pages or other host environments. For information about APIs that are specific to Web pages, please see Web APIs and DOM.
The standards for JavaScript are the ECMAScript Language Specification (ECMA-262) and the ECMAScript Internationalization API specification (ECMA-402). As soon as one browser implements a feature, we try to document it. This means that cases where some proposals for new ECMAScript features have already been implemented in browsers, documentation and examples in MDN articles may use some of those new features. Most of the time, this happens between the stages 3 and 4, and is usually before the spec is officially published.
Do not confuse JavaScript with the Java programming language ‚Äî JavaScript is not "Interpreted Java". Both "Java" and "JavaScript" are trademarks or registered trademarks of Oracle in the U.S. and other countries. However, the two programming languages have very different syntax, semantics, and use.
JavaScript documentation of core language features (pure ECMAScript, for the most part) includes the following:
For more information about JavaScript specifications and related technologies, see JavaScript technologies overview.
Looking to become a front-end web developer?

    We have put together a course that includes all the essential information you need to
    work towards your goal.
  
Get started
Learn how to program in JavaScript with guides and tutorials.
Head over to our Learning Area JavaScript topic if you want to learn JavaScript but have no previous experience with JavaScript or programming. The complete modules available there are as follows:
Answers some fundamental questions such as "what is JavaScript?", "what does it look like?", and "what can it do?", along with discussing key JavaScript features such as variables, strings, numbers, and arrays.
Continues our coverage of JavaScript's key fundamental features, turning our attention to commonly-encountered types of code blocks such as conditional statements, loops, functions, and events.
The object-oriented nature of JavaScript is important to understand if you want to go further with your knowledge of the language and write more efficient code, therefore we've provided this module to help you.
Discusses asynchronous JavaScript, why it is important, and how it can be used to effectively handle potential blocking operations such as fetching resources from a server.
Explores what APIs are, and how to use some of the most common APIs you'll come across often in your development work.
A much more detailed guide to the JavaScript language, aimed at those with previous programming experience either in JavaScript or another language.
JavaScript frameworks are an essential part of modern front-end web development, providing developers with proven tools for building scalable, interactive web applications. This module gives you some fundamental background knowledge about how client-side frameworks work and how they fit into your toolset, before moving on to a series of tutorials covering some of today's most popular ones.
An overview of the basic syntax and semantics of JavaScript for those coming from other programming languages to get up to speed.
Overview of available data structures in JavaScript.
JavaScript provides three different value comparison operations: strict equality using ===, loose equality using ==, and the Object.is() method.
How different methods that visit a group of object properties one-by-one handle the enumerability and ownership of properties.
A closure is the combination of a function and the lexical environment within which that function was declared.
Explanation of the widely misunderstood and underestimated prototype-based inheritance.
Memory life cycle and garbage collection in JavaScript.
JavaScript has a runtime model based on an "event loop".
Browse the complete JavaScript reference documentation.
Get to know standard built-in objects Array, Boolean, Date, Error, Function, JSON, Math, Number, Object, RegExp, String, Map, Set, WeakMap, WeakSet, and others.
Learn more about the behavior of JavaScript's operators instanceof, typeof, new, this, the operator precedence, and more.
Learn how do-while, for-in, for-of, try-catch, let, var, const, if-else, switch, and more JavaScript statements and keywords work.
Learn how to work with JavaScript's functions to develop your applications.
JavaScript classes are the most appropriate way to do object-oriented programming.
This page was last modified on Sep 25, 2023 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
JavaScript (JS) is a lightweight interpreted (or just-in-time compiled) programming language with first-class functions. While it is most well-known as the scripting language for Web pages, many non-browser environments also use it, such as Node.js, Apache CouchDB and Adobe Acrobat. JavaScript is a prototype-based, multi-paradigm, single-threaded, dynamic language, supporting object-oriented, imperative, and declarative (e.g. functional programming) styles.
JavaScript's dynamic capabilities include runtime object construction, variable parameter lists, function variables, dynamic script creation (via eval), object introspection (via for...in and Object utilities), and source-code recovery (JavaScript functions store their source text and can be retrieved through toString()).
This section is dedicated to the JavaScript language itself, and not the parts that are specific to Web pages or other host environments. For information about APIs that are specific to Web pages, please see Web APIs and DOM.
The standards for JavaScript are the ECMAScript Language Specification (ECMA-262) and the ECMAScript Internationalization API specification (ECMA-402). As soon as one browser implements a feature, we try to document it. This means that cases where some proposals for new ECMAScript features have already been implemented in browsers, documentation and examples in MDN articles may use some of those new features. Most of the time, this happens between the stages 3 and 4, and is usually before the spec is officially published.
Do not confuse JavaScript with the Java programming language ‚Äî JavaScript is not "Interpreted Java". Both "Java" and "JavaScript" are trademarks or registered trademarks of Oracle in the U.S. and other countries. However, the two programming languages have very different syntax, semantics, and use.
JavaScript documentation of core language features (pure ECMAScript, for the most part) includes the following:
For more information about JavaScript specifications and related technologies, see JavaScript technologies overview.
Looking to become a front-end web developer?

    We have put together a course that includes all the essential information you need to
    work towards your goal.
  
Get started
Learn how to program in JavaScript with guides and tutorials.
Head over to our Learning Area JavaScript topic if you want to learn JavaScript but have no previous experience with JavaScript or programming. The complete modules available there are as follows:
Answers some fundamental questions such as "what is JavaScript?", "what does it look like?", and "what can it do?", along with discussing key JavaScript features such as variables, strings, numbers, and arrays.
Continues our coverage of JavaScript's key fundamental features, turning our attention to commonly-encountered types of code blocks such as conditional statements, loops, functions, and events.
The object-oriented nature of JavaScript is important to understand if you want to go further with your knowledge of the language and write more efficient code, therefore we've provided this module to help you.
Discusses asynchronous JavaScript, why it is important, and how it can be used to effectively handle potential blocking operations such as fetching resources from a server.
Explores what APIs are, and how to use some of the most common APIs you'll come across often in your development work.
A much more detailed guide to the JavaScript language, aimed at those with previous programming experience either in JavaScript or another language.
JavaScript frameworks are an essential part of modern front-end web development, providing developers with proven tools for building scalable, interactive web applications. This module gives you some fundamental background knowledge about how client-side frameworks work and how they fit into your toolset, before moving on to a series of tutorials covering some of today's most popular ones.
An overview of the basic syntax and semantics of JavaScript for those coming from other programming languages to get up to speed.
Overview of available data structures in JavaScript.
JavaScript provides three different value comparison operations: strict equality using ===, loose equality using ==, and the Object.is() method.
How different methods that visit a group of object properties one-by-one handle the enumerability and ownership of properties.
A closure is the combination of a function and the lexical environment within which that function was declared.
Explanation of the widely misunderstood and underestimated prototype-based inheritance.
Memory life cycle and garbage collection in JavaScript.
JavaScript has a runtime model based on an "event loop".
Browse the complete JavaScript reference documentation.
Get to know standard built-in objects Array, Boolean, Date, Error, Function, JSON, Math, Number, Object, RegExp, String, Map, Set, WeakMap, WeakSet, and others.
Learn more about the behavior of JavaScript's operators instanceof, typeof, new, this, the operator precedence, and more.
Learn how do-while, for-in, for-of, try-catch, let, var, const, if-else, switch, and more JavaScript statements and keywords work.
Learn how to work with JavaScript's functions to develop your applications.
JavaScript classes are the most appropriate way to do object-oriented programming.
This page was last modified on Sep 25, 2023 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
The Object.is() static method determines whether two values are the same value.
The first value to compare.
The second value to compare.
A boolean indicating whether or not the two arguments are the same value.
Object.is() determines whether two values are the same value. Two values are the same if one of the following holds:
Object.is() is not equivalent to the == operator. The == operator applies various coercions to both sides (if they are not the same type) before testing for equality (resulting in such behavior as "" == false being true), but Object.is() doesn't coerce either value.
Object.is() is also not equivalent to the === operator. The only difference between Object.is() and === is in their treatment of signed zeros and NaN values. The === operator (and the == operator) treats the number values -0 and +0 as equal, but treats NaN as not equal to each other.
BCD tables only load in the browser with JavaScript enabled. Enable JavaScript to view data.
This page was last modified on Aug 21, 2023 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
JavaScript (JS) is a lightweight interpreted (or just-in-time compiled) programming language with first-class functions. While it is most well-known as the scripting language for Web pages, many non-browser environments also use it, such as Node.js, Apache CouchDB and Adobe Acrobat. JavaScript is a prototype-based, multi-paradigm, single-threaded, dynamic language, supporting object-oriented, imperative, and declarative (e.g. functional programming) styles.
JavaScript's dynamic capabilities include runtime object construction, variable parameter lists, function variables, dynamic script creation (via eval), object introspection (via for...in and Object utilities), and source-code recovery (JavaScript functions store their source text and can be retrieved through toString()).
This section is dedicated to the JavaScript language itself, and not the parts that are specific to Web pages or other host environments. For information about APIs that are specific to Web pages, please see Web APIs and DOM.
The standards for JavaScript are the ECMAScript Language Specification (ECMA-262) and the ECMAScript Internationalization API specification (ECMA-402). As soon as one browser implements a feature, we try to document it. This means that cases where some proposals for new ECMAScript features have already been implemented in browsers, documentation and examples in MDN articles may use some of those new features. Most of the time, this happens between the stages 3 and 4, and is usually before the spec is officially published.
Do not confuse JavaScript with the Java programming language ‚Äî JavaScript is not "Interpreted Java". Both "Java" and "JavaScript" are trademarks or registered trademarks of Oracle in the U.S. and other countries. However, the two programming languages have very different syntax, semantics, and use.
JavaScript documentation of core language features (pure ECMAScript, for the most part) includes the following:
For more information about JavaScript specifications and related technologies, see JavaScript technologies overview.
Looking to become a front-end web developer?

    We have put together a course that includes all the essential information you need to
    work towards your goal.
  
Get started
Learn how to program in JavaScript with guides and tutorials.
Head over to our Learning Area JavaScript topic if you want to learn JavaScript but have no previous experience with JavaScript or programming. The complete modules available there are as follows:
Answers some fundamental questions such as "what is JavaScript?", "what does it look like?", and "what can it do?", along with discussing key JavaScript features such as variables, strings, numbers, and arrays.
Continues our coverage of JavaScript's key fundamental features, turning our attention to commonly-encountered types of code blocks such as conditional statements, loops, functions, and events.
The object-oriented nature of JavaScript is important to understand if you want to go further with your knowledge of the language and write more efficient code, therefore we've provided this module to help you.
Discusses asynchronous JavaScript, why it is important, and how it can be used to effectively handle potential blocking operations such as fetching resources from a server.
Explores what APIs are, and how to use some of the most common APIs you'll come across often in your development work.
A much more detailed guide to the JavaScript language, aimed at those with previous programming experience either in JavaScript or another language.
JavaScript frameworks are an essential part of modern front-end web development, providing developers with proven tools for building scalable, interactive web applications. This module gives you some fundamental background knowledge about how client-side frameworks work and how they fit into your toolset, before moving on to a series of tutorials covering some of today's most popular ones.
An overview of the basic syntax and semantics of JavaScript for those coming from other programming languages to get up to speed.
Overview of available data structures in JavaScript.
JavaScript provides three different value comparison operations: strict equality using ===, loose equality using ==, and the Object.is() method.
How different methods that visit a group of object properties one-by-one handle the enumerability and ownership of properties.
A closure is the combination of a function and the lexical environment within which that function was declared.
Explanation of the widely misunderstood and underestimated prototype-based inheritance.
Memory life cycle and garbage collection in JavaScript.
JavaScript has a runtime model based on an "event loop".
Browse the complete JavaScript reference documentation.
Get to know standard built-in objects Array, Boolean, Date, Error, Function, JSON, Math, Number, Object, RegExp, String, Map, Set, WeakMap, WeakSet, and others.
Learn more about the behavior of JavaScript's operators instanceof, typeof, new, this, the operator precedence, and more.
Learn how do-while, for-in, for-of, try-catch, let, var, const, if-else, switch, and more JavaScript statements and keywords work.
Learn how to work with JavaScript's functions to develop your applications.
JavaScript classes are the most appropriate way to do object-oriented programming.
This page was last modified on Sep 25, 2023 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.

 
Sign in with a passkey




            New to GitHub?
              Create an account


 
Sign in with a passkey




            New to GitHub?
              Create an account

Thanks for taking the time to contribute to MDN Web Docs! √∞≈∏≈Ω‚Ä∞
This document covers project setup steps along with a set of guidelines for contributing to MDN Web Docs content.\nEveryone participating in this project is expected to follow our Code of Conduct.\nIf you want to jump right in, see Getting started with MDN Web Docs for an overview of how to join, and the Contribute page on MDN for a filtered list of tasks.
Before contributing, make sure you're familiar with the project guidelines and conventions:
We expect contributors to MDN to have some knowledge of web technologies before working on content.\nWe've put together relevant resources to get up to speed on specific topics before contributing:
There are a few things to keep in mind about content on MDN and how it is maintained:
Each document's index.md starts with front-matter, which is written in YAML.\nThe YAML is read by the MDN build system and is used to read the metadata of a document.
The front-matter must be the first thing in the file and must take the form of valid YAML set between triple-dashed lines (---).\nFront-matter defines the document's title and slug, and may also include status, browser-compat and specification information.\nHere's an example of front-matter from the JavaScript page:
You'll need a GitHub account to contribute to MDN Web Docs.\nIf you are comfortable working with git and GitHub, you can skip ahead to Contributing to MDN.\nIf you've created a new GitHub account and want to know what to do next, you can choose one of the following ways to manage changes:
If you want to make a small change like fixing a typo, the GitHub UI is the easiest way to get started.\nIf you've found a typo on the JavaScript landing page, for example, you can propose a fix as follows:
From there, the GitHub UI will walk you through the rest by creating a fork and a branch to commit your changes to.\nAfter you have made changes to your branch, the goal is to open a pull request for your changes to be incorporated.
A pull request represents the work you want to be reviewed, approved, and merged into the main branch of the MDN repository.\nSee the Creating a pull request for more details on creating and handling pull requests successfully.
If you're not certain of the changes that you want to make, get in touch with us!
Note: You can click the View the source on GitHub link at the bottom of an MDN page to jump directly to the page source on GitHub.
If you want to make changes to more than one file, the GitHub UI is not very efficient because you have to make separate pull requests for each file you want to change.\nInstead of using the GitHub UI, you need to use git or a client like the GitHub Desktop or GitHub CLI.\nThe following examples are using plain git commands, but you can use any of the clients mentioned above to perform the equivalent actions.
To fork and clone the repository:
Create a fork of the mdn/content repository to freely experiment with branches and changes.\nAssuming your GitHub username is octocat, your fork would be a copy of the mdn/content repository in your account at https://github.com/octocat/content.
Clone your fork to your local machine.\nAssuming your GitHub username is octocat, you would do something like this:
Create a remote to keep your clone and fork (https://github.com/octocat/content) up-to-date.\nThis example adds a remote named upstream, but you can name it mdn or any other name you like.
When you run git remote -v, you'll see that you have two remotes: upstream and origin.\nThe origin remote is your fork (https://github.com/octocat/content) and the upstream remote is the mdn/content repository.
Keep your fork up-to-date often.\nYou can do this by fetching the latest changes from the mdn/content repository and merging them into your fork.
Create a branch for your changes.\nThis example creates a branch named fix-typo:
The previous sections describe how to get started using the GitHub UI to make small changes to a single file and how to create a fork and clone the repository to prepare for making larger changes.\nThis section describes how to build the project locally and how to prepare your changes for submission.
To serve the site locally, you need to have Node.js and Yarn 1 (Classic) installed.\nYou can check if these are installed by running the following commands:
After you have installed Node.js and Yarn, you can install the dependencies using yarn:
After you have installed all dependencies, you can start the local preview using yarn start:
Once started, a live preview is available at http://localhost:5042/
Set your preferred editor by adding EDITOR=... into a .env file in the project root.\nTo specify VS Code as your preferred editor, for example, use the following command:
You can set the EDITOR environment variable to any editor you like.\nWhen browsing a page server locally, you can press Open in your editor to edit the current file in your preferred editor.
To edit files and track your changes, you should use feature branches.\nFeature branches are created from the main branch and should be named after the feature you're working on.\nThis will make it easier to submit a pull request for your changes.
Note: Open a discussion if your changes will contain large, complex or structural changes. Ask for feedback before embarking on large tasks.
When the server is running, make the changes you would like to make to one or more index.md files.
Open a browser and navigate to the equivalent pages you've changed.\nIf you changed files/en-us/web/javascript/index.md, you would navigate to http://localhost:5042/en-us/docs/web/javascript in your browser, for example.
Check for errors by clicking Show flaws on each previewed page.\nYou may be able to fix flaws by running:
Commit your changes to the branch (our example is using the fix-typo branch) and push the changes to your fork's remote:
To ensure that all MDN documents follow the same formatting, we use both Prettier and Markdownlint to format and lint Markdown files. This helps us enforce uniform styling across all documents with minimal reviewer intervention.
If you have a local checkout of the repository and have installed the dependencies, or you are using github.dev, a pre-commit hook will be installed which automatically runs while making a commit. To save some headache and improve your work flow while authoring, you may wish to configure your editor to automatically run Prettier. Alternatively, you may run yarn fix:md in the command line to manually format all Markdown files.
Note: Automatically formatting changes does not work for pull requests opened using the GitHub Web UI as described in the \"Simple changes\" section. This may result in failed status checks on pull requests. If you're not sure about how to fix this, get in touch with us! for help.
Adding a new document is relatively straightforward, especially if you can start by copying the index.md of a similar document.\nThere are a few things to keep in mind:
Moving one or more documents (or an entire tree of documents) is made easier with the yarn content move command.\nThis command moves the file and fixes up redirects automatically. You can use this command as shown below:
Warning\nDon't edit the _redirects.txt file manually.\nSee the Redirecting a document section for more information.
To use yarn content move, provide the slug of the document you'd like to move (e.g., Learn/Accessibility), and the slug of its new location (e.g., Learn/A11y).\nThe locale of the existing document can be provided as an optional third argument (this defaults to en-US).\nIf the document you'd like to move contains child documents (i.e. it represents a document tree), the yarn content move command will move the entire tree.
Let's say you want to move the entire /en-US/Learn/Accessibility tree to /en-US/Learn/A11y, you can do so as follows:
Starting from a fresh branch:
Move files with yarn content move.\nThis will delete and modify existing files, as well as create new files.
Once files are moved we need to update references to those files in the other content files as well. Use following command to update all the references automatically in one go:
Commit all the changes and push your branch to the remote:
Similar to moving files, you can delete documents or a tree of documents easily by using the yarn content delete command.
Warning:\nDon't delete files or directories from the repository manually; the yarn content delete command handles the necessary changes such as updating the _wikihistory.json file.
You can use this command as shown below:
To use yarn content delete, provide the slug of the document you'd like to delete (e.g., Learn/Accessibility), and the locale as an optional second argument (this defaults to en-US).\nIf the slug of the page you wish to delete contains special characters, include it in quotes. For example:
If the document has child documents (i.e., the document represents a document tree), you must specify the -r, --recursive option, else the command will fail.\nSay you want to delete the entire /en-US/Learn/Accessibility tree and redirect all the deleted documents to Web/Accessibility. You can perform the following steps:
Start from a fresh branch.
Run the yarn content delete command and redirect all deleted documents.
Warning:\nYou should always add a redirect when deleting documents. If there is no obvious alternative, redirect to the nearest \"parent\" of the deleted topic.\nIf you forget to redirect when deleting a file, you can do it afterwards. See the Redirecting a document section.
Commit all of the changes and push your branch to the remote.
If you are moving a document as shown above you don't need to create a redirect.\nHowever, you may need to do so when fixing a broken link or after deleting a document without the --redirect flag.
You may do this by using the yarn content add-redirect command.
Start a fresh branch to work in:
Add a redirect with yarn content add-redirect. The target page can be a page on MDN or an external URL:
Commit all of the changed files and pushing your branch to your fork:
Once you've made your changes and pushed them to a branch on your fork, you can create a pull request to propose your changes to the mdn/content repository.\nSomeone from the MDN team or the MDN Web Docs community will review your changes and provide feedback.
For details on what to do next, see the pull request etiquette section to see how to handle pull requests and get your content merged successfully.
This is the exciting part of contributing to MDN as you're almost done with the contribution process!\nHere are some things to keep in mind at this point:
During reviews, you may be asked to answer questions about your work or to make changes to your suggested edits.\nThis is a common part of the process of making changes in open source projects.\nThere are some important rules of etiquette to remember that will help during the review stage.
Check tests that are run automatically for pull requests (see .github/workflows).\nIf one or more of these tests fail, you must fix them.\nYour pull request will not be approved and merged if there are failing tests.\nIf you don't know how to resolve the underlying issue(s), you can ask for help.
Resolve conflicts if your pull request has merge conflicts with the main branch.\nThis is usually done by merging the main branch into your feature branch (git pull upstream main), and then pushing the updated branch to your fork (git push).
Group logical changes in each pull request so that it contains a single change or a related set of changes.\nIf a pull request becomes too large or contains too many unrelated changes, a reviewer may close your pull request and ask you to submit a new pull request for each set of changes.
Don't re-open pull requests closed by a reviewer.
Don't use git rebase of main over your branch.\nYour changes are replayed on top of the current main branch at that point in time.\nThis might confuse reviewers as notifications on GitHub lead to nowhere.
When contributing to the content you agree to license your contributions\naccording to our license.
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
Documenting web technologies, including CSS, HTML, and JavaScript, since 2005.
See the latest updates to the MDN reference pages about JavaScript regular expressions, including new sections on sub-features and browser compatibility information.
Learn what HTML landmark roles are, how they improve accessibility, and how you can include them on your website effectively.
The Performance API is a group of standards used to measure the performance of web applications.
The CSS nesting module defines a syntax for nesting selectors, providing the ability to nest one style rule inside another, with the selector of the child rule relative to the selector of the parent rule.
Responsibly empowering developers with AI on MDNblog.mozilla.org
Introducing AI Help: Your Trusted Companion for Web Developmentdeveloper.mozilla.org
Introducing the MDN Playground: Bring your code to life!developer.mozilla.org
Update link to canvas size test resultsmdn/content
fix: scalegrid linkmdn/content
Added missing space to the accessibility pagemdn/content
es: Fix sourceCommit to Criteria_for_inclusionmdn/translated-content
[es] add page `MDN/Writing_guidelines/Page_structures/Page_types/CSS_property_page_template` mdn/translated-content
[es] add page `MDN/Writing_guidelines/What_we_write/Criteria_for_inclusion`mdn/translated-content
chore(ci/lint-fix): setup node only oncemdn/translated-content
fix(ci/interfacedata-updater): remove cache functionmdn/content
glossary update till alpnmdn/translated-content
Updated Date.parse()mdn/content
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.

Failed to fetch https://twitter.com/mozdevnet: 400 Client Error: Bad Request for url: https://twitter.com/mozdevnet
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          

MDN Web Docs is an open-source, collaborative project that documents web platform technologies, including CSS, HTML, JavaScript, and Web APIs. We also provide extensive üßë‚Äçüéì learning resources for beginning developers and students.
Note: By participating in and contributing to our projects and discussions, you acknowledge that you have read and agree to the Mozilla community participation guidelines.
MDN's mission is to provide a blueprint for a better internet and empower a new generation of developers and content creators to build it.
The power of MDN Web Docs lies in its vast community of active readers and contributors. Since 2005, approximately 45,000 contributors have created the documentation we know and love. Together, contributors have created over 45,000 documents that make up an up-to-date, comprehensive, and free resource for web developers worldwide. In addition to English-language articles, over 35 volunteers lead translation and localization efforts for Chinese, French, Japanese, Korean, Portuguese, Russian, and Spanish.
You can be part of MDN Web Docs, whether it be through ‚úçÔ∏è content contributions, ‚öôÔ∏è engineering, or ‚ÜîÔ∏è translation work. The MDN Web Docs project welcomes contributions from everyone who shares our goals and wants to contribute constructively and respectfully within our community. üßò‚Äç‚ôÇÔ∏è
We're proud to have a global community of contributors and developers, who also want to say üëã
You can ask questions or get in touch with the MDN Web Docs team and community through any of our communication channels.

        This repository contains compatibility data for Web technologies as displayed on MDN
      



JSON





            4.7k
          




            1.9k
          


        The content behind MDN Web Docs
      



Markdown





            8.5k
          




            22.1k
          


        Home of the MDN live code editor interactive examples
      



HTML





            688
          




            521
          


        The source repository of all translated content for MDN Web Docs
      



Markdown





            1.5k
          




            7.8k
          


        The platform code behind MDN Web Docs
      



TypeScript





            1.1k
          




            475
          


        A place to provide feedback and suggestions for MDN Web Docs
      





            114
          




            44
          


            The content behind MDN Web Docs


            The source repository of all translated content for MDN Web Docs


            This repository contains compatibility data for Web technologies as displayed on MDN


            Code examples that accompany the MDN Web Docs pages relating to Web Audio.


            The clean kuma (ü§ñüßπ)


            Builder of Bits aka The MDN Web Docs interactive examples, example builder


            Sample todo app built with the React/ReactDOM framework.


            The platform code behind MDN Web Docs


            GitHub repo for the MDN Learning Area. 


            Local Library website written in NodeJS/Express; example for the MDN server-side development NodeJS module: https://developer.mozilla.org/en-US/docs/Learn/Server-side/Express_Nodejs.

/usr/lib/python3.10/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features="xml"` into the BeautifulSoup constructor.
  k = self.parse_starttag(i)
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
MDN Web Docs is an open-source, collaborative project documenting Web platform technologies, including CSS, HTML, JavaScript, and Web APIs. We also provide an extensive set of learning resources for beginning developers and students.
We're always striving to connect developers more seamlessly with the tools and information they need to easily build projects on the open Web. Since our beginnings in 2005, Mozilla and the community have amassed around 45,000 pages of free, open-source content.
This guiding principle has made MDN Web Docs the go-to repository of independent information for developers, regardless of brand, browser or platform. We are an open community of devs, writers, and other technologists building resources for a better Web, with over 17 million monthly MDN users from all over the world. Anyone can contribute, and each of the 45,000 individuals who have done so over the past decades has strengthened and improved the resource. We also receive content contributions from our partners, including Microsoft, Google, Samsung, Igalia, W3C and others. Together we continue to drive innovation on the Web and serve the common good.
Through our GitHub documentation repository, contributors can make changes, submit pull requests, have their contributions reviewed and then merged with existing content. Through this workflow, we welcome the vast knowledge and experience of our developer community while maintaining a high level of quality, accurate content.
Our constant quest for innovation starts here, with you. Every part of MDN (docs, demos and the site itself) springs from our incredible open community of developers. Please join us!Get Involved ‚Üí
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
Firefox is no longer supported on Windows 8.1 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.


        Download Firefox ESR 64-bit
      



        Download Firefox ESR 32-bit
      

Firefox is no longer supported on macOS 10.14 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.
Get the not-for-profit-backed browser on Windows, Mac or Linux.
Get the customizable mobile browser for Android smartphones.
Get the mobile browser for your iPhone or iPad.
Simply private mobile browsing.
Learn how Firefox treats your data with respect.
Read about new Firefox features and ways to stay safe online.
Get the details on the latest Firefox updates.
View all Firefox Browsers
See if your email has appeared in a company‚Äôs data breach.
Help prevent Facebook from collecting your data outside their site.
Save and discover the best stories from across the web.
Get protection beyond your browser, on all your devices.
Learn how each Firefox product protects and respects your data.
Sign up for new accounts without handing over your email address.
New features and tools for a customized MDN experience
View all Products
Learn about the values and principles that guide our mission.
Meet the not-for-profit behind Firefox that stands for a better web.
Join the fight for a healthy internet.
Stories about how our people and products are changing the world for the better.
Meet the team that‚Äôs building technology for a better internet.
Work for a mission-driven organization that makes people-first products.
Learn about Mozilla and the issues that matter to us.

More About Mozilla

Gather in this interactive, online, multi-dimensional social space.
Get the Firefox browser built just for developers.
Check out the home for web developer resources.
Donate your voice so the future of the web can hear everyone.
Discover ways to bring bright ideas to life.
Bring your drive, your creativity, your big ideas and your new perspectives to make the difference we‚Äôre aiming for.
No jobs found that match the selected filters.
Subscribe to our open positions RSS feed

          
          
          Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.
          Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
        
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus

Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.


                Get support from our contributors or staff members.
              

                Dig into the knowledge base, tips and tricks, troubleshooting, and so much more.
              

                Find help for common issues and learn about the benefits a Mozilla account provides.
              

Popular Searches:
Update Firefox
Profiles
Firefox Sync

Explore the knowledge base.

          
            We‚Äôre here for you! If you haven‚Äôt found a solution after exploring our help articles, you can get in touch with our support team.
          
        

          MDN is displayed in light mode by default. You can enable dark mode and switch back to light mode at any time on any page.
        

          MDN offline allows you to browse MDN without an internet connection. It can also enable you to have a faster experience while saving data.
        

          MDN Plus is available in 26 countries. Check this article to learn if your country is on the list.
        

          MDN Plus allows you to auto-renew, so you don‚Äôt have to worry about your subscription. Learn the steps to update your payment method in this article.
        
Grow and share your expertise with others. Answer questions and improve our knowledge base.
Learn More

          Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.
        

          Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
        
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
As a contributor, you can report and work on issues.
After you report an issue, the issue gets triaged. Issue triaging is typically done by people assigned the role of a maintainer or an owner.
While reporting an issue or participating in a conversation in an issue, always ensure that your inputs are contributing to the overall progress of the project. Consider whether the issues you open and your comments in an issue are constructive and on topic and are not just adding noise.
Do the following:
Avoid doing the following:
Issues are used to track bugs. An issue must be a single actionable task or a collection of related actionable tasks and must have a clear outcome.
If you think you've found a bug with the content on MDN Web Docs or with the look and feel of the website, search the current open issues in the relevant repository and make sure nobody else has reported the issue.

  If the issue you're opening is not to report a bug but to perform a series of tasks, you can create the issue as a task list.
  Explain the context or reason for performing the tasks in the description.
  Ensure that you list all the actionable tasks as a checklist.

For example:
Remember that if you take on an issue, the expectation is for the work to be completed in a timely manner. If you're not able to make any progress for a week after being assigned or can no longer complete the required task, leave a comment and unassign yourself from the issue.
These are the general steps for working on an issue:
Note: An issue with the needs triage label indicates that the MDN Web Docs core team has not reviewed the issue yet, and you shouldn't begin work on it.
If you spot a bug ‚Äî whether it's a problem with the website's look and feel or an error in documentation ‚Äî you can try to fix it yourself. Learn how you can contribute by going through our contribution guide.
If the bug is small, such as a typo or a minor sentence improvement, or involves an uncontroversial fix, submit a pull request with the changes.

  For all other type of bugs, begin by opening the issue. Add a comment about your intent to work on the issue and if possible, describe your proposed solution or steps to fix the issue.
  Wait for the issue to be triaged, so that the MDN Web Docs team can verify that the issue is legit and approves your proposed solution.


Note: If you open a pull request before the issue has been triaged, your time and effort might go waste if the linked issue is deemed invalid or the solution does not match the one expected by the MDN Web Docs team.
    After the issue is triaged, assign the issue to yourself.
  
Using the guidelines on working on an issue, try to fix the problem by updating the appropriate source, such as:
Each repository includes useful information to guide you on how to contribute.
If you are a maintainer or an owner in the MDN Web Docs GitHub organization, you are responsible for triaging issues in one or more MDN Web Docs repository.
The overall process for triaging includes some general and some issue-specific tasks.
These are the guidelines to follow while triaging each issue.
These are some of the things to keep in mind while reviewing the validity of an issue:
Review each issue against the following checklist to ensure that the issue contains the described information for someone to start working on the bug:
If any of the above information is not present, then you should ask the author of the issue to provide these details, and add the needs info label to the issue. Resume triaging the issue only after those details have been provided (after which, you can remove the needs info label). It is okay to wait for up to a week to get a response from the author.
For each bug, set a priority label based on the severity of the issue to help people who want to work on the most important issues or areas.
In general, critical issues should be fixed immediately and are most likely handled by MDN Web Docs staff and peers.
If possible, add information that can help contributors to fix the issue. The information can be in the form of steps, general approach, links to other similar fixed issues, or reading resources. A well-laid out plan or steps is especially required in issues that are labeled good first issue and can help ramp up new contributors quickly. You can time-box this task to 5-10 minutes.
For example, as a triager, you can add the following information to the issue you are triaging:
Next, set the following labels as appropriate:
Note: After the triage process is complete, remove the needs triage label.
This page was last modified on Oct 11, 2023 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
The power of MDN lies in its vast, vital community of active users and contributors. Since 2005, approximately 45,000 contributors have created the documentation we know and love. Together, contributors have created over 45,000 documents that make up an up-to-date, comprehensive, and free resource for web developers around the world. In addition to English-language articles, over 35 volunteers lead translation and localization efforts for Chinese, French, Japanese, Korean, Portuguese, Russian, and Spanish. With over 200 commits per week, the culture of active contribution is going strong. And you can be a part of it.
MDN collaborates with partners from across the industry, including standards bodies, browser vendors, and other industry leaders. Since 2017, these collaborators are formally represented through the Product Advisory Board (PAB). MDN is an influential resource and the PAB helps ensure that MDN‚Äôs influence puts the web first, not any one vendor or organization, and respects the needs of web developers across the industry. Each quarter, the PAB meets to discuss problems, prioritize content creation, and make connections for future collaborations.
MDN has a unique place right now as a vendor-neutral and authoritative documentation and information resource for web developers. The MDN PAB has helped to bring feedback from the wider web community (including standards engineers, web browser makers and open source developers) into MDN to help keep it strong. As a member of the web community and a fan of MDN it‚Äôs been great to be a part of.
Open Web Docs (OWD), an independent open source organization, is one of the most productive contributors to MDN Web Docs. OWD contributes as part of their mission to support ‚Äúweb platform documentation for the benefit of web developers & designers worldwide.‚Äù The team at OWD has led or contributed to many projects to improve documentation on MDN. They're an invaluable partner in the day-to-day work of making MDN. Read more about OWD‚Äôs activities in their 2022 Impact and Transparency Report and get continuous updates on their Mastodon account.
Open Web Docs strongly believes in MDN as critical infrastructure for the web platform. As a vendor-neutral organization, we are supporting MDN with an independent editorial voice and with the needs of the global community of web developers and designers in mind.
MDN's resources are entirely available under various open source licenses. Detailed information on licensing for reuse of MDN content, especially regarding copyrights and attribution, can be found here.
We are an open community of developers building resources for a better web, regardless of brand, browser, or platform. Anyone can contribute, and each person who does makes us stronger. Together we can continue to drive innovation on the web to serve the greater good. It starts here, with you. Join us!
No matter your specific level of expertise, individual strengths, and interests in coding or writing, there are many ways for you to get involved and tackle important documentation tasks.
Are you ready to become an active part of the MDN community but not sure where to begin? We've got you covered. See our step-by-step directions for making your first contribution to MDN on GitHub.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.

                Several MDN readers have a similar problem: They want to read MDN content in English, but instead are getting pages in a different language. This is a hard problem to solve, but there are some things you can do to see mo‚Ä¶
              
Powered by Discourse, best viewed with JavaScript enabled
Following the success of Interop 2023, we are pleased to confirm that the project will continue in 2024 with a new selection of focus areas, representing areas of the web platform where we think we can have the biggest positive impact on users and web developers. 
Sign up for the Mozilla Developer Newsletter:
If you haven‚Äôt previously confirmed a subscription to a Mozilla-related newsletter you may have to do so. Please check your inbox or your spam filter for an email from us.
  
During the Firefox 120 beta cycle, a new crash signature appeared on our radars with significant volume. Engineers working on Firefox, explore the subtle pitfalls of combining compiler flags.

Puppeteer now supports the next-generation, cross-browser WebDriver BiDi standard. This new protocol makes it easy for web developers to write automated tests that work across multiple browser engines.
A month ago, we introduced our Nightly package for Debian-based Linux distributions. Today, we are proud to announce we made our .deb package available for Developer Edition and Beta!
We're thrilled to announce the first release of llamafile, inviting the open source community to join this groundbreaking project. 

With llamafile, you can effortlessly convert large language model (LLM) weights into executables. Imagine transforming a 4GB file of LLM weights into a binary that runs smoothly on six different operating systems, without requiring installation. 

Mozilla has just launched the AI Guide, a collaborative hub for developers to join forces, inspire each other, and lead the way in groundbreaking generative AI advancements.  The AI Guide‚Äôs initial focus begins with language models and the aim is to become a collaborative community-driven resource covering other types of models.

To deliver against our vision and enable a better online experience for everyone, we‚Äôve been working hard on making Firefox even faster. We‚Äôre extremely happy to report that this has resulted in a significant improvement in speed over the past year. 

Protecting user privacy is a core element of Mozilla‚Äôs vision for the web and the internet at large. In pursuit of this vision, we‚Äôre pleased to announce new partnerships with Fastly and Divvi Up to deploy privacy-preserving technology in Firefox.
Firefox performance on Vue.js has improved significantly throughout the year. Most recently, we sped up reactivity with Proxy optimizations. This change landed in Firefox 118, so it‚Äôs currently on Beta and will ride along to Release by the end of September.
This blog post will walk through how we developed UniFFI: a Rust library for auto-generating foreign language bindings. We will walk through some of the issues that arose along the way and how we handled them.
Artificial intelligence may well prove one of the most impactful and disruptive technologies to come along in years. We want to understand, support, and contribute to these efforts because we believe that they offer one of the best ways to help ensure that the AI systems that emerge are truly trustworthy. With this in mind, a small team within Mozilla‚Äôs innovation group recently undertook a hackathon at our headquarters in San Francisco. Our objective: build a Mozilla internal chatbot prototype.

          Except where otherwise noted, content on this site is licensed
          under the
          Creative Commons Attribution Share-Alike License v3.0
          or any later version.
        
Firefox is no longer supported on Windows 8.1 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.


        Download Firefox ESR 64-bit
      



        Download Firefox ESR 32-bit
      

Firefox is no longer supported on macOS 10.14 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.
Get the not-for-profit-backed browser on Windows, Mac or Linux.
Get the customizable mobile browser for Android smartphones.
Get the mobile browser for your iPhone or iPad.
Simply private mobile browsing.
Learn how Firefox treats your data with respect.
Read about new Firefox features and ways to stay safe online.
Get the details on the latest Firefox updates.
View all Firefox Browsers
See if your email has appeared in a company‚Äôs data breach.
Help prevent Facebook from collecting your data outside their site.
Save and discover the best stories from across the web.
Get protection beyond your browser, on all your devices.
Learn how each Firefox product protects and respects your data.
Sign up for new accounts without handing over your email address.
New features and tools for a customized MDN experience
View all Products
Learn about the values and principles that guide our mission.
Meet the not-for-profit behind Firefox that stands for a better web.
Join the fight for a healthy internet.
Meet the team that‚Äôs building technology for a better internet.
Work for a mission-driven organization that makes people-first products.
Learn about Mozilla and the issues that matter to us.

More About Mozilla

Gather in this interactive, online, multi-dimensional social space.
Get the Firefox browser built just for developers.
Check out the home for web developer resources.
Donate your voice so the future of the web can hear everyone.
Discover ways to bring bright ideas to life.
It is available in the following languages:

          
          
          Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.
          Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
        
Firefox is no longer supported on Windows 8.1 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.


        Download Firefox ESR 64-bit
      



        Download Firefox ESR 32-bit
      

Firefox is no longer supported on macOS 10.14 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.
Get the not-for-profit-backed browser on Windows, Mac or Linux.
Get the customizable mobile browser for Android smartphones.
Get the mobile browser for your iPhone or iPad.
Simply private mobile browsing.
Learn how Firefox treats your data with respect.
Read about new Firefox features and ways to stay safe online.
Get the details on the latest Firefox updates.
View all Firefox Browsers
See if your email has appeared in a company‚Äôs data breach.
Help prevent Facebook from collecting your data outside their site.
Save and discover the best stories from across the web.
Get protection beyond your browser, on all your devices.
Learn how each Firefox product protects and respects your data.
Sign up for new accounts without handing over your email address.
New features and tools for a customized MDN experience
View all Products
Learn about the values and principles that guide our mission.
Meet the not-for-profit behind Firefox that stands for a better web.
Join the fight for a healthy internet.
Stories about how our people and products are changing the world for the better.
Meet the team that‚Äôs building technology for a better internet.
Work for a mission-driven organization that makes people-first products.
Learn about Mozilla and the issues that matter to us.

More About Mozilla

Gather in this interactive, online, multi-dimensional social space.
Get the Firefox browser built just for developers.
Check out the home for web developer resources.
Donate your voice so the future of the web can hear everyone.
Discover ways to bring bright ideas to life.
We care about your privacy. When Mozilla (that‚Äôs us) collects information about you, our Mozilla Privacy Policy describes how we handle that information.
This privacy notice applies to Mozilla operated websites and mobile apps, which include the domains mozilla.org, and firefox.com, among others. This includes, for example, addons.mozilla.org, bugzilla.mozilla.org, careers.mozilla.org, community.mozilla.org, developer.mozilla.org, foundation.mozilla.org, people.mozilla.org, support.mozilla.org, and wiki.mozilla.org.
We may receive personal information from you based on your interaction with us on social media platforms, if you submit a job, intern, grant or fellow application, if you volunteer as a Mozilla community member, if you submit user feedback or a request to us, if you sign up for an account or for a subscription, or if you engage with a product or policy campaign. 
Social Media: If you engage with our accounts on external social media platforms, such as X (formerly Twitter) and Facebook, we may receive personal information about you. If you use these networks, their privacy policies apply, and you are encouraged to read them.
Job, Intern, Grant & Fellow Applicants: Applicants for employment, internship, grant, or fellowship opportunities with Mozilla are required to give us a name, street address, telephone number, email address, and resume, and sometimes additional information as well. We use this information to process and evaluate applications and to communicate with applicants about opportunities. We use Greenhouse to handle employment and intern applications, and Fluxx.io to handle fellowship and MOSS grant applications.
Contributors: Volunteering for Mozilla as a community contributor may require Mozilla and others to communicate with you at the email address that you provide in connection to your contribution and to recognize your efforts. If you contribute to Bugzilla, or our code bases, then your email address and possibly your name will be publicly available to all internet users. If you create an account on Mozilla Connect (platform powered by Khoros, which has its own privacy notice), your name, your avatar, your posts and replies and other information you share will be accessible to Mozilla and other community members. If you create a profile at people.mozilla.org, it will be accessible to Mozilla employees and Mozilla contributors; you can edit your profile data at Profile Settings. We sometimes use contributor information from sources (such as Bugzilla) in dashboards to visually share aggregated data on the Mozilla community. An example is https://wiki.mozilla.org/Contribute/Dashboards. Where possible, we try to minimize contact information that is publicly displayed.
User Feedback: You can provide feedback to us on our products and services on webpages like connect.mozilla.org, through an in-product experience, or through channels such as email, Bugzilla, Matrix, a social media account, our Get Involved page, or through a group like Student Ambassadors. Please minimize the personal information you choose to share on these forums because your comments may be accessible to the public.
Accounts & Subscriptions: Some websites, for example Add-Ons for Firefox, Relay, Monitor, and MDN, require account creation. For account management we use Mozilla accounts, GitHub, or custom systems; learn more about how to manage your Mozilla account data. You may periodically receive emails in connection with your account or through subscriptions. Our email management vendors are SalesForce Marketing Cloud, Amazon Simple Email Service, Mailchimp, SocketLabs, Campaign Monitor or Acoustic, and you can unsubscribe using the link at the bottom of the relevant email. 
Product & Policy Campaigns: Some of our webpages host product or policy campaigns. For example, you can request a link by email or SMS to install Firefox on your mobile device or petition your legislators on internet issues. We may use third parties to manage these campaigns and handle any data that you choose to submit. 
We may use cookies, clear GIFs, third-party web analytics, device information, and IP addresses for functionality and to better understand user interaction with our products, services, and communications. 
Functionality: We may use information such as cookies, device information, and IP addresses to enhance functionality of certain products, services, and communications. For example:
Metrics: We may also use cookies, device information, and IP addresses, along with clear GIFs, cookies and third-party services to help us understand in the aggregate how users engage with our products, services, communications, websites, online campaigns, snippets, devices, and other platforms. We use:
Fraud Prevention: Mozilla has implemented third-party technology, Google‚Äôs Invisible reCAPTCHA, that operates in the background on some of our websites in order to identify fraudulent activity. Use of the Invisible reCAPTCHA is governed by the Google Privacy Policy and Terms of Use.
You can control individual cookie preferences, indicate your cookie preferences to others, and opt out of web analytics and optimization tools. 
Cookie History: You can accept or decline individual cookies in your preferences in the Tools/Options/Privacy history section. Note that certain features of our products and services may not function properly without the aid of cookies. 
Do Not Track: Mozilla does not track users across third-party websites to provide targeted advertising. In addition, if you have configured your browser to send a ‚ÄúDo Not Track‚Äù signal when accessing our websites, Mozilla will not utilize any of the tools described in the Metrics section.
Email: Our marketing communications are optional to receive and you can unsubscribe from the footer of the email or by updating your Mozilla email preferences, or for Thunderbird‚Äôs newsletter, on the Thunderbird website.
Analytics & Optimization: Follow the instructions below to prevent data collection about your visits to Mozilla websites:
Social Media: The social sharing buttons on Mozilla websites are designed not to share data with the applicable social media provider until you specifically click the relevant social media icon.
Some Mozilla websites allow you to make purchases (such as apps or gear), contribute funds to specific Mozilla projects, or make donations in support of Mozilla public and charitable programs. These transactions are often handled by Mozilla‚Äôs third-party vendors.
Payment Processing: When you purchase something via a Mozilla website, contribute funds or make donations, you will send payment through one of our third-party payment providers: Stripe, Apple Pay, PayPal, Venmo or Google Pay. Mozilla receives a record of your account (including your billing address and the last four digits of your payment method) and (where relevant) the status of your account‚Äôs subscription; we may also receive your name, mailing address, and/or email address. This data is used for payment processing, fraud detection and record-keeping purposes. 
Contact and Donation Information: We use Acoustic, Salesforce, Fundraise Up and Campaign Monitor to email receipts and store records, which are retained for 10 years from the date of last payment. If you make a donation to the Mozilla Foundation or Thunderbird, we use Fundraise Up to manage our donations and provide transactional receipts to donors.
To make requests regarding your personal data, please contact us through our Data Subject Access Request Portal. If you have any other questions regarding personal data or our privacy practices, please contact us at compliance@mozilla.com. We respond to all requests we receive from individuals wishing to exercise their data protection rights in accordance with applicable data protection laws.

          
          
          Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.
          Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
        
Firefox is no longer supported on Windows 8.1 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.


        Download Firefox ESR 64-bit
      



        Download Firefox ESR 32-bit
      

Firefox is no longer supported on macOS 10.14 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.
Get the not-for-profit-backed browser on Windows, Mac or Linux.
Get the customizable mobile browser for Android smartphones.
Get the mobile browser for your iPhone or iPad.
Simply private mobile browsing.
Learn how Firefox treats your data with respect.
Read about new Firefox features and ways to stay safe online.
Get the details on the latest Firefox updates.
View all Firefox Browsers
See if your email has appeared in a company‚Äôs data breach.
Help prevent Facebook from collecting your data outside their site.
Save and discover the best stories from across the web.
Get protection beyond your browser, on all your devices.
Learn how each Firefox product protects and respects your data.
Sign up for new accounts without handing over your email address.
New features and tools for a customized MDN experience
View all Products
Learn about the values and principles that guide our mission.
Meet the not-for-profit behind Firefox that stands for a better web.
Join the fight for a healthy internet.
Stories about how our people and products are changing the world for the better.
Meet the team that‚Äôs building technology for a better internet.
Work for a mission-driven organization that makes people-first products.
Learn about Mozilla and the issues that matter to us.

More About Mozilla

Gather in this interactive, online, multi-dimensional social space.
Get the Firefox browser built just for developers.
Check out the home for web developer resources.
Donate your voice so the future of the web can hear everyone.
Discover ways to bring bright ideas to life.
We care about your privacy. When Mozilla (that‚Äôs us) collects information about you, our Mozilla Privacy Policy describes how we handle that information.
This privacy notice applies to Mozilla operated websites and mobile apps, which include the domains mozilla.org, and firefox.com, among others. This includes, for example, addons.mozilla.org, bugzilla.mozilla.org, careers.mozilla.org, community.mozilla.org, developer.mozilla.org, foundation.mozilla.org, people.mozilla.org, support.mozilla.org, and wiki.mozilla.org.
We may receive personal information from you based on your interaction with us on social media platforms, if you submit a job, intern, grant or fellow application, if you volunteer as a Mozilla community member, if you submit user feedback or a request to us, if you sign up for an account or for a subscription, or if you engage with a product or policy campaign. 
Social Media: If you engage with our accounts on external social media platforms, such as X (formerly Twitter) and Facebook, we may receive personal information about you. If you use these networks, their privacy policies apply, and you are encouraged to read them.
Job, Intern, Grant & Fellow Applicants: Applicants for employment, internship, grant, or fellowship opportunities with Mozilla are required to give us a name, street address, telephone number, email address, and resume, and sometimes additional information as well. We use this information to process and evaluate applications and to communicate with applicants about opportunities. We use Greenhouse to handle employment and intern applications, and Fluxx.io to handle fellowship and MOSS grant applications.
Contributors: Volunteering for Mozilla as a community contributor may require Mozilla and others to communicate with you at the email address that you provide in connection to your contribution and to recognize your efforts. If you contribute to Bugzilla, or our code bases, then your email address and possibly your name will be publicly available to all internet users. If you create an account on Mozilla Connect (platform powered by Khoros, which has its own privacy notice), your name, your avatar, your posts and replies and other information you share will be accessible to Mozilla and other community members. If you create a profile at people.mozilla.org, it will be accessible to Mozilla employees and Mozilla contributors; you can edit your profile data at Profile Settings. We sometimes use contributor information from sources (such as Bugzilla) in dashboards to visually share aggregated data on the Mozilla community. An example is https://wiki.mozilla.org/Contribute/Dashboards. Where possible, we try to minimize contact information that is publicly displayed.
User Feedback: You can provide feedback to us on our products and services on webpages like connect.mozilla.org, through an in-product experience, or through channels such as email, Bugzilla, Matrix, a social media account, our Get Involved page, or through a group like Student Ambassadors. Please minimize the personal information you choose to share on these forums because your comments may be accessible to the public.
Accounts & Subscriptions: Some websites, for example Add-Ons for Firefox, Relay, Monitor, and MDN, require account creation. For account management we use Mozilla accounts, GitHub, or custom systems; learn more about how to manage your Mozilla account data. You may periodically receive emails in connection with your account or through subscriptions. Our email management vendors are SalesForce Marketing Cloud, Amazon Simple Email Service, Mailchimp, SocketLabs, Campaign Monitor or Acoustic, and you can unsubscribe using the link at the bottom of the relevant email. 
Product & Policy Campaigns: Some of our webpages host product or policy campaigns. For example, you can request a link by email or SMS to install Firefox on your mobile device or petition your legislators on internet issues. We may use third parties to manage these campaigns and handle any data that you choose to submit. 
We may use cookies, clear GIFs, third-party web analytics, device information, and IP addresses for functionality and to better understand user interaction with our products, services, and communications. 
Functionality: We may use information such as cookies, device information, and IP addresses to enhance functionality of certain products, services, and communications. For example:
Metrics: We may also use cookies, device information, and IP addresses, along with clear GIFs, cookies and third-party services to help us understand in the aggregate how users engage with our products, services, communications, websites, online campaigns, snippets, devices, and other platforms. We use:
Fraud Prevention: Mozilla has implemented third-party technology, Google‚Äôs Invisible reCAPTCHA, that operates in the background on some of our websites in order to identify fraudulent activity. Use of the Invisible reCAPTCHA is governed by the Google Privacy Policy and Terms of Use.
You can control individual cookie preferences, indicate your cookie preferences to others, and opt out of web analytics and optimization tools. 
Cookie History: You can accept or decline individual cookies in your preferences in the Tools/Options/Privacy history section. Note that certain features of our products and services may not function properly without the aid of cookies. 
Do Not Track: Mozilla does not track users across third-party websites to provide targeted advertising. In addition, if you have configured your browser to send a ‚ÄúDo Not Track‚Äù signal when accessing our websites, Mozilla will not utilize any of the tools described in the Metrics section.
Email: Our marketing communications are optional to receive and you can unsubscribe from the footer of the email or by updating your Mozilla email preferences, or for Thunderbird‚Äôs newsletter, on the Thunderbird website.
Analytics & Optimization: Follow the instructions below to prevent data collection about your visits to Mozilla websites:
Social Media: The social sharing buttons on Mozilla websites are designed not to share data with the applicable social media provider until you specifically click the relevant social media icon.
Some Mozilla websites allow you to make purchases (such as apps or gear), contribute funds to specific Mozilla projects, or make donations in support of Mozilla public and charitable programs. These transactions are often handled by Mozilla‚Äôs third-party vendors.
Payment Processing: When you purchase something via a Mozilla website, contribute funds or make donations, you will send payment through one of our third-party payment providers: Stripe, Apple Pay, PayPal, Venmo or Google Pay. Mozilla receives a record of your account (including your billing address and the last four digits of your payment method) and (where relevant) the status of your account‚Äôs subscription; we may also receive your name, mailing address, and/or email address. This data is used for payment processing, fraud detection and record-keeping purposes. 
Contact and Donation Information: We use Acoustic, Salesforce, Fundraise Up and Campaign Monitor to email receipts and store records, which are retained for 10 years from the date of last payment. If you make a donation to the Mozilla Foundation or Thunderbird, we use Fundraise Up to manage our donations and provide transactional receipts to donors.
To make requests regarding your personal data, please contact us through our Data Subject Access Request Portal. If you have any other questions regarding personal data or our privacy practices, please contact us at compliance@mozilla.com. We respond to all requests we receive from individuals wishing to exercise their data protection rights in accordance with applicable data protection laws.

          
          
          Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.
          Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
        
Firefox is no longer supported on Windows 8.1 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.


        Download Firefox ESR 64-bit
      



        Download Firefox ESR 32-bit
      

Firefox is no longer supported on macOS 10.14 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.
Get the not-for-profit-backed browser on Windows, Mac or Linux.
Get the customizable mobile browser for Android smartphones.
Get the mobile browser for your iPhone or iPad.
Simply private mobile browsing.
Learn how Firefox treats your data with respect.
Read about new Firefox features and ways to stay safe online.
Get the details on the latest Firefox updates.
View all Firefox Browsers
See if your email has appeared in a company‚Äôs data breach.
Help prevent Facebook from collecting your data outside their site.
Save and discover the best stories from across the web.
Get protection beyond your browser, on all your devices.
Learn how each Firefox product protects and respects your data.
Sign up for new accounts without handing over your email address.
New features and tools for a customized MDN experience
View all Products
Learn about the values and principles that guide our mission.
Meet the not-for-profit behind Firefox that stands for a better web.
Join the fight for a healthy internet.
Stories about how our people and products are changing the world for the better.
Meet the team that‚Äôs building technology for a better internet.
Work for a mission-driven organization that makes people-first products.
Learn about Mozilla and the issues that matter to us.

More About Mozilla

Gather in this interactive, online, multi-dimensional social space.
Get the Firefox browser built just for developers.
Check out the home for web developer resources.
Donate your voice so the future of the web can hear everyone.
Discover ways to bring bright ideas to life.
June 23, 2016
Please read the terms of this entire document  (‚ÄúTerms‚Äù) carefully because it explains your rights and responsibilities when you visit any of Mozilla‚Äôs websites (‚ÄúWebsites‚Äù), or related feeds, social media, newsletters, source code repositories, and emails (together with Websites, these are collectively referred to as ‚ÄúCommunications‚Äù). By accessing or signing up to receive Communications, you agree to be bound by these Terms.
Our Websites include multiple domains such as mozilla.org, mozillians.org, firefox.com, mozillafestival.org, openstandard.com, openbadges.org and webmaker.org. You may also recognize our Websites by nicknames such as Bugzilla@Mozilla, BMO, MozWiki, MoPad, MozReps, MDN, Marketplace, One and Done, SUMO, and AMO.
Some of our Websites connect you with links, apps or add-ons that are provided by other parties and are subject to separate Terms.
Some Websites require you to register for an account in order to access additional features of a Website or another Mozilla service. If applicable, additional terms will be presented to you. You are responsible for all activities under your account.
Some Websites allow you to create a username during registration. Your use of a username must comply with our Acceptable Use Policy. 
Our Communications include content such as articles, images, photographs, comments, software code, audio and video clips, and other materials (collectively, ‚ÄúContent‚Äù).  Content is authored by Mozilla, contributors to Mozilla projects, and other sources. 
Content authored by Mozilla is generally made available for public sharing and reuse through open licenses such as Creative Commons (for expressive material) or the Mozilla Public License (for software code).  In most cases we ask Mozilla contributors to release Content under open licenses. 
Some Content in our Communications is acquired from sources that prohibit further use of their Content without advance permission.  Where possible, the Content or Website footer will display a notice with the applicable license. You agree to abide by such notices.  Note the following specifics:
You may contribute Content when interacting with our Communications, including but not limited to commenting on an article, blogging, contributing code, or contributing graphics or written material (each a ‚ÄúSubmission‚Äù). Unless your Submission is made under a separate agreement with Mozilla, in which case that agreement will govern, then
For Submissions to Mozilla's open source projects:
For all other Submissions, you agree to the following in connection with each:
The Mozilla Websites, Communications & Cookies Privacy Notice describes how we handle information that we receive from you in connection with our Communications. The Privacy Notice explains, for example, that we place certain cookies on our Websites and how you can opt-out.
If you subscribe to receive our newsletters or register for an account in connection with any of our Websites, you may receive transactional emails from us in connection with your account (for example, legal, privacy, and security updates).
Some of our Websites have online tools that allow you to send emails to others. For example, you can invite your contacts to events on Mozillians.  You agree not to misuse others‚Äô email addresses (for example, by spamming them). 
Other Websites, like MozReps, provide tools that enable users to arrange physical events for anyone to attend. Please exercise caution and good judgment when attending events.
For more information on how to report claims of copyright or trademark infringement, please see: https://www.mozilla.org/about/legal/report-infringement/.
These Terms will continue to apply until ended by either you or Mozilla. You can choose to end them at any time for any reason by discontinuing your use of our Communications and, if applicable, deleting your account.
We may suspend or terminate your access to our Communications at any time for any reason, including, but not limited to, if we reasonably believe: (i) you have violated these Terms, our Acceptable Use Policy, or other relevant policy; (ii) you create risk or possible legal exposure for us; or (iii) our provision of the Communications to you is no longer commercially viable.
In all such cases, these Terms shall terminate, except that the following sections shall continue to apply: Indemnification, Disclaimer; Limitation of Liability, Miscellaneous.
You agree to defend, indemnify and hold harmless Mozilla, its contractors, contributors, licensors, and partners; and the respective directors, officers, employees and agents of the foregoing ("Indemnified Parties") from and against any and all third party claims and expenses, including attorneys' fees, arising out of or related to your use of our Communications (including, but not limited to, from your Submissions or from your violation of any these Terms).
THE COMMUNICATIONS ARE PROVIDED "AS IS" WITH ALL FAULTS. TO THE EXTENT PERMITTED BY LAW, MOZILLA AND THE INDEMNIFIED PARTIES HEREBY DISCLAIM ALL WARRANTIES, WHETHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION WARRANTIES THAT THE COMMUNICATIONS ARE FREE OF DEFECTS, MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE, AND NON-INFRINGING. YOU BEAR THE ENTIRE RISK AS TO USING THE COMMUNICATIONS FOR YOUR PURPOSES AND AS TO THE QUALITY AND PERFORMANCE OF THE COMMUNICATIONS, INCLUDING WITHOUT LIMITATION THE RISK THAT YOUR HARDWARE, SOFTWARE, OR CONTENT IS DELETED OR CORRUPTED, THAT SOMEONE ELSE GAINS UNAUTHORIZED ACCESS TO YOUR INFORMATION, OR THAT ANOTHER USER MISUSES OR MISAPPROPRIATES YOUR SUBMISSION. THIS LIMITATION WILL APPLY NOTWITHSTANDING THE FAILURE OF ESSENTIAL PURPOSE OF ANY REMEDY. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF IMPLIED WARRANTIES, SO THIS DISCLAIMER MAY NOT APPLY TO YOU.
EXCEPT AS REQUIRED BY LAW, MOZILLA AND THE INDEMNIFIED PARTIES WILL NOT BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, OR EXEMPLARY DAMAGES ARISING OUT OF OR IN ANY WAY RELATING TO THESE TERMS OR THE USE OF OR INABILITY TO USE THE COMMUNICATIONS, INCLUDING WITHOUT LIMITATION DIRECT AND INDIRECT DAMAGES FOR LOSS OF GOODWILL, WORK STOPPAGE, LOST PROFITS, LOSS OF DATA, AND COMPUTER FAILURE OR MALFUNCTION, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES AND REGARDLESS OF THE THEORY (CONTRACT, TORT, OR OTHERWISE) UPON WHICH SUCH CLAIM IS BASED. THE COLLECTIVE LIABILITY OF MOZILLA AND THE INDEMNIFIED PARTIES UNDER THIS AGREEMENT WILL NOT EXCEED $500 (FIVE HUNDRED DOLLARS). SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL, CONSEQUENTIAL, OR SPECIAL DAMAGES, SO THIS EXCLUSION AND LIMITATION MAY NOT APPLY TO YOU.
We may update these Terms from time to time to address a new feature of the Communications or to clarify a provision. The updated Terms will be posted online. If the changes are substantive, we will announce the update through our usual channels for such announcements such as blog posts, banners, emails, or forums. Your continued use of our Communications after the effective date of such changes constitutes your acceptance of such changes. To make your review more convenient, we will post an effective date at the top of this page.
These Terms constitute the entire agreement between you and Mozilla concerning our Communications and supersede any prior versions of these Terms. The Communications and these Terms are governed by the laws of the state of California, U.S.A., excluding its conflict of law provisions. All claims and disputes arising out of the Communications or these Terms shall be brought exclusively in the courts of Santa Clara County, California, and you consent to personal jurisdiction in those courts. If any portion of these Terms is held to be invalid or unenforceable, the remaining portions will remain in full force and effect. In the event of a conflict between a translated version of these Terms and the English language version, the English language version shall control. In the event of a conflict between these Terms and relevant additional terms, the additional terms shall control.
Mozilla 
Attn: Mozilla ‚Äì Legal Notices
149 New Montgomery St.
4th Floor
San Francisco, CA 94105
USA
Telephone: 650-903-0800
Fax: 650-903-0875
Legal-notices at mozilla.com

          
          
          Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.
          Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
        
Firefox is no longer supported on Windows 8.1 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.


        Download Firefox ESR 64-bit
      



        Download Firefox ESR 32-bit
      

Firefox is no longer supported on macOS 10.14 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.
Get the not-for-profit-backed browser on Windows, Mac or Linux.
Get the customizable mobile browser for Android smartphones.
Get the mobile browser for your iPhone or iPad.
Simply private mobile browsing.
Learn how Firefox treats your data with respect.
Read about new Firefox features and ways to stay safe online.
Get the details on the latest Firefox updates.
View all Firefox Browsers
See if your email has appeared in a company‚Äôs data breach.
Help prevent Facebook from collecting your data outside their site.
Save and discover the best stories from across the web.
Get protection beyond your browser, on all your devices.
Learn how each Firefox product protects and respects your data.
Sign up for new accounts without handing over your email address.
New features and tools for a customized MDN experience
View all Products
Learn about the values and principles that guide our mission.
Meet the not-for-profit behind Firefox that stands for a better web.
Join the fight for a healthy internet.
Stories about how our people and products are changing the world for the better.
Meet the team that‚Äôs building technology for a better internet.
Work for a mission-driven organization that makes people-first products.
Learn about Mozilla and the issues that matter to us.

More About Mozilla

Gather in this interactive, online, multi-dimensional social space.
Get the Firefox browser built just for developers.
Check out the home for web developer resources.
Donate your voice so the future of the web can hear everyone.
Discover ways to bring bright ideas to life.
Software and other innovations designed to advance our mission.
Become a volunteer contributor in a number of different areas.
Read about our vision for the Web and how we intend to pursue that vision.

  The heart of Mozilla is people. We put people first and do our best to recognize, appreciate and respect the diversity of our global contributors. The Mozilla Project welcomes contributions from everyone who shares our goals and wants to contribute in a healthy and constructive manner within our community. As such, we have adopted this code of conduct and require all those who participate to agree and adhere to these Community Participation Guidelines in order to help us create a safe and positive community experience for all.
  

  These guidelines aim to support a community where all people should feel safe to participate, introduce new ideas and inspire others, regardless of:
  

  Openness, collaboration and participation are core aspects of our work ‚Äî from development on Firefox to collaboratively designing curriculum. We gain strength from diversity and actively seek participation from those who enhance it. These guidelines exist to enable diverse individuals and groups to interact and collaborate to mutual advantage. This document outlines both expected and prohibited behavior.
  

  These guidelines outline our behavior expectations as members of the Mozilla community in all Mozilla activities, both offline and online. Your participation is contingent upon following these guidelines in all Mozilla activities, including but not limited to:
  

  These guidelines work in conjunction with our Anti-Harassment/Discrimination Policies[1], which sets out protections for, and obligations of, Mozilla employees. The Anti-Harassment/Discrimination Policy is crafted with specific legal definitions and requirements in mind.
  

  While these guidelines / code of conduct are specifically aimed at Mozilla‚Äôs work and community, we recognize that it is possible for actions taken outside of Mozilla‚Äôs online or in person spaces to have a deep impact on community health. (For example, in the past, we publicly identified an anonymous posting aimed at a Mozilla employee in a non-Mozilla forum as clear grounds for removal from the Mozilla community.) This is an active topic in the diversity and inclusion realm. We anticipate wide-ranging discussions among our communities about appropriate boundaries.
  

  The following behaviors are expected of all Mozillians:
  

  Value each other‚Äôs ideas, styles and viewpoints. We may not always agree, but disagreement is no excuse for poor manners. Be open to different possibilities and to being wrong. Be respectful in all interactions and communications, especially when debating the merits of different options. Be aware of your impact and how intense interactions may be affecting people. Be direct, constructive and positive. Take responsibility for your impact and your mistakes ‚Äì if someone says they have been harmed through your words or actions, listen carefully, apologize sincerely, and correct the behavior going forward.
  

  We are likely to have some discussions about if and when criticism is respectful and when it‚Äôs not. We must be able to speak directly when we disagree and when we think we need to improve. We cannot withhold hard truths. Doing so respectfully is hard, doing so when others don‚Äôt seem to be listening is harder, and hearing such comments when one is the recipient can be even harder still. We need to be honest and direct, as well as respectful.
  

  Seek diverse perspectives. Diversity of views and of people on teams powers innovation, even if it is not always comfortable. Encourage all voices. Help new perspectives be heard and listen actively. If you find yourself dominating a discussion, it is especially important to step back and encourage other voices to join in. Be aware of how much time is taken up by dominant members of the group. Provide alternative ways to contribute or participate when possible.

  
  Be inclusive of everyone in an interaction, respecting and facilitating people‚Äôs participation whether they are:
  

Remote (on video or phone)
Not native language speakers
Coming from a different culture
Using pronouns other than ‚Äúhe‚Äù or ‚Äúshe‚Äù
Living in a different time zone
Facing other challenges to participate


  Think about how you might facilitate alternative ways to contribute or participate. If you find yourself dominating a discussion, step back. Make way for other voices and listen actively to them.
  
Understand Different Perspectives

  Our goal should not be to ‚Äúwin‚Äù every disagreement or argument. A more productive goal is to be open to ideas that make our own ideas better. Strive to be an example for inclusive thinking. ‚ÄúWinning‚Äù is when different perspectives make our work richer and stronger.
  
Appreciate and Accommodate Our Similarities and Differences

  Mozillians come from many cultures and backgrounds. Cultural differences can encompass everything from official religious observances to personal habits to clothing. Be respectful of people with different cultural practices, attitudes and beliefs. Work to eliminate your own biases, prejudices and discriminatory practices. Think of others‚Äô needs from their point of view. Use preferred titles (including pronouns) and the appropriate tone of voice. Respect people‚Äôs right to privacy and confidentiality. Be open to learning from and educating others as well as educating yourself; it is unrealistic to expect Mozillians to know the cultural practices of every ethnic and cultural group, but everyone needs to recognize one‚Äôs native culture is only part of positive interactions.
  
Lead by Example

  By matching your actions with your words, you become a person others want to follow. Your actions influence others to behave and respond in ways that are valuable and appropriate for our organizational outcomes. Design your community and your work for inclusion. Hold yourself and others accountable for inclusive behaviors. Make decisions based on the highest good for Mozilla‚Äôs mission.
  


  Be inclusive of everyone in an interaction, respecting and facilitating people‚Äôs participation whether they are:
  

  Think about how you might facilitate alternative ways to contribute or participate. If you find yourself dominating a discussion, step back. Make way for other voices and listen actively to them.
  

  Our goal should not be to ‚Äúwin‚Äù every disagreement or argument. A more productive goal is to be open to ideas that make our own ideas better. Strive to be an example for inclusive thinking. ‚ÄúWinning‚Äù is when different perspectives make our work richer and stronger.
  

  Mozillians come from many cultures and backgrounds. Cultural differences can encompass everything from official religious observances to personal habits to clothing. Be respectful of people with different cultural practices, attitudes and beliefs. Work to eliminate your own biases, prejudices and discriminatory practices. Think of others‚Äô needs from their point of view. Use preferred titles (including pronouns) and the appropriate tone of voice. Respect people‚Äôs right to privacy and confidentiality. Be open to learning from and educating others as well as educating yourself; it is unrealistic to expect Mozillians to know the cultural practices of every ethnic and cultural group, but everyone needs to recognize one‚Äôs native culture is only part of positive interactions.
  

  By matching your actions with your words, you become a person others want to follow. Your actions influence others to behave and respond in ways that are valuable and appropriate for our organizational outcomes. Design your community and your work for inclusion. Hold yourself and others accountable for inclusive behaviors. Make decisions based on the highest good for Mozilla‚Äôs mission.
  

  The following behaviors are considered to be unacceptable under these guidelines.
  

  Violence and threats of violence are not acceptable - online or offline. This includes incitement of violence toward any individual, including encouraging a person to commit self-harm. This also includes posting or threatening to post other people‚Äôs personally identifying information (‚Äúdoxxing‚Äù) online.
  

  Conflicts will inevitably arise, but frustration should never turn into a personal attack. It is not okay to insult, demean or belittle others. Attacking someone for their opinions, beliefs and ideas is not acceptable. It is important to speak directly when we disagree and when we think we need to improve, but such discussions must be conducted respectfully and professionally, remaining focused on the issue at hand.
  

  Hurtful or harmful language related to:
  

  is not acceptable. This includes deliberately referring to someone by a gender that they do not identify with, and/or questioning the legitimacy of an individual‚Äôs gender identity. If you‚Äôre unsure if a word is derogatory, don‚Äôt use it. This also includes repeated subtle and/or indirect discrimination; when asked to stop, stop the behavior in question.
  

  Unwelcome sexual attention or unwelcome physical contact is not acceptable. This includes sexualized comments, jokes or imagery in interactions, communications or presentation materials, as well as inappropriate touching, groping, or sexual advances. Additionally, touching a person without permission, including sensitive areas such as their hair, pregnant stomach, mobility device (wheelchair, scooter, etc) or tattoos is unacceptable. This includes physically blocking or intimidating another person. Physical contact or simulated physical contact (such as emojis like ‚Äúkiss‚Äù) without affirmative consent is not acceptable. The sharing or distribution of sexualized images or text is unacceptable.
  

  Sustained disruption of events, forums, or meetings, including talks and presentations, will not be tolerated. This includes:
  

  We will treat influencing or leading such activities the same way we treat the activities themselves, and thus the same consequences apply.
  

  Bad behavior from any Mozillian, including those with decision-making authority, will not be tolerated. Intentional efforts to exclude people (except as part of a consequence of the guidelines or other official action) from Mozilla activities are not acceptable and will be dealt with appropriately.
  

  Reports of harassment/discrimination will be promptly and thoroughly investigated by the people responsible for the safety of the space, event or activity. Appropriate measures will be taken to address the situation.
  

  Anyone being asked to stop unacceptable behavior is expected to comply immediately. Violation of these guidelines can result in anyone being asked to leave an event or online space, either temporarily or for the duration of the event, or being banned from participation in spaces, or future events and activities in perpetuity.
  

  Mozilla Staff are held accountable, in addition to these guidelines, to Mozilla‚Äôs staff Anti-Harassment/Discrimination Policies [1]. Mozilla staff in violation of these guidelines may be subject to further consequences, such as disciplinary action, up to and including termination of employment. For contractors or vendors, violation of these guidelines may affect continuation or renewal of contract.
  

  In addition, any participants who abuse the reporting process will be considered to be in violation of these guidelines and subject to the same consequences. False reporting, especially to retaliate or exclude, will not be accepted or tolerated.
  

  If you believe you‚Äôre experiencing unacceptable behavior that will not be tolerated as outlined above, please use cpg-report@mozilla.com to report. Reports are triaged by the Community Participation Guidelines Response Lead.
  

  After receiving a concise description of your situation, they will review and determine the next steps. In addition to conducting any investigation, they can provide a range of resources, from a private consultation to other community resources. They will involve other colleagues or outside specialists (such as legal counsel), as needed to appropriately address each situation.
  

  Please also report to us if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines, even if the situation is not happening to you.
  

  If you feel you have been unfairly accused of violating these guidelines, please follow the same reporting process.
  

  Each physical or virtual Mozilla space shall have a designated contact.
  

  All Mozilla events will have designated a specific safety guideline with emergency and anti-abuse contacts at the event as well as online. These contacts will be posted prominently throughout the event, and in print and online materials. Event leaders are requested to speak at the event about the guidelines and to ask participants to review and agree to them when they sign up for the event.
  

  Reports will receive an email notice of receipt. Once an incident has been investigated and a decision has been communicated to the relevant parties, all have the opportunity to appeal this decision by sending an email to cpg-questions@mozilla.com.
  

  Everyone is encouraged to ask questions about these guidelines. If you are organizing an event or activity, reach out for tips for building inclusion for your event, activity or space. Your input is welcome and you will always get a response within 24 hours (or on the next weekday, if it is the weekend) if you reach out to cpg-questions@mozilla.com. Please review this change log for updates to this document.
  

  This set of guidelines is distributed under a Creative Commons Attribution-ShareAlike license.
  

  These guidelines have been adapted with modifications from Mozilla‚Äôs original Community Participation Guidelines, the Ubuntu Code of Conduct, Mozilla‚Äôs View Source Conference Code of Conduct, and the Rust Language Code of Conduct, which are based on Stumptown Syndicate‚Äôs Citizen Code of Conduct. Additional text from the LGBTQ in Technology Code of Conduct and the WisCon code of conduct. This document and all associated processes are only possible with the hard work of many, many Mozillians.
  

  Mozilla may amend the guidelines from time to time and may also vary the procedures it sets out where appropriate in a particular case. Your agreement to comply with the guidelines will be deemed agreement to any changes to it. This policy does not form part of any Mozilla employee‚Äôs contract of employment or otherwise have contractual effect.
  
[1] The anti-harassment policy is accessible to paid staff here.
Get the Mozilla newsletter and help us keep it open and free.

Afghanistan
Akrotiri
Albania
Algeria
American Samoa
Andorra
Angola
Anguilla
Antarctica
Antigua and Barbuda
Argentina
Armenia
Aruba
Ashmore and Cartier Islands
Australia
Austria
Azerbaijan
Bahamas, The
Bahrain
Baker Island
Bangladesh
Barbados
Bassas da India
Belarus
Belgium
Belize
Benin
Bermuda
Bhutan
Bolivia
Bosnia and Herzegovina
Botswana
Bouvet Island
Brazil
British Indian Ocean Territory
Brunei
Bulgaria
Burkina Faso
Burma
Burundi
Cambodia
Cameroon
Canada
Cape Verde
Caribbean Netherlands
Cayman Islands
Central African Republic
Chad
Chile
China
Christmas Island
Clipperton Island
Cocos (Keeling) Islands
Colombia
Comoros
Congo (Brazzaville)
Congo (Kinshasa)
Cook Islands
Coral Sea Islands
Costa Rica
Croatia
Cuba
Cura√ßao
Cyprus
Czechia
C√¥te d‚ÄôIvoire
Denmark
Dhekelia
Diego Garcia
Djibouti
Dominica
Dominican Republic
Ecuador
Egypt
El Salvador
Equatorial Guinea
Eritrea
Estonia
Eswatini
Ethiopia
Europa Island
Falkland Islands (Islas Malvinas)
Faroe Islands
Fiji
Finland
France
French Guiana
French Polynesia
French Southern and Antarctic Lands
Gabon
Gambia, The
Gaza Strip
Georgia
Germany
Ghana
Gibraltar
Glorioso Islands
Greece
Greenland
Grenada
Guadeloupe
Guam
Guatemala
Guernsey
Guinea
Guinea-Bissau
Guyana
Haiti
Heard Island and McDonald Islands
Honduras
Hong Kong
Howland Island
Hungary
Iceland
India
Indonesia
Iran
Iraq
Ireland
Isle of Man
Israel
Italy
Jamaica
Jan Mayen
Japan
Jarvis Island
Jersey
Johnston Atoll
Jordan
Juan de Nova Island
Kazakhstan
Kenya
Kingman Reef
Kiribati
Korea, North
Korea, South
Kosovo
Kuwait
Kyrgyzstan
Laos
Latvia
Lebanon
Lesotho
Liberia
Libya
Liechtenstein
Lithuania
Luxembourg
Macau
Madagascar
Malawi
Malaysia
Maldives
Mali
Malta
Marshall Islands
Martinique
Mauritania
Mauritius
Mayotte
Mexico
Micronesia, Federated States of
Midway Islands
Moldova
Monaco
Mongolia
Montenegro
Montserrat
Morocco
Mozambique
Namibia
Nauru
Navassa Island
Nepal
Netherlands
New Caledonia
New Zealand
Nicaragua
Niger
Nigeria
Niue
Norfolk Island
North Macedonia
Northern Mariana Islands
Norway
Oman
Pakistan
Palau
Palmyra Atoll
Panama
Papua New Guinea
Paracel Islands
Paraguay
Peru
Philippines
Pitcairn Islands
Poland
Portugal
Puerto Rico
Qatar
Romania
Russia
Rwanda
R√©union
Saint Barth√©lemy
Saint Helena, Ascension, and Tristan da Cunha
Saint Kitts and Nevis
Saint Lucia
Saint Martin
Saint Pierre and Miquelon
Saint Vincent and the Grenadines
Samoa
San Marino
Saudi Arabia
Senegal
Serbia
Seychelles
Sierra Leone
Singapore
Sint Maarten
Slovakia
Slovenia
Solomon Islands
Somalia
South Africa
South Georgia and South Sandwich Islands
South Sudan
Spain
Spratly Islands
Sri Lanka
Sudan
Suriname
Svalbard
Sweden
Switzerland
Syria
S√£o Tom√© and Pr√≠ncipe
Taiwan
Tajikistan
Tanzania
Thailand
Timor-Leste
Togo
Tokelau
Tonga
Trinidad and Tobago
Tromelin Island
Tunisia
Turkey
Turkmenistan
Turks and Caicos Islands
Tuvalu
Uganda
Ukraine
United Arab Emirates
United Kingdom
United States
Uruguay
Uzbekistan
Vanuatu
Vatican City
Venezuela
Vietnam
Virgin Islands, British
Virgin Islands, U.S.
Wake Island
Wallis and Futuna
West Bank
Western Sahara
Yemen
Zambia
Zimbabwe


Bahasa Indonesia
Deutsch
English
Espa√±ol
Fran√ßais
Polski
Portugu√™s
–†—É—Å—Å–∫–∏–π
Ê≠£È´î‰∏≠Êñá



 HTML
            

 Text
            



 I‚Äôm okay with Mozilla handling my info as explained in this Privacy Notice




          
            Sign Up Now
          
        

              
              
                We will only send you Mozilla-related information.
              
            

If you haven‚Äôt previously confirmed a subscription to a Mozilla-related newsletter, you may have to do so. Please check your inbox or your spam filter for an email from us.

          
          
          Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.
          Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
        
Firefox is no longer supported on Windows 8.1 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.


        Download Firefox ESR 64-bit
      



        Download Firefox ESR 32-bit
      

Firefox is no longer supported on macOS 10.14 and below.
Please download Firefox ESR (Extended Support Release) to use Firefox.
Get the not-for-profit-backed browser on Windows, Mac or Linux.
Get the customizable mobile browser for Android smartphones.
Get the mobile browser for your iPhone or iPad.
Simply private mobile browsing.
Learn how Firefox treats your data with respect.
Read about new Firefox features and ways to stay safe online.
Get the details on the latest Firefox updates.
View all Firefox Browsers
See if your email has appeared in a company‚Äôs data breach.
Help prevent Facebook from collecting your data outside their site.
Save and discover the best stories from across the web.
Get protection beyond your browser, on all your devices.
Learn how each Firefox product protects and respects your data.
Sign up for new accounts without handing over your email address.
New features and tools for a customized MDN experience
View all Products
Learn about the values and principles that guide our mission.
Meet the not-for-profit behind Firefox that stands for a better web.
Join the fight for a healthy internet.
Meet the team that‚Äôs building technology for a better internet.
Work for a mission-driven organization that makes people-first products.
Learn about Mozilla and the issues that matter to us.

More About Mozilla

Gather in this interactive, online, multi-dimensional social space.
Get the Firefox browser built just for developers.
Check out the home for web developer resources.
Donate your voice so the future of the web can hear everyone.
Discover ways to bring bright ideas to life.
It is available in the following languages:

          
          
          Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.
          Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.
        
Failed to fetch https://foundation.mozilla.org/: 403 Client Error: Forbidden for url: https://foundation.mozilla.org/
Web technology reference for developers
Structure of content on the web
Code used to describe document style
General-purpose scripting language
Protocol for transmitting web resources
Interfaces for building web applications
Developing extensions for web browsers
Web technology reference for developers
Learn web development
Learn web development
Learn to structure web content with HTML
Learn to style content using CSS
Learn to run scripts in the browser
Learn to make the web accessible to all
A customized MDN experience
Get real-time assistance and support
All browser compatibility updates at a glance
Learn how to use MDN Plus
Frequently asked questions about MDN Plus
MDN Web Doc's content is available free of charge and is available under various open-source licenses.
This section covers the types of content we provide and the copyrights and licenses that are in effect for each type if you choose to reuse any of it.
Note: The content on MDN Web Docs has been prepared with the contributions of authors from both inside and outside Mozilla. Unless otherwise indicated, the content is available under the terms of the Creative Commons Attribution-ShareAlike license (CC-BY-SA), v2.5 or any later version.
Your reuse of the content here is published under the same license as the original content‚ÄîCC-BY-SA v2.5 or any later version. When reusing the content on MDN Web Docs, you need to ensure that attribution is given to the original content as well as to "Mozilla Contributors". Include a hyperlink (online) or URL (in print) to the specific page of the content being sourced. For example, to provide attribution for this article, you can write:
Attributions and copyright licensing by Mozilla Contributors is licensed under CC-BY-SA 2.5.
In the above example, "Mozilla Contributors" links to the history of the cited page. See Recommended practices for attribution for further explanation.
Code samples added on or after August 20, 2010 are in the public domain CC0. No licensing notice is necessary but if you need one, you can use: Any copyright is dedicated to the Public Domain: https://creativecommons.org/publicdomain/zero/1.0/
Code samples added before August 20, 2010 are available under the MIT license; you should insert the following attribution information into the MIT template: "¬© <date of last wiki page revision> <name of person who put it in the wiki>".
Since the launch of the new Yari MDN platform on December 14 2020, there is currently no way to determine which one you need. We are working on this and will update this content soon.
If you wish to contribute to MDN Web Docs, you agree that your documentation is available under the Attribution-ShareAlike license (or occasionally an alternative license already specified by the page you are editing) and that your code samples are available under Creative Commons CC-0 (a Public Domain dedication).
Warning: No new pages may be created using alternate licenses.
Copyright for contributed materials remains with the author unless the author assigns it to someone else.
If you have any questions or concerns about anything discussed here, please contact the MDN Web Docs team.
The rights in the logos, trademarks, and service marks of the Mozilla Foundation, as well as the look and feel of this website, are not licensed under the Creative Commons license, and to the extent they are works of authorship (like logos and graphic design), they are not included in the work that is licensed under those terms. If you use the text of documents and wish to also use any of these rights, or if you have any other questions about complying with our licensing terms for this collection, you should contact the Mozilla Foundation here: licensing@mozilla.org.

  In general, we do not approve of copying content from other sources and putting it on MDN.
  MDN should be made up of original content wherever possible.
  If we receive a pull request and discover that it contains plagiarized content, we will close it and request that the submitter resubmit the change with the content rewritten into their own words.


Note: Unless there is a good reason to republish the content, we will probably say "no".
    The MDN writing team's decision is final.
  
If someone wants to donate an article to MDN that they previously published on their blog or it makes sense to copy a complex reference sheet to MDN, there may be justification for republishing it. For these cases, discuss your plan with the MDN team beforehand:
If the content is published under a closed license:
If the content is published under an open license:
We regularly get users asking us questions about how to link to MDN Web Docs and whether or not it is even allowed. The short answer is: yes, you can link to MDN Web Docs! Not only is the hypertext link the essence of the web, it is both a way to point your users to valuable resources as well as a show of trust toward the work our community does.
This page was last modified on Jun 6, 2023 by MDN contributors.
Your blueprint for a better internet.
Visit Mozilla Corporation‚Äôs not-for-profit parent, the Mozilla Foundation.Portions of this content are ¬©1998‚Äì2024 by individual mozilla.org contributors. Content available under a Creative Commons license.

Documentation
Grow your business
Resources
Featured articles
Contact
Everything you need to know about DigitalOcean.
Hand-picked favorites.
DigitalOcean ‚Äî January 17, 2024
VentureBeat ‚Äî January 18, 2024
IT Pro ‚Äî October 23, 2023
The latest on all things DigitalOcean.
DigitalOcean research on trends in cloud, developer trends, and small and medium-sized businesses.
Learn more
DigitalOcean is constantly growing and improving. Check out even more product news and updates.
Learn more
DigitalOcean‚Äôs social impact efforts empower change-makers around the globe.
Learn more
Fast Company Best Workplaces for Innovators
Newsweek Most Loved Workplaces
Inc. Best Workplaces 
Built In 100 Best Places To Work In NYC
Built In 100 Best Places To Work In Boston
Built In 50 Best Remote-First Companies to Work For
DigitalOcean (NYSE: DOCN) simplifies cloud computing so builders and businesses can spend more time creating software that changes the world. With its mission-critical infrastructure and fully managed offerings, DigitalOcean helps developers at startups and small and medium-sized businesses (SMBs) rapidly build, deploy and scale, whether creating a digital presence or building digital products. DigitalOcean combines the power of simplicity, security, community and customer support so customers can spend less time managing their infrastructure and more time building innovative applications that drive business growth.
For media inquiries contact press@digitalocean.com
Get free infrastructure credits, robust training, and technical support for worry-free migration to DigitalOcean.
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
Last Updated on October 24, 2023
This Privacy Policy describes how DigitalOcean, LLC and its affiliates (‚ÄúDigitalOcean,‚Äù ‚Äúwe,‚Äù ‚Äúour‚Äù or ‚Äúus‚Äù) collect, use, and share information in connection with your use of our websites (including www.digitalocean.com), services, and applications (collectively, the ‚ÄúServices‚Äù). This Privacy Policy (the ‚ÄúPrivacy Policy‚Äù) does not apply to information our customers may process when using our Services.
If you are looking for California-specific information, check out our CCPA Privacy Notice as well, which is incorporated into this Privacy Policy.
We may collect and receive information about users of our Services (‚Äúusers,‚Äù ‚Äúyou,‚Äù or ‚Äúyour‚Äù) from various sources, including: (i) information you provide through your user account on the Services (your ‚ÄúAccount‚Äù) if you register for the Services; (ii) your use of the Services; and (iii) from third-party websites, services, and partners.
We recommend that you read this Privacy Policy in full, including the Additional Disclosures referenced at the bottom of this document, to ensure you are fully informed. If you have any questions about this Privacy Policy or DigitalOcean‚Äôs data collection, use, and disclosure practices, please contact us at privacy@digitalocean.com.
Information You Provide
I. Account Registration. When you register for an Account, we may ask for your contact information, including items such as name, company name, address, email address, and telephone number. If you choose to refer a friend to our Services, we may also collect your friend‚Äôs email address so that we may send them a referral or promotional code to sign up for our Services.
II. Payment Information. When you add your financial account information to your Account, that information is directed to our third-party payment processor. We do not store your financial account information on our systems; however, we have access to, and may retain, subscriber information through our third-party payment processor.
III. Personal Data. This means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a user in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. ‚ÄúPersonal Data‚Äù includes equivalent terms in other Data Protection Law, such as the CCPA-defined term ‚ÄúPersonal Information,‚Äù as context requires.
IV. User Content. Our ‚ÄúCommunity‚Äù feature allows you to publicly post content on our Services. By registering for our Community, you agree that your profile information and the content you post may be viewed and used by other users and third parties we do not control.
V. Communications. If you contact us directly, we may receive additional information about you such as your name, email address, phone number, the contents of the message and/or attachments you may send us, and any other information you may choose to provide. We may also receive a confirmation when you open an email from us.
The personal information that you are asked to provide, and the reasons why you are asked to provide it, will be made clear to you at the point we ask you to provide your personal information.
Information We Collect When You Use Our Services.
I. Cookies and Other Tracking Technologies. As is true of most websites, we gather certain information automatically and store it in log files. In addition, when you use our Services, we may collect certain information automatically from your device. This information may include internet protocol (‚ÄúIP‚Äù) addresses, browser type, internet service provider (‚ÄúISP‚Äù), referring/exit pages, operating system, date/time stamp, clickstream data, landing page, and referring URL. To collect this information, a cookie may be set on your computer or device when you visit our Services. Cookies contain a small amount of information that allows our web servers to recognize you. We store information that we collect through cookies, log files, and/or clear gifs to record your preferences. We may also automatically collect information about your use of features of our Services, about the functionality of our Services, frequency of visits, and other information related to your interactions with the Services. We may track your use across different websites and services. In some countries, including countries in the European Economic Area (‚ÄúEEA‚Äù), the information referenced above in this paragraph may be considered personal data under applicable data protection laws.
II. Usage of our Services. When you use our Services, we may collect information about your engagement with and utilization of our Services, such as processor and memory usage, storage capacity, navigation of our Services, and system-level metrics. We use this data to operate the Services, maintain and improve the performance and utilization of the Services, develop new features, protect the security and safety of our Services and our customers, and provide customer support. We also use this data to develop aggregate analysis and business intelligence that enable us to operate, protect, make informed decisions, and report on the performance of our business.
Information We Receive from Third Parties.
I. Third-Party Accounts. If you choose to link our Services to a third-party account, we will receive information about that account, such as your authentication token from the third-party account, to authorize linking. If you wish to limit the information available to us, you should visit the privacy settings of your third-party accounts to learn about your options.
We use the information we collect in various ways, including to:
We may share the information we collect in various ways, including the following:
Our legal basis for collecting and using the personal data described above will depend on the personal data concerned and the specific context in which we collect it.
However, we will normally collect personal data from you only (i) where we need the personal data to perform a contract with you; (ii) where the processing is in our legitimate interests and not overridden by your rights; or (iii) where we have your consent to do so.  We have a legitimate interest in operating our Services and communicating with you as necessary to provide these Services, for example when responding to your queries, improving our platform, undertaking marketing, or for the purposes of detecting or preventing illegal activities.
In some cases, we may also have a legal obligation to collect personal data from you or may otherwise need the personal data to protect your vital interests or those of another person.
If we ask you to provide personal data to comply with a legal requirement or to perform a contract with you, we will make this clear at the relevant time and advise you whether the provision of your personal data is mandatory or not (as well as of the possible consequences if you do not provide your personal data).
You may access other third-party offerings through the Services, for example by clicking on links to those third-party offerings from within the Services. DigitalOcean is not responsible for the privacy policies and/or practices of these third-party offerings, and we encourage you to carefully review their privacy policies.
DigitalOcean is committed to protecting your information. To do so, we employ a variety of security technologies and measures designed to protect information from unauthorized access, use, or disclosure. The measures we use are designed to provide a level of security appropriate to the risk of processing your personal data. Please note that no service is completely secure. While we strive to protect your data, we cannot guarantee that unauthorized access, hacking, data loss or a data breach will never occur.
For more information on DigitalOcean‚Äôs security controls, please see the following resources:
We retain personal data we collect from you where we have an ongoing legitimate business need to do so (for example, to provide you with a service you have requested or to comply with applicable legal, tax, or accounting requirements).
When we have no ongoing legitimate business need to process your personal data, we will either delete or anonymize it or, if this is not possible (for example, because your personal data has been stored in backup archives), then we will securely store your personal data and isolate it from any further processing until deletion is possible.
If you are a registered user, you may access certain information associated with your Account by logging into our Services or emailing privacy@digitalocean.com. If you terminate your Account, any public activity on your Account prior to deletion may remain stored on our servers and may remain accessible to the public.
To protect your privacy and security, we may also take reasonable steps to verify your identity before updating or removing your information. The information you provide us may be archived or stored periodically by us according to backup processes conducted in the ordinary course of business for disaster recovery purposes. Your ability to access and correct your information may be temporarily limited where access and correction could: inhibit DigitalOcean‚Äôs ability to comply with a legal obligation; inhibit DigitalOcean‚Äôs ability to investigate, make or defend legal claims; result in disclosure of personal data about a third party; or result in breach of a contract or disclosure of trade secrets or other proprietary business information belonging to DigitalOcean or a third party.
If you are a resident of the EEA, Switzerland, or the United Kingdom (‚ÄúUK‚Äù), you have the following data protection rights:
We respond to all requests we receive from individuals wishing to exercise their data protection rights in accordance with applicable data protection laws.
You can use some of the features of the Services without registering, thereby limiting the type of information that we collect.
You may unsubscribe from receiving certain promotional emails from us. If you wish to do so, simply follow the instructions found at the end of the email. Even if you unsubscribe, we may still contact you for informational, transactional, account-related, or similar purposes.
Many browsers have an option for disabling cookies, which may prevent your browser from accepting new cookies or enable selective use of cookies. Please note that, if you choose not to accept cookies, some features and the personalization of our Services may no longer work for you. You will continue to receive advertising material but it will not be tailored to your interests.
DigitalOcean does not knowingly collect information from children under the age of 13, and children under 13 are prohibited from using our Services. If you learn that a child has provided us with personal data in violation of this Privacy Policy, you can alert us at privacy@digitalocean.com.
This Privacy Policy may be modified from time to time, so please review it frequently. Changes to this Privacy Policy will be posted on our websites. If we materially change the ways in which we use or share personal data previously collected from you through our Services, we will notify you through our Services, on this page, by email, or other communication.
With respect to Personal Data of Data Subjects located in the EEA, Switzerland, or the United Kingdom that Customer transfers to DigitalOcean or permits DigitalOcean to access, the parties agree that by executing the DPA they also execute the Standard Contractual Clauses, which will be incorporated by reference and form an integral part of the DPA. The parties agree that, with respect to the elements of the Standard Contractual Clauses that require the parties‚Äô input, Schedules 1-3 contain all the relevant information.
DigitalOcean complies with the EU-U.S. Data Privacy Framework (‚ÄúEU-U.S. DPF‚Äù), the UK Extension to the EU-U.S. DPF (‚ÄúUK Extension‚Äù), and the Swiss-U.S. Data Privacy Framework (‚ÄúSwiss-U.S. DPF‚Äù) as set forth by the U.S. Department of Commerce. DigitalOcean has certified to the U.S. Department of Commerce that it adheres to the EU-U.S. Data Privacy Framework Principles with regard to the processing of personal data received from the European Union in reliance on the EU-U.S. DPF and from the United Kingdom (and Gibraltar) in reliance on the UK Extension.  DigitalOcean has also certified to the U.S. Department of Commerce that it adheres to the Swiss-U.S. Data Privacy Framework Principles (together with the EU-U.S. Data Privacy Framework Principles, the ‚ÄúDPF Principles‚Äù) with regard to the processing of personal data received from Switzerland in reliance on the Swiss-U.S. DPF.  If there is any conflict between the terms in this Privacy Policy and the DPF Principles, the DPF Principles will govern. To learn more about the Data Privacy Framework (‚ÄúDPF‚Äù) program, and to view our certification, please visit https://www.dataprivacyframework.gov/. DigitalOcean‚Äôs compliance with the EU-U.S. DPF, UK Extension, and Swiss-U.S. DPF is subject to the investigatory and enforcement powers of the Federal Trade Commission.
Please also refer to our GDPR FAQ and Data Processing Agreement for more information about data transfers.
DigitalOcean complies with the DPF Principles for all onward transfers of personal data from the EU, UK, and Switzerland, including the onward transfer liability provisions.
DigitalOcean commits to resolve DPF Principles-related complaints about our collection and use of your personal information. EU, UK, and Swiss individuals with inquiries or complaints regarding our handling of personal data received in reliance on the EU-U.S. DPF, the UK Extension, and the Swiss-U.S. DPF should first contact DigitalOcean using the details provided below.
DigitalOcean also commits to referring unresolved complaints concerning our handling of personal data received in reliance on the EU-U.S. DPF, the UK Extension, and the Swiss-U.S. DPF to JAMS, an alternative dispute resolution provider based in the United States. If you do not receive timely acknowledgment of your DPF Principles-related complaint from us, or if we have not addressed your DPF Principles-related complaint to your satisfaction, please visit https://www.jamsadr.com/eu-us-data-privacy-framework for more information or to file a complaint. The services of JAMS are provided at no cost to you.
For complaints regarding DPF compliance not resolved by any of the other DPF mechanisms, you have the possibility, under certain conditions, to invoke binding arbitration. Further information can be found in Annex 1 of the DPF Principles.
If you have any questions or concerns about this Privacy Policy, please email us at privacy@digitalocean.com or write to us at:
DigitalOcean
101 6th Ave
New York, NY 10013
ATTN: Legal
The data controller of your personal information is DigitalOcean, LLC.
CCPA Privacy Notice
Failed to fetch https://investors.digitalocean.com/: 403 Client Error: Forbidden for url: https://investors.digitalocean.com/
Was this page helpful?
The DigitalOcean API lets you programmatically manage your Droplets and other resources using conventional HTTP requests. Any action that you can perform through the DigitalOcean Control Panel (except for creating personal access tokens) can also be performed with the API.





An Introduction to the DigitalOcean Products API



Get familiar with the structure and behavior of the DigitalOcean API.






How to Use the DigitalOcean API



See examples of how to format requests to interact with the DigitalOcean API.






How to Create a Personal Access Token



Create a personal access token for use with the DigitalOcean API.








DigitalOcean API Docs



Programmatically manage Droplets and other DigitalOcean resources using conventional HTTP requests. All of the functionality in the DigitalOcean Control Panel is also available through the API.






Spaces API Reference Documentation



Programmatically manage your data with Spaces‚Äô AWS S3-compatible object storage API






DigitalOcean Metadata API Docs



The metadata API allows a Droplet to access information about itself including user data, Droplet ID, datacenter region, and IP addresses.






OAuth API Reference Documentation



The OAuth API is a secure method for authenticating users and allowing third-party applications limited access to your servers or DigitalOcean user accounts.








godo



The official DigitalOcean API client for Go.

github.com






DropletKit



The official DigitalOcean API client for Ruby.

github.com






DigitalOcean API Libraries



Official and community-created client libraries that let you use the DigitalOcean API in a variety of programming languages.



Keep up to date with changes to our APIs using the API filter in our release notes section.
You can also subscribe to the release notes RSS feed.
Ubuntu 23.04 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 23.04 image from our platform in 30 days.

Ubuntu 23.04 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 23.04 image from our platform in 30 days.
Released v1.101.0 of doctl, the official DigitalOcean CLI. This release adds support for scalable storage for PostgreSQL and MySQL databases and Kafka topic management.

Released v1.101.0 of doctl, the official DigitalOcean CLI. This release adds support for scalable storage for PostgreSQL and MySQL databases and Kafka topic management.
Fedora 37 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 37 image from our platform in 30 days.

Fedora 37 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 37 image from our platform in 30 days.
For more information, see the full release notes.

                    
                    
Please try using alternative keywords or simplifying your search terms.
Was this page helpful?
Release notes track incremental improvements and major releases for the DigitalOcean cloud platform.
You can subscribe to the release notes  RSS feed.

Due to the high frequency of its updates, we keep a separate  changelog for Kubernetes version updates.
You can now retrieve crash logs for apps in App Platform.

You can now retrieve crash logs for apps in App Platform.
We have updated the following buildpacks:

Hugo buildpack: The default version of Hugo has been updated from v0.118.2 to v0.121.2. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.
Python buildpack: A new Python v4 buildpack version has been released that removes support for Python 3.7. If you are on Ubuntu-22 and have an existing Python app that is on v3, v2, v1 or v0, we recommend upgrading to v4.

Python buildpack v4:

Default Python version is now 3.12.
Added support for Python 3.11.7 and 3.12.1.
Added support for Python 3.12 and 3.11.6.
Dropped support for Python 3.7.


Python buildpack v3:

Added support for Python 3.12 and 3.11.6.




Go buildpack: Additional Go versions have been added and default versions of Go have been updated. For more information and configuration options, see the buildpack‚Äôs documentation page.

Add go1.21.1, go1.21.2, go1.21.3, go1.21.4, go1.21.5 and go1.21.6
Add go1.20.8, go1.20.9, go1.20.10, go1.20.11, go1.20.12 and go1.20.13
Add go1.19.13
go1.20 defaults to go1.20.13
go1.21 defaults to go1.21.5
go1.19 defaults to go1.19.13


PHP buildpack: Updates to the PHP v1 buildpack are listed below. If you have an existing PHP app that uses v0, please upgrade to v1, see: How to Upgrade Buildpacks in App Platform.

PHP buildpack v2:

Add PHP/8.1.26 - PHP/8.1.27
Add PHP/8.2.13 - PHP/8.2.14
Add PHP/8.3.0 - PHP/8.3.1
Add composer/2.6.6






We have updated the following buildpacks:
Daily backups for Droplets are in early availability. You can now enable daily backups on new and existing Droplets in NYC3 and SFO3.

Daily backups for Droplets are in early availability. You can now enable daily backups on new and existing Droplets in NYC3 and SFO3.
We have removed the built-in Kubernetes Dashboard from the control panel.
As an alternative, you can use the Kubernetes Dashboard 1-Click App from the DigitalOcean Marketplace, Cilium Hubble, or other open-source options for monitoring and visualizing Kubernetes workloads.

We have removed the built-in Kubernetes Dashboard from the control panel.
As an alternative, you can use the Kubernetes Dashboard 1-Click App from the DigitalOcean Marketplace, Cilium Hubble, or other open-source options for monitoring and visualizing Kubernetes workloads.
Ubuntu 23.04 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 23.04 image from our platform in 30 days.

Ubuntu 23.04 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 23.04 image from our platform in 30 days.
All currently supported DigitalOcean Kubernetes versions now have Cilium Hubble, Hubble Relay and Hubble UI enabled. For more information, see Use Cilium Hubble.

All currently supported DigitalOcean Kubernetes versions now have Cilium Hubble, Hubble Relay and Hubble UI enabled. For more information, see Use Cilium Hubble.
You can now deploy apps to App Platform using public and private repositories on GitHub Container Registry. You can also now deploy images using private Docker Hub repositories.

You can now deploy apps to App Platform using public and private repositories on GitHub Container Registry. You can also now deploy images using private Docker Hub repositories.
The GST rate for Singapore has increased from 8% to 9% in accordance with Budget 2022 from the Inland Revenue Authority of Singapore (IRAS). Learn more about Singapore taxes.

The GST rate for Singapore has increased from 8% to 9% in accordance with Budget 2022 from the Inland Revenue Authority of Singapore (IRAS). Learn more about Singapore taxes.
The VAT rate for Switzerland and Liechtenstein has increased from 7.7% to 8.1% in accordance with the Federal Tax Administration of Switzerland (FTA)‚Äôs amendment to the AHV Act. Learn more about Switzerland and Liechtenstein taxes.

The VAT rate for Switzerland and Liechtenstein has increased from 7.7% to 8.1% in accordance with the Federal Tax Administration of Switzerland (FTA)‚Äôs amendment to the AHV Act. Learn more about Switzerland and Liechtenstein taxes.
We have released an updated Container Registry experience to give customers enhanced management of their private registries. This includes additional features to add, validate, and edit containers, as well as changes to ensure Gradient Deployments with containers start successfully. For more information, see Manage Containers.

We have released an updated Container Registry experience to give customers enhanced management of their private registries. This includes additional features to add, validate, and edit containers, as well as changes to ensure Gradient Deployments with containers start successfully. For more information, see Manage Containers.
Gradient Deployments can now be created with secured endpoints using basic access authentication encoded tokens. For more information, see Endpoint Security.

Gradient Deployments can now be created with secured endpoints using basic access authentication encoded tokens. For more information, see Endpoint Security.
NVIDIA H100 GPUs are now available both on-demand and for guaranteed capacity reservations in the NYC2 region via Paperspace‚Äôs sales team. For more information, see the Paperspace NVIDIA H100 reference page.

NVIDIA H100 GPUs are now available both on-demand and for guaranteed capacity reservations in the NYC2 region via Paperspace‚Äôs sales team. For more information, see the Paperspace NVIDIA H100 reference page.
Ubuntu-18 will be deprecated for App Platform apps in 2024. We recommended that all apps upgrade to Ubuntu-22 as soon as possible.
We have updated the following buildpacks:

PHP buildpack: Updates to PHP v2 buildpack are listed below. If you are on Ubuntu-22 and have an existing PHP app that is on v0 or v1, we recommend upgrading to v2.

PHP buildpack v2:

Add PHP/8.2.11 - PHP/8.2.12
Add PHP/8.1.24 - PHP/8.1.25
Upgrade ext-Redis to 6.0.2




NodeJS buildpack: A new NodeJS v1 buildpack version has been released that has an updated default version of NodeJS v20 for Ubuntu-22. If you are on Ubuntu-22 and have an existing NodeJS app that is on v0, we recommend upgrading to v1.

NodeJS buildpack v1:

Default NodeJS version is now 20.x for Ubuntu-22 apps.






Ubuntu-18 will be deprecated for App Platform apps in 2024. We recommended that all apps upgrade to Ubuntu-22 as soon as possible.
We have updated the following buildpacks:
Paperspace changes are now tracked through DigitalOcean‚Äôs release notes. See Paperspace changelog for the new Paperspace products and features released prior to 14 December 2023.

Paperspace changes are now tracked through DigitalOcean‚Äôs release notes. See Paperspace changelog for the new Paperspace products and features released prior to 14 December 2023.
Released v1.101.0 of doctl, the official DigitalOcean CLI. This release adds support for scalable storage for PostgreSQL and MySQL databases and Kafka topic management.

Released v1.101.0 of doctl, the official DigitalOcean CLI. This release adds support for scalable storage for PostgreSQL and MySQL databases and Kafka topic management.
Fedora 37 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 37 image from our platform in 30 days.

Fedora 37 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 37 image from our platform in 30 days.
You can now add additional storage independently from your chosen database configuration plan when creating or resizing Kafka clusters on DigitalOcean. This provides a more economic option for increasing storage, rather than upgrading your cluster‚Äôs entire plan.
For more details, see our guides on how to resize Kafka clusters.

You can now add additional storage independently from your chosen database configuration plan when creating or resizing Kafka clusters on DigitalOcean. This provides a more economic option for increasing storage, rather than upgrading your cluster‚Äôs entire plan.
For more details, see our guides on how to resize Kafka clusters.
Paperspace users now receive invoices from DigitalOcean. Invoice emails are sent from [email protected] with a PDF copy attached.

Paperspace users now receive invoices from DigitalOcean. Invoice emails are sent from [email protected] with a PDF copy attached.
Learn more on Changes to Paperspace Billing.
The Fedora 39 (fedora-39-x64) base image is now available in the control panel and via the API.

The Fedora 39 (fedora-39-x64) base image is now available in the control panel and via the API.
Additional Spaces CDN PoPs are now available. For the full list, see Spaces availability.

Additional Spaces CDN PoPs are now available. For the full list, see Spaces availability.
SnapShooter Server File backup jobs now have an Include Git Repos? option to fully back up Git repos and their historical metadata. Without this option enabled, the files in a Git repo are backed up but the .git metadata directory is ignored.
See How to Back Up Files with SnapShooter for more information on backing up files with SnapShooter.

SnapShooter Server File backup jobs now have an Include Git Repos? option to fully back up Git repos and their historical metadata. Without this option enabled, the files in a Git repo are backed up but the .git metadata directory is ignored.
See How to Back Up Files with SnapShooter for more information on backing up files with SnapShooter.
Ubuntu-18 has been deprecated for App Platform apps. We recommended that all apps upgrade to Ubuntu-22 as soon as possible.
We have updated the following buildpacks:

Hugo buildpack: The default version of Hugo has been updated from v0.111.3 to v0.118.2. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.
Go buildpack: A new Golang v1 buildpack version has been released that removes support for Heroku-18. If you are on Ubuntu-22 and have an existing Go app that uses v0, we recommend upgrading to v1. For more information and configuration options, see the buildpack‚Äôs documentation page.

Golang buildpack v1:

Add go1.21.0
Add go1.20.3 - go1.20.7
Add go1.19.8 - go1.19.12
go1.20 defaults to go1.20.7
go1.19 defaults to go1.19.12
Drop support for heroku-18 stack


Golang buildpack v0:

Add go1.20.3 and go1.20.4
Add go1.19.8 and go1.19.9
go1.20 defaults to go1.20.4
go1.19 defaults to go1.19.9




PHP buildpack: A new PHP v2 buildpack has been released and updates are listed below. If you are on Ubuntu-22 and have an existing PHP app that is on v0 or v1, we recommend upgrading to v2.

PHP buildpack v2:

Add PHP/8.2.5 - PHP/8.2.10
Add PHP/8.1.18 - PHP/8.1.10
Add PHP/8.0.29 - PHP/8.0.30
Drop support for heroku-18 stack
Drop support for PHP 7.1 and 7.2


PHP buildpack v1:

Add PHP/8.2.5
Add PHP/8.1.18




Python buildpack: A new Python v3 buildpack version has been released that removes support for Heroku-18. If you are on Ubuntu-22 and have an existing Python app that is on v2, v1 or v0, we recommend upgrading to v3.

Python buildpack v3:

Add Python 3.9.17, and 3.9.18
Add Python 3.10.12, 3.10.13, 3.11.4, and 3.11.5
Default Python version is now 3.11.5.
Removed support for Heroku-18.






Ubuntu-18 has been deprecated for App Platform apps. We recommended that all apps upgrade to Ubuntu-22 as soon as possible.
We have updated the following buildpacks:
You can now deploy container images to App Platform using digests. Digests are immutable references to container images. Unlike tags, digests permanently refer to a specific iteration of an image.
You can only deploy an image using a digest by updating your app‚Äôs spec at this time.

You can now deploy container images to App Platform using digests. Digests are immutable references to container images. Unlike tags, digests permanently refer to a specific iteration of an image.
You can only deploy an image using a digest by updating your app‚Äôs spec at this time.
The cors and routes fields under the services array in the App Platform app spec have been deprecated. The cors field now resides in the rules array of the ingress section of the spec. The routes field has been replaced by the match field in the rules array of the ingress section of the spec.
The following truncated example spec demonstrates the format for each updated field:

    
        
            
ingress:
  rules:
  - component:
      name: api
    match:
      path:
        prefix: /api
  - component:
      name: website
    cors:
      allow_origins:
      - prefix: https://internal.example-app.com
    match:
      path:
        prefix: /

        
    


The cors and routes fields under the services array in the App Platform app spec have been deprecated. The cors field now resides in the rules array of the ingress section of the spec. The routes field has been replaced by the match field in the rules array of the ingress section of the spec.
The following truncated example spec demonstrates the format for each updated field:
DigitalOcean Kafka is now in general availability. For more details, see our Kafka documentation and regional availability matrix.

DigitalOcean Kafka is now in general availability. For more details, see our Kafka documentation and regional availability matrix.
We are incrementally making additional Spaces CDN PoPs available for existing customers, starting on 6 November 2023 and finishing on 21 November 2023. For the full upcoming list, see Spaces availability.

We are incrementally making additional Spaces CDN PoPs available for existing customers, starting on 6 November 2023 and finishing on 21 November 2023. For the full upcoming list, see Spaces availability.
Ubuntu 23.10 (ubuntu-23-10-x64) base image is now available in the control panel and via the API.

Ubuntu 23.10 (ubuntu-23-10-x64) base image is now available in the control panel and via the API.
SnapShooter accounts created after 19 October 2023 cannot use Google Drive or Dropbox as a SnapShooter storage provider. Please use SnapShooter Simple Storage, Spaces Object Storage, or other storage providers instead.

SnapShooter accounts created after 19 October 2023 cannot use Google Drive or Dropbox as a SnapShooter storage provider. Please use SnapShooter Simple Storage, Spaces Object Storage, or other storage providers instead.
The Kafka plan featuring 24 VCPUs, 96 GB RAM, and 600 GB of storage is now deprecated.

The Kafka plan featuring 24 VCPUs, 96 GB RAM, and 600 GB of storage is now deprecated.
Backups now remain available for four weeks even if the associated Droplet is deleted. Previously, deleting a Droplet would also delete its backups.
You can view your backups and their expiration dates in the control panel and convert them to snapshots.

Backups now remain available for four weeks even if the associated Droplet is deleted. Previously, deleting a Droplet would also delete its backups.
You can view your backups and their expiration dates in the control panel and convert them to snapshots.
Released v1.100.0 of doctl, the official DigitalOcean CLI. This release adds new commands for managing uptime alerts and retrieving advanced database configuration options.

Released v1.100.0 of doctl, the official DigitalOcean CLI. This release adds new commands for managing uptime alerts and retrieving advanced database configuration options.
The Kubernetes API endpoints /v2/kubernetes/clusters/<cluster ID>/kubeconfig and /v2/kubernetes/clusters/<cluster ID>/credentials now require API tokens to have write scope.

The Kubernetes API endpoints /v2/kubernetes/clusters/<cluster ID>/kubeconfig and /v2/kubernetes/clusters/<cluster ID>/credentials now require API tokens to have write scope.
Premium CPUs for General Purpose Droplets are now available in AMS3 and SFO3.

Premium CPUs for General Purpose Droplets are now available in AMS3 and SFO3.
The following three Kafka plans are now deprecated:

48 VCPUs, 192 GB RAM, 1,200 GB
96 VCPUs, 384 GB RAM, 2,400 GB
120 VCPUs, 480 GB RAM, 3,000 GB


The following three Kafka plans are now deprecated:
App Platform now supports the Aptfile buildpack. The Aptfile buildpack lets you install system-level Ubuntu packages during your app‚Äôs build process.

App Platform now supports the Aptfile buildpack. The Aptfile buildpack lets you install system-level Ubuntu packages during your app‚Äôs build process.
PostgreSQL 11 is now deprecated. Starting on 6 November 2023, all existing PostgreSQL 11 database clusters will automatically upgrade to PostgreSQL 15 during the cluster‚Äôs upgrade window.

PostgreSQL 11 is now deprecated. Starting on 6 November 2023, all existing PostgreSQL 11 database clusters will automatically upgrade to PostgreSQL 15 during the cluster‚Äôs upgrade window.
Our DDoS Protection service is now available and active for all DigitalOcean customers at no additional cost.
DDoS Protection covers Droplets, Kubernetes clusters, managed databases, load balancers, and assigned reserved IPs.

Our DDoS Protection service is now available and active for all DigitalOcean customers at no additional cost.
DDoS Protection covers Droplets, Kubernetes clusters, managed databases, load balancers, and assigned reserved IPs.
Ubuntu 22 is now the default stack for all App Platform apps. This upgrade provides security updates, newer versions of buildpacks, and new features, such as upgrading to newer Node.js versions.
You can downgrade your app‚Äôs stack back to Ubuntu 18 if your app experiences compatibility issues.

Ubuntu 22 is now the default stack for all App Platform apps. This upgrade provides security updates, newer versions of buildpacks, and new features, such as upgrading to newer Node.js versions.
You can downgrade your app‚Äôs stack back to Ubuntu 18 if your app experiences compatibility issues.
Tax collection for Egypt has begun. Learn more about taxes in Egypt.

Tax collection for Egypt has begun. Learn more about taxes in Egypt.
The following MySQL and PostgreSQL plans are now deprecated:

MySQL and PostgreSQL plans with 8 vCPUs, 32 GB RAM, and 600 GB SSD ($480 per month)
MySQL and PostgreSQL plans with 16 vCPUs, 64 GB RAM, and 1220 GB SSD ($960 per month)

All of your existing database clusters with these plans are still functional and accessible to you. However, you cannot resize them. To regain access to these features, fork your database to a new cluster with a supported plan. For more detailed steps, see our guides on how to fork MySQL databases and fork PostgreSQL databases.

The following MySQL and PostgreSQL plans are now deprecated:
All of your existing database clusters with these plans are still functional and accessible to you. However, you cannot resize them. To regain access to these features, fork your database to a new cluster with a supported plan. For more detailed steps, see our guides on how to fork MySQL databases and fork PostgreSQL databases.
When creating or resizing MySQL or PostgreSQL clusters on DigitalOcean, you can now add additional storage independently from your chosen database configuration plan. This provides a more economic option for increasing storage, rather than upgrading your cluster‚Äôs entire plan.
For more details, see our guides on how to resize MySQL clusters and resize PostgreSQL clusters.

When creating or resizing MySQL or PostgreSQL clusters on DigitalOcean, you can now add additional storage independently from your chosen database configuration plan. This provides a more economic option for increasing storage, rather than upgrading your cluster‚Äôs entire plan.
For more details, see our guides on how to resize MySQL clusters and resize PostgreSQL clusters.
Premium CPUs are now available for General Purpose Droplets.



Click here to view the full list of new General Purpose Droplet plans with Premium CPUs.




Slug
vCPUs
RAM (GB)
Disk (GB)
Transfer (TB)
Price




g-2vcpu-8gb-intel
2
8
25
4



$76.00 per month







gd-2vcpu-8gb-intel
2
8
50
4



$79.00 per month







g-4vcpu-16gb-intel
4
16
50
5



$151.00 per month







gd-4vcpu-16gb-intel
4
16
100
5



$158.00 per month







g-8vcpu-32gb-intel
8
32
100
6



$302.00 per month







gd-8vcpu-32gb-intel
8
32
200
6



$317.00 per month







g-16vcpu-64gb-intel
16
64
200
7



$605.00 per month







gd-16vcpu-64gb-intel
16
64
400
7



$634.00 per month







g-32vcpu-128gb-intel
32
128
400
8



$1210.00 per month







gd-32vcpu-128gb-intel
32
128
800
8



$1268.00 per month







g-48vcpu-192gb-intel
48
192
600
9



$1814.00 per month







gd-48vcpu-192gb-intel
48
192
1200
9



$1901.00 per month










The new plans are available through the control panel and the API in NYC1, NYC3, SFO2, TOR1, FRA1, BLR1, and SYD1.

Premium CPUs are now available for General Purpose Droplets.
The new plans are available through the control panel and the API in NYC1, NYC3, SFO2, TOR1, FRA1, BLR1, and SYD1.
We have added Swagger functionality to the API documentation. Using an API key, you can now use the Swagger‚Äôs ‚ÄúTry it out‚Äù feature to interact with the API from the documentation.

We have added Swagger functionality to the API documentation. Using an API key, you can now use the Swagger‚Äôs ‚ÄúTry it out‚Äù feature to interact with the API from the documentation.
Ubuntu 23.04 (ubuntu-23-04-x64) base image is now available in the control panel and via the API.

Ubuntu 23.04 (ubuntu-23-04-x64) base image is now available in the control panel and via the API.
DigitalOcean Managed Databases now supports Apache Kafka in early availability. For more details, see our Kafka documentation and regional availability matrix.

DigitalOcean Managed Databases now supports Apache Kafka in early availability. For more details, see our Kafka documentation and regional availability matrix.
Password-based authentication for newly created Alma 9 Droplets will be disabled 30 days from today, on October 25, 2023, due to an incompatibility between Alma 9‚Äôs password authentication mechanism and DigitalOcean‚Äôs provisioning system. SSH-based login will remain available.
Previously created Alma 9 Droplets will not be affected.

Password-based authentication for newly created Alma 9 Droplets will be disabled 30 days from today, on October 25, 2023, due to an incompatibility between Alma 9‚Äôs password authentication mechanism and DigitalOcean‚Äôs provisioning system. SSH-based login will remain available.
Previously created Alma 9 Droplets will not be affected.
Password-based authentication for newly created Rocky 8 Droplets will be disabled 30 days from today, on October 25, 2023, due to an incompatibility between Rocky 8‚Äôs password authentication mechanism and DigitalOcean‚Äôs provisioning system. SSH-based login will remain available.
Previously created Rocky 8 Droplets will not be affected.

Password-based authentication for newly created Rocky 8 Droplets will be disabled 30 days from today, on October 25, 2023, due to an incompatibility between Rocky 8‚Äôs password authentication mechanism and DigitalOcean‚Äôs provisioning system. SSH-based login will remain available.
Previously created Rocky 8 Droplets will not be affected.
App Platform now supports Google Trust as a Certificate Authority.
When configuring a domain in App Platform, if the domain has a CAA record, you must specify both Google Trust and Let‚Äôs Encrypt in the CAA record for App Platform to issue certificates.

App Platform now supports Google Trust as a Certificate Authority.
When configuring a domain in App Platform, if the domain has a CAA record, you must specify both Google Trust and Let‚Äôs Encrypt in the CAA record for App Platform to issue certificates.
We have released the Vendor API which allows Marketplace vendors to update existing Droplet 1-Click Apps programmatically. See the Vendor API documentation for more information.

We have released the Vendor API which allows Marketplace vendors to update existing Droplet 1-Click Apps programmatically. See the Vendor API documentation for more information.
All Functions API calls now require read-write tokens, even if they are for read-only actions. See the Functions section of the DigitalOcean API reference for more details.

All Functions API calls now require read-write tokens, even if they are for read-only actions. See the Functions section of the DigitalOcean API reference for more details.
The 429 error response to reaching our API‚Äôs burst rate limit now includes a Retry-After header to indicate how long to wait (in seconds) before retrying a request. This additional header enables the configuration of automatic retries and exponentional backoffs in DigitalOcean clients such as doctl, Terraform, and Godo. Learn more about our API burst limit structure in our API Documentation.

The 429 error response to reaching our API‚Äôs burst rate limit now includes a Retry-After header to indicate how long to wait (in seconds) before retrying a request. This additional header enables the configuration of automatic retries and exponentional backoffs in DigitalOcean clients such as doctl, Terraform, and Godo. Learn more about our API burst limit structure in our API Documentation.
Released v1.98.0 of doctl, the official DigitalOcean CLI. This release adds support for automatically retrying API requests that fail with a 429 or 500-level error. The number of attempts can be configured using the --http-retry-max flag or DIGITALOCEAN_HTTP_RETRY_MAX environment variable. To disable retries altogether, set to 0.

Released v1.98.0 of doctl, the official DigitalOcean CLI. This release adds support for automatically retrying API requests that fail with a 429 or 500-level error. The number of attempts can be configured using the --http-retry-max flag or DIGITALOCEAN_HTTP_RETRY_MAX environment variable. To disable retries altogether, set to 0.
You can now upgrade your app stacks to Ubuntu 22 on App Platform. This upgrade provides security updates, newer versions of buildpacks, and new features, such as upgrading to newer Node.js versions.

You can now upgrade your app stacks to Ubuntu 22 on App Platform. This upgrade provides security updates, newer versions of buildpacks, and new features, such as upgrading to newer Node.js versions.
We have released new plans for Basic Droplets with Premium CPUs with different vCPU:RAM ratios.
The new plans are available through the control panel and the API for all data centers. However, plans with a 1:4 vCPU:RAM ratio (like the 2 vCPU and 8 GB RAM plan) are not yet available in LON1, SGP1, and NYC1.



Click here to view the full list of new plans.




Slug
vCPUs
RAM (GB)
Disk (GB)
Transfer (TB)
Price




s-2vcpu-8gb-amd
2
8
100
5



$42.00 per month







s-4vcpu-16gb-amd
4
16
200
8



$84.00 per month







s-8vcpu-32gb-amd
8
32
400
10



$168.00 per month







s-1vcpu-1gb-35gb-intel
1
1
35
1



$8.00 per month







s-1vcpu-2gb-70gb-intel
1
2
70
2



$16.00 per month







s-2vcpu-2gb-90gb-intel
2
2
90
3



$24.00 per month







s-2vcpu-4gb-120gb-intel
2
4
120
4



$32.00 per month







s-2vcpu-8gb-160gb-intel
2
8
160
5



$48.00 per month







s-4vcpu-8gb-240gb-intel
4
8
240
6



$64.00 per month







s-4vcpu-16gb-320gb-intel
4
16
320
8



$96.00 per month







s-8vcpu-16gb-480gb-intel
8
16
480
9



$128.00 per month







s-8vcpu-32gb-640gb-intel
8
32
640
10



$192.00 per month










The previous plans for Basic Droplets with Premium CPUs are no longer available in the control panel, but remain available through the API and CLI with the same slugs.



Click here to view the full list of deprecated plans.




Slug
vCPUs
RAM (GB)
Disk (GB)
Transfer (TB)
Price




s-1vcpu-1gb-intel
1
1
25
1



$7.00 per month







s-1vcpu-2gb-intel
1
2
50
2



$14.00 per month







s-2vcpu-2gb-intel
1
2
60
3



$21.00 per month







s-2vcpu-4gb-intel
1
4
80
4



$28.00 per month







s-4vcpu-8gb-intel
1
8
160
6



$56.00 per month







s-8vcpu-16gb-intel
1
16
320
9



$112.00 per month











We have released new plans for Basic Droplets with Premium CPUs with different vCPU:RAM ratios.
The new plans are available through the control panel and the API for all data centers. However, plans with a 1:4 vCPU:RAM ratio (like the 2 vCPU and 8 GB RAM plan) are not yet available in LON1, SGP1, and NYC1.
The previous plans for Basic Droplets with Premium CPUs are no longer available in the control panel, but remain available through the API and CLI with the same slugs.
We now support ACH direct debit payments for qualifying customers with U.S. bank accounts. Learn more about managing payment methods on DigitalOcean.

We now support ACH direct debit payments for qualifying customers with U.S. bank accounts. Learn more about managing payment methods on DigitalOcean.
We have reenabled the creation of new resources in SFO2 for all customers.

We have reenabled the creation of new resources in SFO2 for all customers.
The Ubuntu 22.10 distribution has reached end of life and is deprecated as of 20 July 2023:
The image will be removed from the control panel starting on 20 July 2023 but will remain accessible for Droplet creation via the API for 30 days after the initial deprecation. If you need to use Ubuntu 22.10 after the image has been fully deprecated, you can create Droplets from a snapshot of a Droplet with that version or from a custom image.

The Ubuntu 22.10 distribution has reached end of life and is deprecated as of 20 July 2023:
The image will be removed from the control panel starting on 20 July 2023 but will remain accessible for Droplet creation via the API for 30 days after the initial deprecation. If you need to use Ubuntu 22.10 after the image has been fully deprecated, you can create Droplets from a snapshot of a Droplet with that version or from a custom image.
PostgreSQL clusters on DigitalOcean now support the pgvector extension, for vector similarity search. For a full list of supported extensions, see our guide Supported PostgreSQL Extensions.

PostgreSQL clusters on DigitalOcean now support the pgvector extension, for vector similarity search. For a full list of supported extensions, see our guide Supported PostgreSQL Extensions.
Ubuntu 18.04 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 18.04 image from our platform in 30 days.

Ubuntu 18.04 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 18.04 image from our platform in 30 days.
Released v1.97.0 of doctl, the official DigitalOcean CLI. This release updates the default behavior of the doctl registry login command to set a 30-day expiry for the registry API token that is created when logging in. The previous default behavior was to create a registry API token that did not expire.
To create a registry API token that does not expire, you can set the new --never-expire flag to true. To set a different expiry time than the default 30 days, you can set the --expiry-seconds flag to an integer representing the number of seconds until the token should expire.
This also adds support for interacting with uptime checks via doctl. Please see the doctl monitoring uptime command reference for more information.

Released v1.97.0 of doctl, the official DigitalOcean CLI. This release updates the default behavior of the doctl registry login command to set a 30-day expiry for the registry API token that is created when logging in. The previous default behavior was to create a registry API token that did not expire.
To create a registry API token that does not expire, you can set the new --never-expire flag to true. To set a different expiry time than the default 30 days, you can set the --expiry-seconds flag to an integer representing the number of seconds until the token should expire.
This also adds support for interacting with uptime checks via doctl. Please see the doctl monitoring uptime command reference for more information.
The VAT rate for Turkey has increased from 18% to 20% in accordance with Presidential Decree No. 7346 published in the Official Gazette on 7 July 2023. Learn more about tax in Turkey.

The VAT rate for Turkey has increased from 18% to 20% in accordance with Presidential Decree No. 7346 published in the Official Gazette on 7 July 2023. Learn more about tax in Turkey.
Spaces is now available in BLR1. You can view the availability of all of our products by datacenter in the regional availability matrix.

Spaces is now available in BLR1. You can view the availability of all of our products by datacenter in the regional availability matrix.
Fedora 36 reached end of life on 18 May 2023. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 36 image from our platform in 30 days.

Fedora 36 reached end of life on 18 May 2023. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 36 image from our platform in 30 days.
DigitalOcean has acquired Paperspace. Learn more in the Paperspace acquisition blog post.

DigitalOcean has acquired Paperspace. Learn more in the Paperspace acquisition blog post.
We no longer bill for outbound data transfer that we determine is dropped by a DigitalOcean firewall rule. Learn more about bandwidth billing.

We no longer bill for outbound data transfer that we determine is dropped by a DigitalOcean firewall rule. Learn more about bandwidth billing.
New Mexico‚Äôs Gross Receipts Tax has been reduced to 4.875% from 5.125%. Learn more about tax in the United States of America.

New Mexico‚Äôs Gross Receipts Tax has been reduced to 4.875% from 5.125%. Learn more about tax in the United States of America.
South Dakota‚Äôs Retail Sales and Use Tax has reduced from 4.5% to 4.2%. Learn more about tax in the United States of America.

South Dakota‚Äôs Retail Sales and Use Tax has reduced from 4.5% to 4.2%. Learn more about tax in the United States of America.
Tax collection for Tanzania has begun. Learn more about Tanzania taxes.

Tax collection for Tanzania has begun. Learn more about Tanzania taxes.
Debian 12 (debian-12-x64) base image is now available in the control panel and via the API.

Debian 12 (debian-12-x64) base image is now available in the control panel and via the API.
Rocky 8.4 and 8.5 have reached end of life. Per our image deprecation policy, these images are now only available via the API. We will remove the Rocky 8.4 and 8.5 images from our platform in 30 days.

Rocky 8.4 and 8.5 have reached end of life. Per our image deprecation policy, these images are now only available via the API. We will remove the Rocky 8.4 and 8.5 images from our platform in 30 days.
The Domains and DNS management service now only allows you to add domains with known top-level domains (TLDs) publicly recognized by ICANN.

The Domains and DNS management service now only allows you to add domains with known top-level domains (TLDs) publicly recognized by ICANN.
Tax collection for Indonesia has begun. Learn more about taxes in Indonesia.

Tax collection for Indonesia has begun. Learn more about taxes in Indonesia.
Tax collection for IaaS and PaaS services has begun for customers in Rhode Island. Learn more about United States of America taxes.

Tax collection for IaaS and PaaS services has begun for customers in Rhode Island. Learn more about United States of America taxes.
PostgreSQL 15 is now available for database clusters. You can also now perform in-place upgrades for PostgreSQL clusters to newer versions without any downtime. We currently support PostgreSQL 12, 13, 14, and 15.

PostgreSQL 15 is now available for database clusters. You can also now perform in-place upgrades for PostgreSQL clusters to newer versions without any downtime. We currently support PostgreSQL 12, 13, 14, and 15.
Premium Intel CPUs are now available for CPU-Optimized Droplets in TOR1.

Premium Intel CPUs are now available for CPU-Optimized Droplets in TOR1.
Released v1.94.0 of doctl, the official DigitalOcean CLI. This release updates the doctl auth init prompt and deprecates the --algorithm flag for load balancer sub-commands.

Released v1.94.0 of doctl, the official DigitalOcean CLI. This release updates the doctl auth init prompt and deprecates the --algorithm flag for load balancer sub-commands.
Fedora 38 (fedora-38-x64) base image is now available in the control panel and via the API.

Fedora 38 (fedora-38-x64) base image is now available in the control panel and via the API.
DigitalOcean Functions now supports functions written in Go 1.20, PHP 8.2, and Python 3.11.
Visit the Functions documentation to learn more about which runtimes are available.

DigitalOcean Functions now supports functions written in Go 1.20, PHP 8.2, and Python 3.11.
Visit the Functions documentation to learn more about which runtimes are available.
We have updated the following buildpacks:

Hugo buildpack: The default version of Hugo has been updated from v0.109.0 to v0.111.3. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.
Go buildpack: Additional Go versions have been added and default versions of Go have been updated. For more information and configuration options, see the buildpack‚Äôs documentation page.

Add go1.20, go1.20.1, and go1.20.2
Add go1.19.4, go1.19.5, go1.19.6, and go1.19.7
Add go1.18.9, go1.18.10
go1.20 defaults to 1.20.2
go1.19 defaults to 1.19.7
go1.18 defaults to go1.18.10


PHP buildpack: Updates to the PHP v1 buildpack are listed below. If you have an existing PHP app that is on v0, please upgrade to v1.

PHP buildpack v1:

Add PHP/8.1.17
Add PHP/8.0.28




Python buildpack: A new Python v2 buildpack version has been released that removes support for Python 3.6. Updates to the Python v1 buildpack are listed below. If you have an existing Python app that is on v1 or v0, please upgrade to v2.

Python buildpack v2:

Drop support for Python 3.6
Add Python 3.10.11, 3.10.10, 3.11.3, and 3.11.2
Default Python version is now 3.11.3


Python buildpack v1:

Add Python 3.10.10 and 3.11.2
Default Python version is now 3.11.2






We have updated the following buildpacks:
Premium Intel CPUs are now available for CPU-Optimized Droplets in BLR1.

Premium Intel CPUs are now available for CPU-Optimized Droplets in BLR1.
You can now remap and redirect URL paths in your apps on App Platform. For example, if you have the existing path /your-app/api/functions/js/post in your app, you can create a rewrite that masks that path with the simpler path, /your-app/api/post. Or you can redirect traffic from a specified path to a different URL on the internet.
Additionally, app routing information is now specified under the ingress stanza of app specs.

You can now remap and redirect URL paths in your apps on App Platform. For example, if you have the existing path /your-app/api/functions/js/post in your app, you can create a rewrite that masks that path with the simpler path, /your-app/api/post. Or you can redirect traffic from a specified path to a different URL on the internet.
Additionally, app routing information is now specified under the ingress stanza of app specs.
The largest CPU-Optimized Droplet plan is now available in BLR1.

The largest CPU-Optimized Droplet plan is now available in BLR1.
We have extended the promotional period for CPU-Optimized Droplets with Premium Intel CPUs (no billing for outbound data transfer at speeds faster than 2 Gbps) from 30 April 2023 to 30 June 2023. Learn more about bandwidth billing.

We have extended the promotional period for CPU-Optimized Droplets with Premium Intel CPUs (no billing for outbound data transfer at speeds faster than 2 Gbps) from 30 April 2023 to 30 June 2023. Learn more about bandwidth billing.
Premium Intel CPUs are now available for CPU-Optimized Droplets in SFO2.

Premium Intel CPUs are now available for CPU-Optimized Droplets in SFO2.
MongoDB 6.0 is now available in the control panel and via the API. To upgrade your MongoDB cluster to version 6.0, see our guide on upgrading your database cluster

MongoDB 6.0 is now available in the control panel and via the API. To upgrade your MongoDB cluster to version 6.0, see our guide on upgrading your database cluster
We have finished rolling out NVMe for volumes in all regions. Newly-created volumes in all regions are now on NVMe-based storage.

We have finished rolling out NVMe for volumes in all regions. Newly-created volumes in all regions are now on NVMe-based storage.
Spaces now automatically delete any incomplete multipart uploads older than 90 days to prevent billing and to free up storage.

Spaces now automatically delete any incomplete multipart uploads older than 90 days to prevent billing and to free up storage.
Premium Intel CPUs are now available for CPU-Optimized Droplets. You can create CPU-Optimized Droplets with Premium Intel CPUs in NYC1, NYC3, FRA1, AMS3, SFO3, and SYD1.
Compared to CPU-Optimized Droplets with Regular Intel CPUs, CPU-Optimized Droplets with Premium Intel CPUs have the latest hardware and five times more network throughput.
Additionally, for a promotional period from 1 February through 30 April 2023, we will not bill for outbound data transfer at speeds faster than 2 Gbps for CPU-Optimized Droplets with Premium Intel CPUs. Learn more about bandwidth billing.
You can use this plan for both standalone Droplets and Kubernetes nodes. You can also resize your existing Droplets to this node plan.

Premium Intel CPUs are now available for CPU-Optimized Droplets. You can create CPU-Optimized Droplets with Premium Intel CPUs in NYC1, NYC3, FRA1, AMS3, SFO3, and SYD1.
Compared to CPU-Optimized Droplets with Regular Intel CPUs, CPU-Optimized Droplets with Premium Intel CPUs have the latest hardware and five times more network throughput.
Additionally, for a promotional period from 1 February through 30 April 2023, we will not bill for outbound data transfer at speeds faster than 2 Gbps for CPU-Optimized Droplets with Premium Intel CPUs. Learn more about bandwidth billing.
You can use this plan for both standalone Droplets and Kubernetes nodes. You can also resize your existing Droplets to this node plan.
Newer Spaces buckets now have an improved limit of 800 total operations per second. To check whether a bucket has this new limit, see our Spaces rate limits.

Newer Spaces buckets now have an improved limit of 800 total operations per second. To check whether a bucket has this new limit, see our Spaces rate limits.
We have deprecated our legacy load balancer scaling system in all datacenter regions. This includes the deprecation of the do-loadbalancer-size-slug annotation for DigitalOcean Kubernetes load balancers.
Horizontal scaling is now available in all regions.

We have deprecated our legacy load balancer scaling system in all datacenter regions. This includes the deprecation of the do-loadbalancer-size-slug annotation for DigitalOcean Kubernetes load balancers.
Horizontal scaling is now available in all regions.
DigitalOcean has acquired SnapShooter, a backup and recovery solutions provider. Learn more in our blog post.

DigitalOcean has acquired SnapShooter, a backup and recovery solutions provider. Learn more in our blog post.
Released v1.92.0 of doctl, the official DigitalOcean CLI. This release updates the doctl auth init prompt and deprecates the --algorithm flag for load balancer sub-commands.

Released v1.92.0 of doctl, the official DigitalOcean CLI. This release updates the doctl auth init prompt and deprecates the --algorithm flag for load balancer sub-commands.
Fedora 35 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 35 image from our platform in 30 days.

Fedora 35 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 35 image from our platform in 30 days.
The tax rate for Englewood, Colorado in the United States of America has increased from 3.5% to 3.8%. Learn more about USA taxes.

The tax rate for Englewood, Colorado in the United States of America has increased from 3.5% to 3.8%. Learn more about USA taxes.
The Luxembourg Tax Authorities (LTA) temporarily decreased the VAT rate from 17% to 16%. We have begun charging the adjusted VAT rate to private individuals (B2C sales) located in Luxembourg, which will be visible on invoices issues on 1 February 2023. Learn more about EU taxes.

The Luxembourg Tax Authorities (LTA) temporarily decreased the VAT rate from 17% to 16%. We have begun charging the adjusted VAT rate to private individuals (B2C sales) located in Luxembourg, which will be visible on invoices issues on 1 February 2023. Learn more about EU taxes.
The Goods and Services Tax (GST) rate for Singapore has increased from 7% to 8%. Learn more about Singapore taxes.

The Goods and Services Tax (GST) rate for Singapore has increased from 7% to 8%. Learn more about Singapore taxes.
Released v1.91.0 of doctl, the official DigitalOcean CLI. This release adds support for creating and updating firewall rules for load balancers.

Released v1.91.0 of doctl, the official DigitalOcean CLI. This release adds support for creating and updating firewall rules for load balancers.
RancherOS is now fully deprecated on our platform and is no longer available in the control panel or API.

RancherOS is now fully deprecated on our platform and is no longer available in the control panel or API.
Released v1.89.0 of doctl, the official DigitalOcean CLI. This release adds support for creating serverless namespaces in the syd1 region and creating monitoring alert policies for load balancer metrics.

Released v1.89.0 of doctl, the official DigitalOcean CLI. This release adds support for creating serverless namespaces in the syd1 region and creating monitoring alert policies for load balancer metrics.
You can now customize the amount of time a load balancer allows HTTP connections to remain idle before closing it. The maximum amount time you can set is 600 seconds (10 minutes).
Setting a custom time out length has no effect on HTTPS and HTTP/2 forwarding rules using TLS passthrough.

You can now customize the amount of time a load balancer allows HTTP connections to remain idle before closing it. The maximum amount time you can set is 600 seconds (10 minutes).
Setting a custom time out length has no effect on HTTPS and HTTP/2 forwarding rules using TLS passthrough.
DigitalOcean Load Balancers and DOKS load balancers now support the HTTP/3 protocol.

DigitalOcean Load Balancers and DOKS load balancers now support the HTTP/3 protocol.
Released v1.88.0 of doctl, the official DigitalOcean CLI. This release adds a flag to the load balancer create command that allows you to configure its HTTP idle timeout.

Released v1.88.0 of doctl, the official DigitalOcean CLI. This release adds a flag to the load balancer create command that allows you to configure its HTTP idle timeout.
We have updated the following buildpacks:


Hugo buildpack: The default version of Hugo has been updated from v0.101.0 to v0.104.3. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.


Go buildpack: Additional Go versions have been added and default versions of Go have been updated. For more information and configuration options, see the buildpack‚Äôs documentation page.

Add go1.19
Add go1.19.1
Add go1.18.6
go1.18 defaults to 1.18.6
go1.19 defaults to 1.19.1



Python buildpack: A new Python v1 buildpack has been released alongside the current v0 buildpack. Existing Python apps will remain on v0, while new apps will start using v1. If you have an existing Python app, see: How to Upgrade Buildpacks in App Platform.

Python buildpack v1:

Python 3.7.15, 3.8.15, 3.9.15 and 3.10.8 are now available
The default Python version for new apps is now 3.10.8 (previously 3.10.7)
Drop support for Python 2.7, 3.4 and 3.5
Drop support for PyPy
Python 3.7.14, 3.8.14 and 3.9.14 are now available





PHP buildpack: A new PHP v1 buildpack has been released alongside the current v0 buildpack. Existing PHP apps will remain on v0, while new apps will start using v1. If you have an existing PHP app, see: How to Upgrade Buildpacks in App Platform.

PHP buildpack v1:

Add PHP/7.4.32
Add PHP/8.0.24
Add PHP/8.1.11
Add Composer/2.4.2
Drop support for Composer/1.x.x





Ruby buildpack: A new Ruby v1 buildpack has been released alongside the current v0 buildpack. Existing Ruby apps will remain on v0, while new apps will start using v1. If you have an existing Ruby app, see: How to Upgrade Buildpacks in App Platform.

Ruby buildpack v1:

Default Ruby version is now 3.1.2






We have updated the following buildpacks:
Hugo buildpack: The default version of Hugo has been updated from v0.101.0 to v0.104.3. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.
Go buildpack: Additional Go versions have been added and default versions of Go have been updated. For more information and configuration options, see the buildpack‚Äôs documentation page.
Python buildpack: A new Python v1 buildpack has been released alongside the current v0 buildpack. Existing Python apps will remain on v0, while new apps will start using v1. If you have an existing Python app, see: How to Upgrade Buildpacks in App Platform.
PHP buildpack: A new PHP v1 buildpack has been released alongside the current v0 buildpack. Existing PHP apps will remain on v0, while new apps will start using v1. If you have an existing PHP app, see: How to Upgrade Buildpacks in App Platform.
Ruby buildpack: A new Ruby v1 buildpack has been released alongside the current v0 buildpack. Existing Ruby apps will remain on v0, while new apps will start using v1. If you have an existing Ruby app, see: How to Upgrade Buildpacks in App Platform.
The Fedora 37 (fedora-37-x64) base image is now available in the control panel and via the API.

The Fedora 37 (fedora-37-x64) base image is now available in the control panel and via the API.
Released v1.87.0 of doctl, the official DigitalOcean CLI. This release promotes the option to enable high availability on existing Kubernetes clusters to General Availability. It also adds a flag that allows you to add a load balancer to a specified project upon its creation.

Released v1.87.0 of doctl, the official DigitalOcean CLI. This release promotes the option to enable high availability on existing Kubernetes clusters to General Availability. It also adds a flag that allows you to add a load balancer to a specified project upon its creation.
DigitalOcean Kubernetes clusters originally created with version 1.20 or older have an outdated version of our control plane architecture, which does not allow you to enable high availability. However, you can now upgrade your control plane to our new version. This upgrade option is available for Kubernetes versions currently 1.22 and later.
To check whether you can upgrade your cluster to the new control plane, see our guide.

DigitalOcean Kubernetes clusters originally created with version 1.20 or older have an outdated version of our control plane architecture, which does not allow you to enable high availability. However, you can now upgrade your control plane to our new version. This upgrade option is available for Kubernetes versions currently 1.22 and later.
To check whether you can upgrade your cluster to the new control plane, see our guide.
You can now enable high availability on existing Kubernetes clusters. For detailed steps, see our guide.

You can now enable high availability on existing Kubernetes clusters. For detailed steps, see our guide.
We have launched the Sydney, Australia (syd1) datacenter region, which supports most Droplet types, managed databases, and other products. Learn more in the regional availability matrix.

We have launched the Sydney, Australia (syd1) datacenter region, which supports most Droplet types, managed databases, and other products. Learn more in the regional availability matrix.
Released v1.86.0 of doctl, the official DigitalOcean CLI. This release includes new doctl apps list-buildpacks and doctl apps upgrade-buildpack subcommands allowing you to manually upgrade an app‚Äôs buildpacks to their latest major versions.

Released v1.86.0 of doctl, the official DigitalOcean CLI. This release includes new doctl apps list-buildpacks and doctl apps upgrade-buildpack subcommands allowing you to manually upgrade an app‚Äôs buildpacks to their latest major versions.
The DigitalOcean API now accepts the YAML content-type when submitting app specs for App Platform.

The DigitalOcean API now accepts the YAML content-type when submitting app specs for App Platform.
All Spaces rate limits have increased to double their previous amount. For a list of the current rate limits, see our Limits page.

All Spaces rate limits have increased to double their previous amount. For a list of the current rate limits, see our Limits page.
AlmaLinux OS versions 8.6 and 9 base images are now available in the control panel and via the API.

AlmaLinux OS versions 8.6 and 9 base images are now available in the control panel and via the API.
Ubuntu 22.10 (ubuntu-22-10-x64) base image is now available in the control panel and via the API.

Ubuntu 22.10 (ubuntu-22-10-x64) base image is now available in the control panel and via the API.
Released v1.84.0 of doctl, the official DigitalOcean CLI. This release adds a --wait flag to the doctl database create subcommand.

Released v1.84.0 of doctl, the official DigitalOcean CLI. This release adds a --wait flag to the doctl database create subcommand.
Premium AMD Droplets now also include servers powered by third generation AMD EPYC processors.

Premium AMD Droplets now also include servers powered by third generation AMD EPYC processors.
When creating a new Kubernetes cluster, you can add a free database operator (now in beta), which allows you to automatically link new databases to your cluster. For more details, see our guide.

When creating a new Kubernetes cluster, you can add a free database operator (now in beta), which allows you to automatically link new databases to your cluster. For more details, see our guide.
do-operator, our operator for managing and consuming DigitalOcean resources from a Kubernetes cluster, is now an open-source beta project.

do-operator, our operator for managing and consuming DigitalOcean resources from a Kubernetes cluster, is now an open-source beta project.
The IOPS and throughput limits for volumes are now 50% higher. For a list of the new limits by Droplet type, see our updated limits page. To reach the new limits, you must power cycle the Droplet with the attached volume or detach and reattach the volume.

The IOPS and throughput limits for volumes are now 50% higher. For a list of the new limits by Droplet type, see our updated limits page. To reach the new limits, you must power cycle the Droplet with the attached volume or detach and reattach the volume.
Released v1.83.0 of doctl, the official DigitalOcean CLI. This release adds updated confirmation dialogs and a --wait flag to the doctl compute load-balancer create subcommand.

Released v1.83.0 of doctl, the official DigitalOcean CLI. This release adds updated confirmation dialogs and a --wait flag to the doctl compute load-balancer create subcommand.
Tax collection for Kazakhstan has begun. Learn more about taxes in Kazakhstan.

Tax collection for Kazakhstan has begun. Learn more about taxes in Kazakhstan.
DigitalOcean Functions now support a maximum timeout of 15 minutes.
Longer timeouts enable functions to handle more complex and compute-intensive tasks such as video and image processing, data transformation, and report generation.
Visit the Functions documentation to learn more about creating and working with long-running functions.

DigitalOcean Functions now support a maximum timeout of 15 minutes.
Longer timeouts enable functions to handle more complex and compute-intensive tasks such as video and image processing, data transformation, and report generation.
Visit the Functions documentation to learn more about creating and working with long-running functions.
DigitalOcean Functions now has limited beta access to scheduled triggers.
Scheduled function triggers allow you to set a cron-like schedule for running your function. An optional payload may be included with each invocation. Each function may have multiple triggers with different schedules and payloads.
Visit How to Schedule Functions to learn more about creating and working with scheduled triggers from the command line or control panel interface.

DigitalOcean Functions now has limited beta access to scheduled triggers.
Scheduled function triggers allow you to set a cron-like schedule for running your function. An optional payload may be included with each invocation. Each function may have multiple triggers with different schedules and payloads.
Visit How to Schedule Functions to learn more about creating and working with scheduled triggers from the command line or control panel interface.
Released v1.82.0 of doctl, the official DigitalOcean CLI. This release upgrades godo to v1.86.0 and adds support for building App Platform apps locally.

Released v1.82.0 of doctl, the official DigitalOcean CLI. This release upgrades godo to v1.86.0 and adds support for building App Platform apps locally.
Users who sign up for DigitalOcean through the referral program now receive a $200 account credit, increased from $100.

Users who sign up for DigitalOcean through the referral program now receive a $200 account credit, increased from $100.
We have added the project_id field to the Reserved IP service‚Äôs API. The project_id field allows you to create and associate Reserved IPs with a DigitalOcean Project.
Use the project_id and region fields in a Create a new Reserved IP request to create a Reserved IP within a project. You can assign the IP address to a Droplet later using a Reserved IP action request.
We have added the project_id field to the following Reserved IP API responses:

Create a new Reserved IP (POST)
List Reserved IPs (GET)
Retrive an existing Reserved IP (GET)
Initiate a Reserved IP action (POST)


We have added the project_id field to the Reserved IP service‚Äôs API. The project_id field allows you to create and associate Reserved IPs with a DigitalOcean Project.
Use the project_id and region fields in a Create a new Reserved IP request to create a Reserved IP within a project. You can assign the IP address to a Droplet later using a Reserved IP action request.
We have added the project_id field to the following Reserved IP API responses:
App Platform now supports automatically re-deploying apps when updated container images are pushed to DigitalOcean Container Registry. See How to Deploy from Container Images for more information.

App Platform now supports automatically re-deploying apps when updated container images are pushed to DigitalOcean Container Registry. See How to Deploy from Container Images for more information.
DigitalOcean Functions now has support for multiple namespaces.
Namespaces are a level of isolation and organization for functions. They allow you to isolate functions by project, by environment (production versus development, for example), by region, or by any other grouping that facilitates your development workflow.
Visit the Functions documentation to learn more about creating and working with multiple namespaces.

DigitalOcean Functions now has support for multiple namespaces.
Namespaces are a level of isolation and organization for functions. They allow you to isolate functions by project, by environment (production versus development, for example), by region, or by any other grouping that facilitates your development workflow.
Visit the Functions documentation to learn more about creating and working with multiple namespaces.
When you create a Droplet using the API (POST /v2/droplets), you can now specify a region (like NYC) instead of a specific datacenter (like NYC3). The API then creates your Droplet in any available datacenter within your specified region. For example, if you want to create a Droplet in San Francisco, you can use the region sfo to guarantee that the Droplet will be in SFO1, SFO2, or SFO3. Additionally, you can omit the region entirely (or set it to an empty string) to create a Droplet in any available region.

When you create a Droplet using the API (POST /v2/droplets), you can now specify a region (like NYC) instead of a specific datacenter (like NYC3). The API then creates your Droplet in any available datacenter within your specified region. For example, if you want to create a Droplet in San Francisco, you can use the region sfo to guarantee that the Droplet will be in SFO1, SFO2, or SFO3. Additionally, you can omit the region entirely (or set it to an empty string) to create a Droplet in any available region.
Released v1.80.0 of doctl, the official DigitalOcean CLI. This release adds support for creating and managing multiple namespaces for serverless functions.

Released v1.80.0 of doctl, the official DigitalOcean CLI. This release adds support for creating and managing multiple namespaces for serverless functions.
Released v1.81.0 of doctl, the official DigitalOcean CLI. This release adds two new pieces of functionality. When creating a reserved IP, you can now specify the project it should be placed in. It also makes the --region flag an optional argument for the compute droplet create sub-command.

Released v1.81.0 of doctl, the official DigitalOcean CLI. This release adds two new pieces of functionality. When creating a reserved IP, you can now specify the project it should be placed in. It also makes the --region flag an optional argument for the compute droplet create sub-command.
Rocky 9 (rockylinux-9-x64) base image is now available in the control panel and via the API.

Rocky 9 (rockylinux-9-x64) base image is now available in the control panel and via the API.
DigitalOcean Uptime is now in general availability.
Uptime is a monitoring service that checks the health of any URL or IP address. You can use it to monitor the latency, uptime, and SSL certificate of any website or host, and can choose to receive alerts via email or Slack when your site is down, experiencing high latency, or has an SSL certificate that‚Äôs about to expire. Learn more about Uptime.

DigitalOcean Uptime is now in general availability.
Uptime is a monitoring service that checks the health of any URL or IP address. You can use it to monitor the latency, uptime, and SSL certificate of any website or host, and can choose to receive alerts via email or Slack when your site is down, experiencing high latency, or has an SSL certificate that‚Äôs about to expire. Learn more about Uptime.
Redis 7.0 is now available when creating new databases. You can no longer create Redis 6.0 clusters. On 5 November 2022, we will officially no longer support 6.0 and will automatically upgrade all existing clusters to 7.0, with no expected downtime or interruptions.

Redis 7.0 is now available when creating new databases. You can no longer create Redis 6.0 clusters. On 5 November 2022, we will officially no longer support 6.0 and will automatically upgrade all existing clusters to 7.0, with no expected downtime or interruptions.
To comply with Kenya‚Äôs Revenue Authority amendments in Finance Act 2022, we now charge VAT to B2B and B2C customers in Kenya. Learn more about Kenya taxes.

To comply with Kenya‚Äôs Revenue Authority amendments in Finance Act 2022, we now charge VAT to B2B and B2C customers in Kenya. Learn more about Kenya taxes.
Tax collection for Liechtenstein has begun. Learn more about taxes for Switzerland and Liechtenstein.

Tax collection for Liechtenstein has begun. Learn more about taxes for Switzerland and Liechtenstein.
In App Platform, you can now create bindable environment variables for your PostgreSQL database connection pools. For detailed instructions, see our reference page.

In App Platform, you can now create bindable environment variables for your PostgreSQL database connection pools. For detailed instructions, see our reference page.
A new CPU-Optimized Droplet plan with more computing power is now available. This plan features 48 vCPUs (up from the previous maximum of 32) and 96 GB of memory (up from the previous maximum of 64).
This large CPU-Optimized Droplet plan is available where CPU-Optimized Droplets are already available, except for BLR1 and SFO2.
You can use this plan for both standalone Droplets and Kubernetes nodes. You can also resize your existing Droplets to this node plan.

A new CPU-Optimized Droplet plan with more computing power is now available. This plan features 48 vCPUs (up from the previous maximum of 32) and 96 GB of memory (up from the previous maximum of 64).
This large CPU-Optimized Droplet plan is available where CPU-Optimized Droplets are already available, except for BLR1 and SFO2.
You can use this plan for both standalone Droplets and Kubernetes nodes. You can also resize your existing Droplets to this node plan.
Released v1.79.0 of doctl, the official DigitalOcean CLI. This release adds several databases options sub-commands you can use to look up create-time options for database clusters, such as supported engines and versions.

Released v1.79.0 of doctl, the official DigitalOcean CLI. This release adds several databases options sub-commands you can use to look up create-time options for database clusters, such as supported engines and versions.
Debian 9 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Debian 9 image from our platform in 30 days.

Debian 9 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Debian 9 image from our platform in 30 days.
Ubuntu 21.10 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 21.10 image from our platform in 30 days.

Ubuntu 21.10 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 21.10 image from our platform in 30 days.
Rocky 8.6 (rockylinux-8-x64) base image is now available in the control panel and via the API.

Rocky 8.6 (rockylinux-8-x64) base image is now available in the control panel and via the API.
You can now upgrade your App Platform app‚Äôs and its components‚Äô buildpacks to their latest version. For detailed steps, see our guide, How to Upgrade Buildpacks in App Platform.

You can now upgrade your App Platform app‚Äôs and its components‚Äô buildpacks to their latest version. For detailed steps, see our guide, How to Upgrade Buildpacks in App Platform.
Tax collection for Cambodia has begun. Learn more about Cambodia taxes.

Tax collection for Cambodia has begun. Learn more about Cambodia taxes.
You can now set up automatic recurring payments with PayPal. You can still log in to make one time payments with PayPal. Learn more about how to pay your bill.

You can now set up automatic recurring payments with PayPal. You can still log in to make one time payments with PayPal. Learn more about how to pay your bill.
We have updated the following buildpacks:


Hugo buildpack: The default version of Hugo has been updated from 0.99.1 to 0.101.0. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.


Go buildpack: Additional Go versions have been added and default versions of Go have been updated. For more information and configuration options, see the buildpack‚Äôs documentation page.

Add go1.17.9
Add go1.17.10
Add go1.18.1
Add go1.18.2
Add go1.18.3
go1.18 defaults to 1.18.3
go1.17 defaults to 1.17.10




We have updated the following buildpacks:
Hugo buildpack: The default version of Hugo has been updated from 0.99.1 to 0.101.0. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.
Go buildpack: Additional Go versions have been added and default versions of Go have been updated. For more information and configuration options, see the buildpack‚Äôs documentation page.
Spaces and the Spaces CDN now support HTTP/2 clients. HTTP/2-conformant clients now receive HTTP/2 responses, while others receive HTTP/1.1 responses. In certain cases, such as when an HTTP/2 request has a formatting error, it may downgrade to HTTP/1.1 for operational reasons, as permitted by the HTTP/2 specification.

Spaces and the Spaces CDN now support HTTP/2 clients. HTTP/2-conformant clients now receive HTTP/2 responses, while others receive HTTP/1.1 responses. In certain cases, such as when an HTTP/2 request has a formatting error, it may downgrade to HTTP/1.1 for operational reasons, as permitted by the HTTP/2 specification.
Newly-created volumes in NYC1, NYC3, SFO2, SFO3, FRA1, SGP1, LON1, and AMS3 are now on NVMe-based storage. Most existing volumes and volumes in BLR1 and TOR1 remain on SSD-based storage. We‚Äôre continuing to roll out NVMe across all volumes in all regions. In the interim, you can migrate volumes using rsync or similar tools to copy data.

Newly-created volumes in NYC1, NYC3, SFO2, SFO3, FRA1, SGP1, LON1, and AMS3 are now on NVMe-based storage. Most existing volumes and volumes in BLR1 and TOR1 remain on SSD-based storage. We‚Äôre continuing to roll out NVMe across all volumes in all regions. In the interim, you can migrate volumes using rsync or similar tools to copy data.
The DigitalOcean API now supports listing Droplets by name by using the name query parameter, as in GET /v2/droplets?name="your_droplet_name". Learn more in the API documentation.

The DigitalOcean API now supports listing Droplets by name by using the name query parameter, as in GET /v2/droplets?name="your_droplet_name". Learn more in the API documentation.
Fedora 34 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 34 image from our platform in 30 days.

Fedora 34 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 34 image from our platform in 30 days.
The following pricing changes are now in effect:


A new $4 Droplet with 512MB of memory, 10GB of storage, 1 vCPU, and 500GB of outbound data transfer is now available in NYC1, FRA1, SFO3, SGP1, and AMS3. The slug is s-1vcpu-512mb-10gb.


We have simplified pricing for DigitalOcean Kubernetes and some Managed Databases for better accuracy and predictibility.


The prices of Droplets, Snapshots, Load Balancers, Reserved IPs, and Custom Images have increased.


There is no change to pricing for Spaces, backups, volumes, DigitalOcean Container Registry, or App Platform. There are also no changes to inbound data transfer or bandwidth pricing.
This is our first major price change in 10 years, and we believe the new model better fits our understanding of our customers and the expanded breadth of our offerings. For a more detailed breakdown of the changes, see our blog post on our new pricing.

The following pricing changes are now in effect:
A new $4 Droplet with 512MB of memory, 10GB of storage, 1 vCPU, and 500GB of outbound data transfer is now available in NYC1, FRA1, SFO3, SGP1, and AMS3. The slug is s-1vcpu-512mb-10gb.
We have simplified pricing for DigitalOcean Kubernetes and some Managed Databases for better accuracy and predictibility.
The prices of Droplets, Snapshots, Load Balancers, Reserved IPs, and Custom Images have increased.
There is no change to pricing for Spaces, backups, volumes, DigitalOcean Container Registry, or App Platform. There are also no changes to inbound data transfer or bandwidth pricing.
This is our first major price change in 10 years, and we believe the new model better fits our understanding of our customers and the expanded breadth of our offerings. For a more detailed breakdown of the changes, see our blog post on our new pricing.
Taxes for New Mexico in the United States of America have decreased to 5%. Learn more about taxes in the United States of America.

Taxes for New Mexico in the United States of America have decreased to 5%. Learn more about taxes in the United States of America.
Tax collection for Nigeria has begun. Learn more about Nigeria taxes..

Tax collection for Nigeria has begun. Learn more about Nigeria taxes..
Released v1.78.0 of doctl, the official DigitalOcean CLI. This release renames the sandbox commands to serverless. Aliases are provided for backwards compatibility. Additionally, the output of the account get command now includes the name of the active team.

Released v1.78.0 of doctl, the official DigitalOcean CLI. This release renames the sandbox commands to serverless. Aliases are provided for backwards compatibility. Additionally, the output of the account get command now includes the name of the active team.
In order to improve security, DigitalOcean no longer accepts TLS 1.0 and TLS 1.1 connections. This includes connections to www.digitalocean.com, cloud.digitalocean.com, and api.digitalocean.com.

In order to improve security, DigitalOcean no longer accepts TLS 1.0 and TLS 1.1 connections. This includes connections to www.digitalocean.com, cloud.digitalocean.com, and api.digitalocean.com.
High-availability control plane is now Generally Available in all regions where DigitalOcean Kubernetes is supported.

High-availability control plane is now Generally Available in all regions where DigitalOcean Kubernetes is supported.
SMTP (port 25) is now blocked for all new accounts. Learn more about SMTP blocking.

SMTP (port 25) is now blocked for all new accounts. Learn more about SMTP blocking.
We have renamed the Floating IP product to Reserved IPs. The Reserved IP service retains the same functionality as the prior service.
We have added new API endpoints and fields (reserved_ips) to reflect the name change, but the service‚Äôs original Floating IP endpoints and fields (floating_ips) will remain available until the fall of 2023. Please update any automation, scripts, or services that use these endpoints to reflect these changes.
If you are using the Projects API to query Reserved IP resources, the endpoint still returns reserved IP addresses in the floating_ips field.

We have renamed the Floating IP product to Reserved IPs. The Reserved IP service retains the same functionality as the prior service.
We have added new API endpoints and fields (reserved_ips) to reflect the name change, but the service‚Äôs original Floating IP endpoints and fields (floating_ips) will remain available until the fall of 2023. Please update any automation, scripts, or services that use these endpoints to reflect these changes.
If you are using the Projects API to query Reserved IP resources, the endpoint still returns reserved IP addresses in the floating_ips field.
Released v1.77.0 of doctl, the official DigitalOcean CLI. This release deprecates the floating-ip commands in favor of the new reserved-ip ones.

Released v1.77.0 of doctl, the official DigitalOcean CLI. This release deprecates the floating-ip commands in favor of the new reserved-ip ones.
UDP support is now available for all DigitalOcean Load Balancers. This includes UDP support for DOKS load balancers.
To use UDP for DOKS load balancers, clusters must use Kubernetes version 1.21.11-do.1, 1.22.8-do.1, or higher.

UDP support is now available for all DigitalOcean Load Balancers. This includes UDP support for DOKS load balancers.
To use UDP for DOKS load balancers, clusters must use Kubernetes version 1.21.11-do.1, 1.22.8-do.1, or higher.
You can now cancel a Space‚Äôs scheduled deletion in the control panel. For more details, see How to Destroy Spaces.

You can now cancel a Space‚Äôs scheduled deletion in the control panel. For more details, see How to Destroy Spaces.
When creating a Droplet via the API, we now release the Droplet‚Äôs IP address when it is in the active state, instead of the new state.

When creating a Droplet via the API, we now release the Droplet‚Äôs IP address when it is in the active state, instead of the new state.
When creating an app, you can add the app to a project. If you do not specify a project, it gets assigned to the default project. You can also move an app between projects.

When creating an app, you can add the app to a project. If you do not specify a project, it gets assigned to the default project. You can also move an app between projects.
MongoDB 5.0 is now available in the control panel and via the API. To upgrade your MongoDB cluster to version 5.0, see our guide on upgrading your database cluster

MongoDB 5.0 is now available in the control panel and via the API. To upgrade your MongoDB cluster to version 5.0, see our guide on upgrading your database cluster
Starter tier apps now support rolling back to a previous deployment.

Starter tier apps now support rolling back to a previous deployment.
We have deprecated the FreeBSD image on our platform. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 34 image from our platform in 30 days.

We have deprecated the FreeBSD image on our platform. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 34 image from our platform in 30 days.
We have updated the following buildpacks:


Hugo buildpack: The default version of Hugo has been updated from 0.94.2 to 0.99.1. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.


Python buildpack: We have updated the default versions of the following platform tooling:

Updated pip from 21.3.1 to 22.0.4 for Python 3.7+
Updated setuptools from 57.5.0 to 59.6.0 for Python 3.6 and 60.10.0 for Python 3.7+
Updated wheel from 0.37.0 to 0.37.1 for Python 2.7 and Python 3.5+

Python 3.9.13 is now available. The default version remains set to 3.10.4. You can configure the Python version used at runtime by specifying a runtime.txt file at the root of your source code. For more information and configuration options, see the Python Dev Guide.



We have updated the following buildpacks:
Hugo buildpack: The default version of Hugo has been updated from 0.94.2 to 0.99.1. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.
Python buildpack: We have updated the default versions of the following platform tooling:
Python 3.9.13 is now available. The default version remains set to 3.10.4. You can configure the Python version used at runtime by specifying a runtime.txt file at the root of your source code. For more information and configuration options, see the Python Dev Guide.
DigitalOcean Functions and functions components in App Platform are now in general availability.
Functions are blocks of code that run on demand in response to requests. DigitalOcean Functions let developers execute their code on DigitalOcean without managing compute resources like Droplets or Kubernetes clusters.

DigitalOcean Functions and functions components in App Platform are now in general availability.
Functions are blocks of code that run on demand in response to requests. DigitalOcean Functions let developers execute their code on DigitalOcean without managing compute resources like Droplets or Kubernetes clusters.
Released v1.76.0 of doctl, the official DigitalOcean CLI. This release adds support for our new serverless Functions product and updates godo to support new App Platform features.

Released v1.76.0 of doctl, the official DigitalOcean CLI. This release adds support for our new serverless Functions product and updates godo to support new App Platform features.
A beta of build performance improvements has been added. This functionality leverages kata-containers technology for improved speed, efficiency, and compatibility.

A beta of build performance improvements has been added. This functionality leverages kata-containers technology for improved speed, efficiency, and compatibility.
Spaces no longer supports downgrading TLS connections to TLS 1.1 or using cipher suites with SHA1 or DHE. Spaces currently returns soft S3 error messages and will gradually transition to hard TLS errors over a 4-6 week period.

Spaces no longer supports downgrading TLS connections to TLS 1.1 or using cipher suites with SHA1 or DHE. Spaces currently returns soft S3 error messages and will gradually transition to hard TLS errors over a 4-6 week period.
Released v1.75.0 of doctl, the official DigitalOcean CLI. This release contains fixes and improvements for a beta product. If you are not a member of the beta group, the new features will not be available to you at this time.

Released v1.75.0 of doctl, the official DigitalOcean CLI. This release contains fixes and improvements for a beta product. If you are not a member of the beta group, the new features will not be available to you at this time.
Released v1.74.0 of doctl, the official DigitalOcean CLI. This release introduces new functionality for a beta product. If you are not a member of the beta group, the new features will not be available to you at this time.

Released v1.74.0 of doctl, the official DigitalOcean CLI. This release introduces new functionality for a beta product. If you are not a member of the beta group, the new features will not be available to you at this time.
Marketplace Add-Ons are now generally available. Add-Ons are software as a service (SaaS) offerings from third-party vendors. Read more in the Marketplace Add-Ons blog post, browse the list of available Add-Ons, or learn more about becoming a Marketplace vendor.

Marketplace Add-Ons are now generally available. Add-Ons are software as a service (SaaS) offerings from third-party vendors. Read more in the Marketplace Add-Ons blog post, browse the list of available Add-Ons, or learn more about becoming a Marketplace vendor.
Ubuntu 22.04 LTS (ubuntu-22-04-x64) base image is now available in the control panel and via the API.

Ubuntu 22.04 LTS (ubuntu-22-04-x64) base image is now available in the control panel and via the API.
We have started rolling out UDP support for DigitalOcean Load Balancers. This includes UDP support for DOKS load balancers.

We have started rolling out UDP support for DigitalOcean Load Balancers. This includes UDP support for DOKS load balancers.
MongoDB clusters now support two Dedicated Droplet types: General Purpose and Memory-Optimized. If these options are available in your region, you can select them when creating a new cluster or resizing an existing one.

MongoDB clusters now support two Dedicated Droplet types: General Purpose and Memory-Optimized. If these options are available in your region, you can select them when creating a new cluster or resizing an existing one.
We have updated the default version of Python in the Python buildpack has been updated from 3.9.9 to 3.10.4. You can configure the Python version used at runtime by specifying a runtime.txt file at the root of your source code. For more information and configuration options, see the Python Dev Guide.

We have updated the default version of Python in the Python buildpack has been updated from 3.9.9 to 3.10.4. You can configure the Python version used at runtime by specifying a runtime.txt file at the root of your source code. For more information and configuration options, see the Python Dev Guide.
You can now add new resources and databases when you create a new App Platform app, instead of adding only to existing apps. The new app creation workflow can now detect multiple app resources.

You can now add new resources and databases when you create a new App Platform app, instead of adding only to existing apps. The new app creation workflow can now detect multiple app resources.
Tax collection has begun for several cities in the United States: Boulder, Colorado Springs, Englewood, and Fort Collins in Colorado, and Chicago in Illinois. Learn more about United States of America taxes.

Tax collection has begun for several cities in the United States: Boulder, Colorado Springs, Englewood, and Fort Collins in Colorado, and Chicago in Illinois. Learn more about United States of America taxes.
We have updated the default version of Hugo in the Hugo buildpack has been updated from 0.82.0 to 0.94.2. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.

We have updated the default version of Hugo in the Hugo buildpack has been updated from 0.82.0 to 0.94.2. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.
Released v1.73.0 of doctl, the official DigitalOcean CLI. This release updates godo to support new App Platform features.

Released v1.73.0 of doctl, the official DigitalOcean CLI. This release updates godo to support new App Platform features.
v2.19.0 of the DigitalOcean Terraform Provider is now available. This release adds custom region support of the digitalocean_container_registry resource.

v2.19.0 of the DigitalOcean Terraform Provider is now available. This release adds custom region support of the digitalocean_container_registry resource.
Basic Droplets can now have Regular AMD CPUs. Additionally, you can now change between Premium AMD and Premium Intel CPUs when resizing Droplets. Learn more about resizing Droplets and how to choose a Droplet plan.

Basic Droplets can now have Regular AMD CPUs. Additionally, you can now change between Premium AMD and Premium Intel CPUs when resizing Droplets. Learn more about resizing Droplets and how to choose a Droplet plan.
DigitalOcean API access tokens now begin with an identifiable prefix in order to
distinguish them from other similar tokens. Tokens now use the following prefixes:

dop_v1_ for personal access tokens generated in the control panel
doo_v1_ for tokens generated by application using the OAuth flow
dor_v1_ for OAuth refresh tokens


DigitalOcean API access tokens now begin with an identifiable prefix in order to
distinguish them from other similar tokens. Tokens now use the following prefixes:
Spaces no longer supports downgrading TLS connections to TLS 1.0, and will transition from returning soft S3 error messages to hard TLS errors over the next month.

Spaces no longer supports downgrading TLS connections to TLS 1.0, and will transition from returning soft S3 error messages to hard TLS errors over the next month.
Released v1.72.0 of doctl, the official DigitalOcean CLI. This release introduces new doctl compute tag apply and doctl compute tag remove commands that support using tags with multiple resources in a single operation.

Released v1.72.0 of doctl, the official DigitalOcean CLI. This release introduces new doctl compute tag apply and doctl compute tag remove commands that support using tags with multiple resources in a single operation.
Centos Stream 9 (centos-stream-9-x64) base image is now available in the control panel and via the API.

Centos Stream 9 (centos-stream-9-x64) base image is now available in the control panel and via the API.
The DigitalOcean OAuth API has been updated to include additional information when users authorize an application under a team context. The JSON bodies for both the access grant and refresh grant responses will now include team_uuid and team_name attributes inside of the info object.

The DigitalOcean OAuth API has been updated to include additional information when users authorize an application under a team context. The JSON bodies for both the access grant and refresh grant responses will now include team_uuid and team_name attributes inside of the info object.
Released v1.71.0 of doctl, the official DigitalOcean CLI. This release adds regions support to Container Registry commands including the introduction of the new doctl registry options available-regions command.

Released v1.71.0 of doctl, the official DigitalOcean CLI. This release adds regions support to Container Registry commands including the introduction of the new doctl registry options available-regions command.
You can now search for and install Kubernetes 1-Click apps from the new Marketplace tab of DOKS clusters.

You can now search for and install Kubernetes 1-Click apps from the new Marketplace tab of DOKS clusters.
High-availability control plane (early availability) is now available in all regions where DOKS is supported.

High-availability control plane (early availability) is now available in all regions where DOKS is supported.
When creating a Container Registry, you can now choose one of the following datacenter regions to host it in: NYC3, SFO3, AMS3, SGP1, and FRA1. However, you cannot change a registry‚Äôs datacenter after creation.

When creating a Container Registry, you can now choose one of the following datacenter regions to host it in: NYC3, SFO3, AMS3, SGP1, and FRA1. However, you cannot change a registry‚Äôs datacenter after creation.
v2.18.0 of the DigitalOcean Terraform Provider is now available. This release adds a new digitalocean_spaces_bucket_policy resource as well as support for configuring log destinations and alert policies in the digitalocean_app resource.

v2.18.0 of the DigitalOcean Terraform Provider is now available. This release adds a new digitalocean_spaces_bucket_policy resource as well as support for configuring log destinations and alert policies in the digitalocean_app resource.
You can now configure your MySQL, PostgreSQL, and Redis Managed Databases by making a PATCH request to /v2/databases/{database_cluster_uuid}/config. For example:
{
  "config": {
    "sql_mode": "ANSI,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,STRICT_ALL_TABLES",
    "sql_require_primary_key": true
  }
}
For more details, see the full reference documentation for the managed databases API.

You can now configure your MySQL, PostgreSQL, and Redis Managed Databases by making a PATCH request to /v2/databases/{database_cluster_uuid}/config. For example:
For more details, see the full reference documentation for the managed databases API.
PostgreSQL 14 is now available for database clusters. You can upgrade earlier versions of PostgreSQL clusters to newer versions without any downtime using the DigitalOcean Control Panel.

PostgreSQL 14 is now available for database clusters. You can upgrade earlier versions of PostgreSQL clusters to newer versions without any downtime using the DigitalOcean Control Panel.
You can now create personal access tokens with an expiry interval. After the interval passes, the token can no longer authenticate you to the API and it disappears from your account. To create tokens with expiry intervals, see How to Create a Personal Access Token.

You can now create personal access tokens with an expiry interval. After the interval passes, the token can no longer authenticate you to the API and it disappears from your account. To create tokens with expiry intervals, see How to Create a Personal Access Token.
The database online migration feature for the MySQL, PostgreSQL, and Redis database engines no longer supports migrating databases from clusters inside of DigitalOcean to other clusters inside of DigitalOcean.

The database online migration feature for the MySQL, PostgreSQL, and Redis database engines no longer supports migrating databases from clusters inside of DigitalOcean to other clusters inside of DigitalOcean.
Tax collection for Ukraine has begun. Learn more about Ukraine taxes.

Tax collection for Ukraine has begun. Learn more about Ukraine taxes.
To continue improving collaboration on DigitalOcean, we have begun incrementally converting existing customers‚Äô personal accounts to team accounts.

To continue improving collaboration on DigitalOcean, we have begun incrementally converting existing customers‚Äô personal accounts to team accounts.
Released v1.70.0 of doctl, the official DigitalOcean CLI. This release adds support for App Platform features, such as AppDomainSpec.Certificate, MinimumTLSVersion, appServiceSpecHealthCheck.Port and more.

Released v1.70.0 of doctl, the official DigitalOcean CLI. This release adds support for App Platform features, such as AppDomainSpec.Certificate, MinimumTLSVersion, appServiceSpecHealthCheck.Port and more.
We have updated the following buildpacks:

Hugo buildpack: The default version of Hugo has been updated from v0.104.3 to v0.109.0. You can override the default version by setting a HUGO_VERSION environment variable. For more information and configuration options, see the buildpack‚Äôs documentation page.
Go buildpack: Additional Go versions have been added and default versions of Go have been updated. For more information and configuration options, see the buildpack‚Äôs documentation page.

Add go1.19.2, and go1.19.3
Add go1.18.4, go1.18.5, go1.18.7, and go1.18.8
Add go1.17.11, go1.17.12, and go1.17.13
go1.19 defaults to go1.19.3
go1.18 defaults to go1.18.8
go1.17 defaults to go1.17.13


Python buildpack: Updates to the Python v1 buildpack are below. If you have an existing Python app that is on v0, please upgrade to v1, see: How to Upgrade Buildpacks in App Platform.

Python buildpack v1:

Python 3.7.16, 3.8.16, 3.9.16, 3.10.9, and 3.11.1 are now available
The default Python version for new apps is now 3.10.9 (previously 3.10.8)
Add support for Python 3.11




PHP buildpack: Updates to the PHP v1 buildpack are below. If you have an existing PHP app that is on v0, please upgrade to v1, see: How to Upgrade Buildpacks in App Platform.

PHP buildpack v1:

Add PHP/7.4.33
Add PHP/8.0.25
Add PHP/8.1.12






We have updated the following buildpacks:
All new signups on DigitalOcean can now invite teammates immediately upon creating their account.

All new signups on DigitalOcean can now invite teammates immediately upon creating their account.
Online migration is now available for the MySQL, PostgreSQL, and Redis database engines. Online migration allows you to migrate databases from external servers or cloud providers to databases in your DigitalOcean account.

Online migration is now available for the MySQL, PostgreSQL, and Redis database engines. Online migration allows you to migrate databases from external servers or cloud providers to databases in your DigitalOcean account.
Tax collection for Puerto Rico has begun. Learn more about Puerto Rico taxes.

Tax collection for Puerto Rico has begun. Learn more about Puerto Rico taxes.
Managed Let‚Äôs Encrypt certificates will begin using Elliptic Curve Digital Signature Algorithm (ECDSA) instead of RSA. ECDSA is equally secure and more computationally efficient than RSA. ECDSA certificates follow the shorter root chain and aren‚Äôt rooted using the DST Root CA X3 cross-sign which expired on 30 September 2021.
As we roll out this change, new Let‚Äôs Encrypt certificates provisioned for DigitalOcean Load Balancers and Spaces will increasingly use ECDSA and existing certificiates secured with RSA will be secured with ECDSA upon auto-renewal. This change doesn‚Äôt require any action from DigitalOcean customers.

Managed Let‚Äôs Encrypt certificates will begin using Elliptic Curve Digital Signature Algorithm (ECDSA) instead of RSA. ECDSA is equally secure and more computationally efficient than RSA. ECDSA certificates follow the shorter root chain and aren‚Äôt rooted using the DST Root CA X3 cross-sign which expired on 30 September 2021.
As we roll out this change, new Let‚Äôs Encrypt certificates provisioned for DigitalOcean Load Balancers and Spaces will increasingly use ECDSA and existing certificiates secured with RSA will be secured with ECDSA upon auto-renewal. This change doesn‚Äôt require any action from DigitalOcean customers.
FreeBSD 11.4 (zfs and ufs), Fedora 33, CentOS 8, and Ubuntu 21.04 have reached end of life. Per our image deprecation policy, these images are now only available via the API. We will remove these images from our platform in 30 days.

FreeBSD 11.4 (zfs and ufs), Fedora 33, CentOS 8, and Ubuntu 21.04 have reached end of life. Per our image deprecation policy, these images are now only available via the API. We will remove these images from our platform in 30 days.
v2.17.0 of the DigitalOcean Terraform Provider is now available. This release adds support for:

Fetching load balancer resources by ID in a datasource.
Updating the name and description for default VPC networks.

It also includes bug fixes.

v2.17.0 of the DigitalOcean Terraform Provider is now available. This release adds support for:
It also includes bug fixes.
Released v1.69.0 of doctl, the official DigitalOcean CLI. This release contains a number of bug fixes and adds support to the kubernetes cluster kubeconfig save sub-command for setting an alias for a cluster‚Äôs context name.

Released v1.69.0 of doctl, the official DigitalOcean CLI. This release contains a number of bug fixes and adds support to the kubernetes cluster kubeconfig save sub-command for setting an alias for a cluster‚Äôs context name.
You can now resize load balancers once per minute, instead of once per hour. The cost is prorated based on how long the load balancer operates at each size, with a minimum charge of $0.01.

You can now resize load balancers once per minute, instead of once per hour. The cost is prorated based on how long the load balancer operates at each size, with a minimum charge of $0.01.
All DigitalOcean databases now support App Platform apps as trusted sources,  including MongoDB.

All DigitalOcean databases now support App Platform apps as trusted sources,  including MongoDB.
Tax collection for several states and cities in the United States of America has begun. Charges will appear on the February invoice. Learn more about USA taxes.

Tax collection for several states and cities in the United States of America has begun. Charges will appear on the February invoice. Learn more about USA taxes.
We now support adding Google Pay as a stored payment method for Chromium-based browsers (Chrome, Brave, Microsoft Edge).

We now support adding Google Pay as a stored payment method for Chromium-based browsers (Chrome, Brave, Microsoft Edge).
Rocky Linux 8.5 x64 (rockylinux-8-x64) base image is now available in the control panel and via the API.

Rocky Linux 8.5 x64 (rockylinux-8-x64) base image is now available in the control panel and via the API.
Released v1.68.0 of doctl, the official DigitalOcean CLI. This release contains improvements to the registry subcommands including: revoking credentials on logout and a new list-manifests subcommand.

Released v1.68.0 of doctl, the official DigitalOcean CLI. This release contains improvements to the registry subcommands including: revoking credentials on logout and a new list-manifests subcommand.
App Platform now supports rolling back an app to a previous deployment for Basic and Professional tier apps.

App Platform now supports rolling back an app to a previous deployment for Basic and Professional tier apps.
To improve collaboration on our platform, a percentage of new signups on DigitalOcean will begin with a team account.

To improve collaboration on our platform, a percentage of new signups on DigitalOcean will begin with a team account.
You can now optimize your storage space in Container Registry with garbage collection and more management options for images and tags.

You can now optimize your storage space in Container Registry with garbage collection and more management options for images and tags.
The Droplet Console now supports running the SSH daemon, sshd, on a custom port. Previously, it required sshd to listen on port 22.

The Droplet Console now supports running the SSH daemon, sshd, on a custom port. Previously, it required sshd to listen on port 22.
Released v1.67.0 of doctl, the official DigitalOcean CLI. This release updates godo to support new App Platform features.

Released v1.67.0 of doctl, the official DigitalOcean CLI. This release updates godo to support new App Platform features.
You can now scale load balancers with more granularity by adding or removing nodes. The number of nodes a load balancer contains determines how many simultaneous connections and requests per second it can manage.
Each additional node increases the load balancer‚Äôs maximum:

Requests per second by 10,000
Simultaneous connections by 10,000
New SSL connections per second by 250

You can add up to 100 nodes to a load balancer.

You can now scale load balancers with more granularity by adding or removing nodes. The number of nodes a load balancer contains determines how many simultaneous connections and requests per second it can manage.
Each additional node increases the load balancer‚Äôs maximum:
You can add up to 100 nodes to a load balancer.
General Purpose Droplets are now available in BLR1.

General Purpose Droplets are now available in BLR1.
Fedora 35 base image is now available in the control panel and via the API.

Fedora 35 base image is now available in the control panel and via the API.
v2.16.0 of the DigitalOcean Terraform Provider is now available. This release adds support for:

Scaling load balancers using the size_unit field.
source_kubernetes_ids and destination_kubernetes_ids attributes for Kubernetes firewall rules.

It also includes bug fixes.

v2.16.0 of the DigitalOcean Terraform Provider is now available. This release adds support for:
It also includes bug fixes.
v2.15.0 of the DigitalOcean Terraform Provider is now available. This release adds support for:

Disabling automatic DNS record creation when using Let‚Äôs Encrypting certificates.
Revoking OAuth tokens when credentials are destroyed.

It also includes bug fixes.

v2.15.0 of the DigitalOcean Terraform Provider is now available. This release adds support for:
It also includes bug fixes.
Released v1.66.0 of doctl, the official DigitalOcean CLI. This release includes a number of new features including support for PowerShell completion and a new --upsert flag for the doctl apps create command that updates the app in the given app spec if it already exists.

Released v1.66.0 of doctl, the official DigitalOcean CLI. This release includes a number of new features including support for PowerShell completion and a new --upsert flag for the doctl apps create command that updates the app in the given app spec if it already exists.
App Platform now supports forwarding application runtime logs to Logtail.

App Platform now supports forwarding application runtime logs to Logtail.
We have deprecated TLS DHE ciphers for all load balancers.

We have deprecated TLS DHE ciphers for all load balancers.
We now support Google Pay for one-time payments.

We now support Google Pay for one-time payments.
Ubuntu 21.10 base image is now available in the control panel and via the API.

Ubuntu 21.10 base image is now available in the control panel and via the API.
Ubuntu 20.10 has reached its end of life. Per our image deprecation policy, you can now only deploy the Ubuntu 20.10 image via the API. We will remove the Ubuntu 20.10 image from the platform on 25 November 2021.

Ubuntu 20.10 has reached its end of life. Per our image deprecation policy, you can now only deploy the Ubuntu 20.10 image via the API. We will remove the Ubuntu 20.10 image from the platform on 25 November 2021.
You can now opt out of DigitalOcean automatically creating DNS records for Let‚Äôs Encrypt certificates during SSL certificate creation, load balancer creation, and SSL forwarding rule management.

You can now opt out of DigitalOcean automatically creating DNS records for Let‚Äôs Encrypt certificates during SSL certificate creation, load balancer creation, and SSL forwarding rule management.
Released v1.65.0 of doctl, the official DigitalOcean CLI. This release includes a number of new features:

The --ha flag was added to the kubernetes cluster create sub-command to optionally create a cluster configured with a highly-available control plane. This feature is in early availability
The kubernetes cluster sub-commands now include a ‚ÄúSupport Features‚Äù field when displaying version options
The --disable-lets-encrypt-dns-records flag was added to the compute load-balancer create sub-command to optionally disable automatic DNS record creation for Let‚Äôs Encrypt certificates that are added to the load balancer


Released v1.65.0 of doctl, the official DigitalOcean CLI. This release includes a number of new features:
High-availability control plane is now in early availability in the following regions: ams3, nyc1, sfo3, and sgp1.

High-availability control plane is now in early availability in the following regions: ams3, nyc1, sfo3, and sgp1.
v2.14.0 of the DigitalOcean Terraform Provider is now available. This release adds support for the high availability (ha) attribute when creating Kubernetes clusters.

v2.14.0 of the DigitalOcean Terraform Provider is now available. This release adds support for the high availability (ha) attribute when creating Kubernetes clusters.
v2.13.0 of the DigitalOcean Terraform Provider is now available. This release adds support for retrieving the CA certificate for database clusters. It also includes bug fixes.

v2.13.0 of the DigitalOcean Terraform Provider is now available. This release adds support for retrieving the CA certificate for database clusters. It also includes bug fixes.
Tax collection for Georgia has begun. Charges will appear on the November invoice. Learn more about tax for Georgia.

Tax collection for Georgia has begun. Charges will appear on the November invoice. Learn more about tax for Georgia.
Tax collection for Japan has begun. Charges will appear on the November invoice.

Tax collection for Japan has begun. Charges will appear on the November invoice.
App Platform now supports forwarding application runtime logs to external log management providers. Currently, we only support Papertrail and Datadog.

App Platform now supports forwarding application runtime logs to external log management providers. Currently, we only support Papertrail and Datadog.
Team members with the biller role no longer have view-only access to a team‚Äôs shared resources. Billers have full access to billing information only and no access to shared resources or team settings.

Team members with the biller role no longer have view-only access to a team‚Äôs shared resources. Billers have full access to billing information only and no access to shared resources or team settings.
Debian 11.0 base image is now available in the control panel and via the API.

Debian 11.0 base image is now available in the control panel and via the API.
v2.12.1 of the DigitalOcean Terraform Provider is now available. This release adds CORS support for apps on App Platform and the ability to create monitoring alerts.

v2.12.1 of the DigitalOcean Terraform Provider is now available. This release adds CORS support for apps on App Platform and the ability to create monitoring alerts.
App Platform is now available in TOR1 and LON1.

App Platform is now available in TOR1 and LON1.
The load balancer and Spaces services now support wildcard Let‚Äôs Encrypt certificates.

The load balancer and Spaces services now support wildcard Let‚Äôs Encrypt certificates.
The Go buildpack for App Platform received some updates:
We added support for GO v1.17.1 and v1.16.8
See our sample Go app for more information on how to implement Go applications on App Platform.

The Go buildpack for App Platform received some updates:
We added support for GO v1.17.1 and v1.16.8
See our sample Go app for more information on how to implement Go applications on App Platform.
We have begun charging a 7% Value Added Tax (VAT) to customers in Thailand. This VAT rate is temporarily reduced until 30 September 2021. Unless the TRD extends the temporary reduction, the rate will increase back to the standard VAT rate of 10% on 1 October 2021.

We have begun charging a 7% Value Added Tax (VAT) to customers in Thailand. This VAT rate is temporarily reduced until 30 September 2021. Unless the TRD extends the temporary reduction, the rate will increase back to the standard VAT rate of 10% on 1 October 2021.
You can now assign floating IP addresses to Droplets that use custom images.

You can now assign floating IP addresses to Droplets that use custom images.
Released v1.64.0 of doctl, the official DigitalOcean CLI. This release includes support for managing App Platform alerts.

Released v1.64.0 of doctl, the official DigitalOcean CLI. This release includes support for managing App Platform alerts.
You can monitor and set up alerts for events for your app and its components using App Platform.

You can monitor and set up alerts for events for your app and its components using App Platform.
Released v1.63.0 of doctl, the official DigitalOcean CLI. This release includes a number of new features:

The database firewall sub-commands now support apps as trusted sources
New monitoring alert sub-commands for creating and managing alert policies
The --droplet-agent flag was added to the compute droplet create sub-command to optionally disable installing the agent for the Droplet web console


Released v1.63.0 of doctl, the official DigitalOcean CLI. This release includes a number of new features:
MongoDB is now available as a managed database engine in the AMS3, BLR1, FRA1, LON1, NYC1, NYC3, SFO3, SGP1, and TOR1 regions.

MongoDB is now available as a managed database engine in the AMS3, BLR1, FRA1, LON1, NYC1, NYC3, SFO3, SGP1, and TOR1 regions.
The Droplet Console is now in General Availability.

The Droplet Console is now in General Availability.
App Platform now supports apps as trusted sources for databases. We support PostgreSQL, MySQL, and Redis clusters.

App Platform now supports apps as trusted sources for databases. We support PostgreSQL, MySQL, and Redis clusters.
You can now add Kubernetes clusters as sources or destinations in Cloud Firewall rules.

You can now add Kubernetes clusters as sources or destinations in Cloud Firewall rules.
When updating an SSH key‚Äôs name using the API, if the request body does not contain a new name, the SSH key‚Äôs name will now retain its previous value. Previously, if the request body did not contain a new name, the SSH key‚Äôs name would update to a default value of either the comment field or the first 23 characters from the public key.

When updating an SSH key‚Äôs name using the API, if the request body does not contain a new name, the SSH key‚Äôs name will now retain its previous value. Previously, if the request body did not contain a new name, the SSH key‚Äôs name would update to a default value of either the comment field or the first 23 characters from the public key.
CentOS Linux is reaching end of life; CentOS Linux 8 reaches EOL at the end of 2021 and there will be no CentOS Linux 9. As potential replacements, we have released two new Linux distributions for Droplets: CentOS Stream 8 (centos-stream-8-x64) and Rocky Linux 8.4 x64 (rockylinux-8-x64).

CentOS Linux is reaching end of life; CentOS Linux 8 reaches EOL at the end of 2021 and there will be no CentOS Linux 9. As potential replacements, we have released two new Linux distributions for Droplets: CentOS Stream 8 (centos-stream-8-x64) and Rocky Linux 8.4 x64 (rockylinux-8-x64).
Released v1.62.0 of doctl, the official DigitalOcean CLI. This release fixes a handful of bugs and introduces new flags on existing commands:

The apps logs command now supports tailing live logs with the --tail flag. This lets application owners select the most recent logs from their applications
The --wait flag was added to apps create and apps update to block these commands until an application is fully created or updated


Released v1.62.0 of doctl, the official DigitalOcean CLI. This release fixes a handful of bugs and introduces new flags on existing commands:
v2.10.0 of the DigitalOcean Terraform Provider is now available. This release adds support for Kubernetes maintenance policies.

v2.10.0 of the DigitalOcean Terraform Provider is now available. This release adds support for Kubernetes maintenance policies.
v2.10.1 of the DigitalOcean Terraform Provider is now available. This release adds bug fixes and other improvements.

v2.10.1 of the DigitalOcean Terraform Provider is now available. This release adds bug fixes and other improvements.
The MongoDB database engine is now in general availability.

The MongoDB database engine is now in general availability.
App Platform can now deploy apps from a monorepo.

App Platform can now deploy apps from a monorepo.
The new Droplet Console is now in private beta. The Droplet Console gives you one-click SSH access to your Droplet from within a web browser, so you don‚Äôt need a password or SSH keys to connect.

The new Droplet Console is now in private beta. The Droplet Console gives you one-click SSH access to your Droplet from within a web browser, so you don‚Äôt need a password or SSH keys to connect.
We‚Äôve separated your user information from personal account settings. User information is now accessible from within your personal account or your teams on the My Account page, which is accessible in the profile icon menu in the top right of the control panel, under Manage Account.

We‚Äôve separated your user information from personal account settings. User information is now accessible from within your personal account or your teams on the My Account page, which is accessible in the profile icon menu in the top right of the control panel, under Manage Account.
A new Python + NodeJS group buildpack was added. Python apps can now use NodeJS to render assets during build or as a dependency at runtime.

A new Python + NodeJS group buildpack was added. Python apps can now use NodeJS to render assets during build or as a dependency at runtime.
v2.7.0 of the DigitalOcean Terraform Provider is now available. This release adds support for distributing images to multiple regions.

v2.7.0 of the DigitalOcean Terraform Provider is now available. This release adds support for distributing images to multiple regions.
PostgreSQL 13 is now available for database clusters. You can also now perform in-place upgrades for PostgreSQL clusters to newer versions without any downtime. We currently support PostgreSQL 10, 11, 12, and 13.

PostgreSQL 13 is now available for database clusters. You can also now perform in-place upgrades for PostgreSQL clusters to newer versions without any downtime. We currently support PostgreSQL 10, 11, 12, and 13.
Fedora 32 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 32 image from our platform in 30 days.

Fedora 32 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 32 image from our platform in 30 days.
Ubuntu 16.04 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 16.04 image from our platform in 30 days.

Ubuntu 16.04 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 16.04 image from our platform in 30 days.
Storage-Optimized Droplets are now available in SGP1.

Storage-Optimized Droplets are now available in SGP1.
Released v1.60.0 of doctl, the official DigitalOcean CLI. This release introduces an auth remove sub-command to simplify removing an auth context when managing multiple accounts. The databases user reset sub-command now supports resetting the user password for all database engine types.

Released v1.60.0 of doctl, the official DigitalOcean CLI. This release introduces an auth remove sub-command to simplify removing an auth context when managing multiple accounts. The databases user reset sub-command now supports resetting the user password for all database engine types.
We have updated the Floating IP API responses to better align with our newer API models. The droplet and region fields now use the same response models used in the /v2/droplets and /v2/regions endpoints. Specifically:

The private_networking feature is now displayed under the features field under droplet instead of the features field under region.
The vpc_uuid field now populates with the correct values.
The Droplet‚Äôs type displays base when the Droplet uses a base image (i.e. Ubuntu, CentOS).
The Droplet‚Äôs networks field now includes private and floating IP addresses, if applicable.


We have updated the Floating IP API responses to better align with our newer API models. The droplet and region fields now use the same response models used in the /v2/droplets and /v2/regions endpoints. Specifically:
Premium AMD Droplets are now available in NYC1, SGP1, AMS3, BLR1, LON1, and TOR1. You can view the availability of all of our products by datacenter in the regional availability matrix.

Premium AMD Droplets are now available in NYC1, SGP1, AMS3, BLR1, LON1, and TOR1. You can view the availability of all of our products by datacenter in the regional availability matrix.
Fedora 34 base image is now available in the control panel and via the API.

Fedora 34 base image is now available in the control panel and via the API.
Ubuntu 21.04 base image is now available in the control panel and via the API.

Ubuntu 21.04 base image is now available in the control panel and via the API.
Released v1.59.0 of doctl, the official DigitalOcean CLI. This release includes a new apps propose sub-command and improvements to the apps spec validate sub-command.

Released v1.59.0 of doctl, the official DigitalOcean CLI. This release includes a new apps propose sub-command and improvements to the apps spec validate sub-command.
With the completion of datacenter work, we have re-enabled resizing between regular and premium Intel Droplets in NYC3 and SGP1.

With the completion of datacenter work, we have re-enabled resizing between regular and premium Intel Droplets in NYC3 and SGP1.
To comply with new provincial requirements in Canada, we are now collecting Provincial Sales Tax (PST) at a rate of 7% for customers in British Columbia and 6% in Saskatchewan. For more details, see Canada tax information.

To comply with new provincial requirements in Canada, we are now collecting Provincial Sales Tax (PST) at a rate of 7% for customers in British Columbia and 6% in Saskatchewan. For more details, see Canada tax information.
Tax collection for Kenya has begun. Charges will appear on the May invoice.

Tax collection for Kenya has begun. Charges will appear on the May invoice.
You can now deploy managed databases on Droplets with dedicated CPUs for the PostgreSQL, MySQL, and Redis engines.

You can now deploy managed databases on Droplets with dedicated CPUs for the PostgreSQL, MySQL, and Redis engines.
v2.7.0 of the DigitalOcean Terraform Provider is now available. This release adds support for Kubernetes node pool taints and resizing load balancers.

v2.7.0 of the DigitalOcean Terraform Provider is now available. This release adds support for Kubernetes node pool taints and resizing load balancers.
Released v1.58.0 of doctl, the official DigitalOcean CLI. This release adds a --wait flag to the apps create-deployment command which blocks until the deployment is complete. By default, the registry kubernetes-manifest now generates a manifest that applies the secret to all the namespaces in the Kubernetes cluster using the DOSecret operator.

Released v1.58.0 of doctl, the official DigitalOcean CLI. This release adds a --wait flag to the apps create-deployment command which blocks until the deployment is complete. By default, the registry kubernetes-manifest now generates a manifest that applies the secret to all the namespaces in the Kubernetes cluster using the DOSecret operator.
You can now do the following on Kubernetes clusters:


Use surge upgrade when upgrading an existing cluster. Surge upgrade is enabled by default when you create a new cluster.


Move a Kubernetes cluster and its associated resources, such as Droplets, load balancers and volumes, to a project using the DigitalOcean Control Panel or doctl command-line tool. You can also assign a project when you create a new cluster. If you do not specify a project, it gets assigned to the default project.


Delete resources, such as load balancers and volumes, associated with a Kubernetes cluster using the DigitalOcean Control Panel, API or the doctl command-line tool.



You can now do the following on Kubernetes clusters:
Use surge upgrade when upgrading an existing cluster. Surge upgrade is enabled by default when you create a new cluster.
Move a Kubernetes cluster and its associated resources, such as Droplets, load balancers and volumes, to a project using the DigitalOcean Control Panel or doctl command-line tool. You can also assign a project when you create a new cluster. If you do not specify a project, it gets assigned to the default project.
Delete resources, such as load balancers and volumes, associated with a Kubernetes cluster using the DigitalOcean Control Panel, API or the doctl command-line tool.
You can now resize load balancers to better match their performance to their workload.

You can now resize load balancers to better match their performance to their workload.
v2.6.0 of the DigitalOcean Terraform Provider is now available. This release enables surge upgrades for Kubernetes clusters by default and adds a digitalocean_firewall data source.

v2.6.0 of the DigitalOcean Terraform Provider is now available. This release enables surge upgrades for Kubernetes clusters by default and adds a digitalocean_firewall data source.
Storage Optimized Droplets are now available in TOR1 and BLR1.

Storage Optimized Droplets are now available in TOR1 and BLR1.
Online migration for PostgreSQL and Redis databases has been released in Beta. Select users can now migrate Redis and PostgreSQL databases that reside inside and outside of DigitalOcean to existing database clusters in their DigitalOcean account. Redis migrations from AWS ElasticCache are not currently supported.

Online migration for PostgreSQL and Redis databases has been released in Beta. Select users can now migrate Redis and PostgreSQL databases that reside inside and outside of DigitalOcean to existing database clusters in their DigitalOcean account. Redis migrations from AWS ElasticCache are not currently supported.
With the completion of the SGP1 capacity augmentation, we have re-enabled the creation of new Spaces in SGP1.

With the completion of the SGP1 capacity augmentation, we have re-enabled the creation of new Spaces in SGP1.
Fixed a bug with DigitalOcean Load Balancers that prevented outbound data transfer from Droplets from being added to bandwidth usage totals. Any inconsistencies will be updated on the April invoice.

Fixed a bug with DigitalOcean Load Balancers that prevented outbound data transfer from Droplets from being added to bandwidth usage totals. Any inconsistencies will be updated on the April invoice.
The PHP buildpack received some updates:

Support for PHP 8.x was added in addition to PHP 7.x. By default, App Platform will use PHP 7.x unless the app specifies a version requirement via Composer.
Support for Composer 2.x was added in addition to Composer 1.x. App Platform will use the same version of Composer that generated the app‚Äôs composer.lock file.

For more information and configuration options, see the buildpack‚Äôs documentation page.

The PHP buildpack received some updates:
For more information and configuration options, see the buildpack‚Äôs documentation page.
Our Basic Droplet plans now include Premium Intel and AMD Droplets, which have faster Intel and AMD CPUs and NVMe SSDs. Premium Intel plans are availabile in all regions, and Premium AMD plans are available in NYC3, SFO3, and FRA1.
All users can create Premium Droplets, resize from Regular Intel Droplets to Premium Intel Droplets, and create Premium Droplets as worker nodes in Kubernetes clusters. Resizing between Regular Intel Droplets to Premium Intel Droplets is disabled in NYC3 and SGP1 until the end of March 2021.

Our Basic Droplet plans now include Premium Intel and AMD Droplets, which have faster Intel and AMD CPUs and NVMe SSDs. Premium Intel plans are availabile in all regions, and Premium AMD plans are available in NYC3, SFO3, and FRA1.
All users can create Premium Droplets, resize from Regular Intel Droplets to Premium Intel Droplets, and create Premium Droplets as worker nodes in Kubernetes clusters. Resizing between Regular Intel Droplets to Premium Intel Droplets is disabled in NYC3 and SGP1 until the end of March 2021.
Released v1.57.0 of doctl, the official DigitalOcean CLI. This release enables surge upgrades by default for newly created Kubernetes clusters and important bugfixes for App Platform logs and Kubernetes cascading deletes.

Released v1.57.0 of doctl, the official DigitalOcean CLI. This release enables surge upgrades by default for newly created Kubernetes clusters and important bugfixes for App Platform logs and Kubernetes cascading deletes.
App Platform is now available in SGP1 and BLR1.

App Platform is now available in SGP1 and BLR1.
FreeBSD 12.1 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the FreeBSD 12.1 image from our platform in 30 days.

FreeBSD 12.1 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the FreeBSD 12.1 image from our platform in 30 days.
Released v1.56.0 of doctl, the official DigitalOcean CLI. This release includes support for database firewalls management, Kubernetes cascading deletes, and installing Kubernetes 1-Clicks to existing clusters.

Released v1.56.0 of doctl, the official DigitalOcean CLI. This release includes support for database firewalls management, Kubernetes cascading deletes, and installing Kubernetes 1-Clicks to existing clusters.
v2.5.0 of the DigitalOcean Terraform Provider is now available. This release adds a number of improvements to the digitalocean_app resource including: support for images as a component source, support for job components, support for internal_ports for services, and support for wildcard domains.

v2.5.0 of the DigitalOcean Terraform Provider is now available. This release adds a number of improvements to the digitalocean_app resource including: support for images as a component source, support for job components, support for internal_ports for services, and support for wildcard domains.
Spaces are now available in SFO3.

Spaces are now available in SFO3.
Due to capacity limits in the region, we have disabled the creation of new resources in SFO2 for new customers. Existing customers with resources in SFO2 are unaffected and can still create and destroy resources in SFO2.

Due to capacity limits in the region, we have disabled the creation of new resources in SFO2 for new customers. Existing customers with resources in SFO2 are unaffected and can still create and destroy resources in SFO2.
We‚Äôve improved the account deactivation experience to more clearly guide users through the actions necessary to deactivate an account.

We‚Äôve improved the account deactivation experience to more clearly guide users through the actions necessary to deactivate an account.
Released v1.55.0 of doctl, the official DigitalOcean CLI. This release adds Docker Hub and GitLab support to App Platform‚Äôs create and update commands.

Released v1.55.0 of doctl, the official DigitalOcean CLI. This release adds Docker Hub and GitLab support to App Platform‚Äôs create and update commands.
App Platform now supports launching components from public DockerHub image sources.

App Platform now supports launching components from public DockerHub image sources.
v2.4.0 of the DigitalOcean Terraform Provider is now available. This release includes support for deployments from GitLab and app-wide environment variables for the digitalocean_app resource, bug fixes, and other improvements.

v2.4.0 of the DigitalOcean Terraform Provider is now available. This release includes support for deployments from GitLab and app-wide environment variables for the digitalocean_app resource, bug fixes, and other improvements.
You can now do the following on App Platform:


Deploy apps from a registry that has been uploaded to a DigitalOcean Container Registry.


Deploy apps from source code contained in a GitLab repository.


Edit CORS policies for your app.



You can now do the following on App Platform:
Deploy apps from a registry that has been uploaded to a DigitalOcean Container Registry.
Deploy apps from source code contained in a GitLab repository.
Edit CORS policies for your app.
In response to the United Kingdom departing as a member state of the European Union, DigitalOcean has obtained a standalone UK VAT ID and continued charging and collecting VAT at a rate of 20% on its business-to-consumer sales in the UK. Business customers with a valid UK VAT ID are subject to the reverse charge mechanism.

In response to the United Kingdom departing as a member state of the European Union, DigitalOcean has obtained a standalone UK VAT ID and continued charging and collecting VAT at a rate of 20% on its business-to-consumer sales in the UK. Business customers with a valid UK VAT ID are subject to the reverse charge mechanism.
Load balancers now come in small, medium, and large sizes. The larger the load balancer, the more simultaneous connections and requests per second it can manage. Existing load balancers are now considered ‚Äúsmall‚Äù load balancers and are unaffected by this change.

Load balancers now come in small, medium, and large sizes. The larger the load balancer, the more simultaneous connections and requests per second it can manage. Existing load balancers are now considered ‚Äúsmall‚Äù load balancers and are unaffected by this change.
CentOS 6 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the CentOS 6 image from our platform in 30 days.

CentOS 6 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the CentOS 6 image from our platform in 30 days.
Load balancers now come in small, medium, and large sizes. The larger the load balancer, the more simultaneous connections and requests per second it can manage. Existing load balancers are now considered ‚Äúsmall‚Äù load balancers and are unaffected by this change.
You can specify the size of a load balancer during its creation using the size field. The available size values are lb-small, lb-medium, or lb-large.
Example request body:
{
  "name": "example-lb-01",
  "region": "nyc3",
  "size": "lb-small",
  "forwarding_rules": [
    {
      "entry_protocol": "https",
      "entry_port": 444,
      "target_protocol": "https",
      "target_port": 443,
      "tls_passthrough": true
    }
  ],
  "health_check": {
    "protocol": "http",
    "port": 80,
    "path": "/",
    "check_interval_seconds": 10,
    "response_timeout_seconds": 5,
    "healthy_threshold": 5,
    "unhealthy_threshold": 3
  },
  "sticky_sessions": {
    "type": "none"
  }
  ]
}
Once you have created a load balancer, you can‚Äôt change its size.

Load balancers now come in small, medium, and large sizes. The larger the load balancer, the more simultaneous connections and requests per second it can manage. Existing load balancers are now considered ‚Äúsmall‚Äù load balancers and are unaffected by this change.
You can specify the size of a load balancer during its creation using the size field. The available size values are lb-small, lb-medium, or lb-large.
Example request body:
Once you have created a load balancer, you can‚Äôt change its size.
Released v1.54.0 of doctl, the official DigitalOcean CLI. This release user confirmation before container registry garbage collection is started.

Released v1.54.0 of doctl, the official DigitalOcean CLI. This release user confirmation before container registry garbage collection is started.
v2.3.0 of the DigitalOcean Terraform Provider is now available. This release includes support for configuring the size of a digitalocean_loadbalancer resource. OpenBSD binaries are now built and available for download.

v2.3.0 of the DigitalOcean Terraform Provider is now available. This release includes support for configuring the size of a digitalocean_loadbalancer resource. OpenBSD binaries are now built and available for download.
Fixed a bug that intermittently caused blank control panel pages in certain GeoIP regions.

Fixed a bug that intermittently caused blank control panel pages in certain GeoIP regions.
We recently replaced Standard Droplet plans with Basic Droplet plans. Today, we have deprecated Standard Droplet plans from the API for new users. Existing customers will retain access to these plans.



Click here to view the complete list of deprecated plans.




Class
Slug
vCPUs
RAM
Disk
Transfer
Monthly Price




Standard
512mb
1
512 MB
20 GB
1 TB
$5


Standard
1gb
1
1 GB
30 GB
2 TB
$10


Standard
2gb
2
2 GB
40 GB
3 TB
$20


Standard
4gb
2
3 GB
60 GB
4 TB
$40


Standard
8gb
4
4 GB
80 GB
5 TB
$80


Standard
16gb
8
16 GB
160 GB
6 TB
$160


Standard
32gb
12
32 GB
320 GB
7 TB
$320


Standard
48gb
16
48 GB
480 GB
8 TB
$480


Standard
64gb
20
64 GB
640 GB
9 TB
$640


Standard
96gb
24
96 GB
960 GB
10 TB
$960


Standard
s-1vcpu-3gb
1
3 GB
60 GB
3 TB
$15


Standard
s-3vcpu-1gb
3
1 GB
60 GB
3 TB
$15


Standard
s-6vcpu-16gb
6
16 GB
320 GB
6 TB
$80


Standard
s-8vcpu-32gb
8
32 GB
640 GB
7 TB
$160


Standard
s-12vcpu-48gb
12
48 GB
960 GB
8 TB
$240


Standard
s-16vcpu-64gb
16
16 GB
1280 GB
9 TB
$320


Standard
s-20vcpu-96gb
20
20 GB
1920 GB
10 TB
$480


Standard
s-24vcpu-128gb
24
24 GB
2560 GB
11 TB
$640


Standard
s-32vcpu-192gb
32
32 GB
3840 GB
12 TB
$960


High Memory
m-16gb
2
16 GB
60 GB
5 TB
$75


High Memory
m-32gb
4
32 GB
90 GB
5 TB
$150


High Memory
m-64gb
8
64 GB
200 GB
5 TB
$300


High Memory
m-128gb
16
128 GB
340 GB
5 TB
$600


High Memory
m-224gb
32
224 GB
500 GB
5 TB
$1100





You can view Droplet plans, the resources they provide, and the size slug used to identify them programmatically by querying the /v2/sizes endpoint.

We recently replaced Standard Droplet plans with Basic Droplet plans. Today, we have deprecated Standard Droplet plans from the API for new users. Existing customers will retain access to these plans.
You can view Droplet plans, the resources they provide, and the size slug used to identify them programmatically by querying the /v2/sizes endpoint.
Released v1.53.0 of doctl, the official DigitalOcean CLI. This release adds support for container registry garbage collection of untagged manifests.

Released v1.53.0 of doctl, the official DigitalOcean CLI. This release adds support for container registry garbage collection of untagged manifests.
Fedora 31 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 31 image from our platform in 30 days.

Fedora 31 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 31 image from our platform in 30 days.
Redis 6 Managed Databases are now available. Redis 6 includes enhanced security features and client-side caching. You can no longer create Redis 5 clusters, but Redis 6 clusters are fully backwards compatible.

Redis 6 Managed Databases are now available. Redis 6 includes enhanced security features and client-side caching. You can no longer create Redis 5 clusters, but Redis 6 clusters are fully backwards compatible.
Droplet-related error messages now display on the Droplet‚Äôs History page in the control panel.

Droplet-related error messages now display on the Droplet‚Äôs History page in the control panel.
v2.2.0 of the DigitalOcean Terraform Provider is now available. This release adds a new digitalocean_ssh_keys data source and a digitalocean_custom_image resource.

v2.2.0 of the DigitalOcean Terraform Provider is now available. This release adds a new digitalocean_ssh_keys data source and a digitalocean_custom_image resource.
We have reduced the prices of Memory-Optimized Droplets by about 11%. Existing Memory-Optimized Droplets will be charged at the lowered price from the month of November on, reflected in the December invoice.

We have reduced the prices of Memory-Optimized Droplets by about 11%. Existing Memory-Optimized Droplets will be charged at the lowered price from the month of November on, reflected in the December invoice.
We have released Storage-Optimized Droplet plans. Storage-Optimized Droplets have NVMe SSD storage and are best for extra-large databases, caches, and analytics workloads.
All users can now create Storage-Optimized Droplets in AMS3, FRA1, LON1, NYC1, and SFO3 using the control panel, API, or CLI. The slugs for the new plans are so-2vcpu-16gb, so-4vcpu-32gb, so-16vcpu-64gb, so-24vcpu-128gb, and so-32vcpu-256gb.

We have released Storage-Optimized Droplet plans. Storage-Optimized Droplets have NVMe SSD storage and are best for extra-large databases, caches, and analytics workloads.
All users can now create Storage-Optimized Droplets in AMS3, FRA1, LON1, NYC1, and SFO3 using the control panel, API, or CLI. The slugs for the new plans are so-2vcpu-16gb, so-4vcpu-32gb, so-16vcpu-64gb, so-24vcpu-128gb, and so-32vcpu-256gb.
Released v1.52.0 of doctl, the official DigitalOcean CLI. This release adds a --force-rebuild flag to doctl apps create-deployment.

Released v1.52.0 of doctl, the official DigitalOcean CLI. This release adds a --force-rebuild flag to doctl apps create-deployment.
Improved build caching for all App Platform build environments. Dependencies and other data are now cached and reused between builds to improve performance. Dockerfile builds continue to make use of Docker layer caching.
The Hugo buildpack received some updates:

The default version has been upgraded to 0.78.0.
Added support for running Hugo as a Service component. The default and recommended behavior is to deploy it as a Static Site.
Removed the implicit build command‚Äîa build command is now required. Existing apps have had their specs automatically updated.


Improved build caching for all App Platform build environments. Dependencies and other data are now cached and reused between builds to improve performance. Dockerfile builds continue to make use of Docker layer caching.
The Hugo buildpack received some updates:
v2.1.0 of the DigitalOcean Terraform Provider is now available. DigitalOcean Container Registry is now in general availability and requires a subscription plan. As a result, the digitalocean_container_registry resource now requires setting a subscription_tier_slug which is supported with this release.

v2.1.0 of the DigitalOcean Terraform Provider is now available. DigitalOcean Container Registry is now in general availability and requires a subscription plan. As a result, the digitalocean_container_registry resource now requires setting a subscription_tier_slug which is supported with this release.
We have released a ‚ÄúDeploy to DigitalOcean‚Äù button for App Platform. You can now embed a button into your GitHub repo or website that allows users to deploy your app directly to DigitalOcean.
We have also released a jobs feature for App Platform. The job feature allows you to run application code at a scheduled time.

We have released a ‚ÄúDeploy to DigitalOcean‚Äù button for App Platform. You can now embed a button into your GitHub repo or website that allows users to deploy your app directly to DigitalOcean.
We have also released a jobs feature for App Platform. The job feature allows you to run application code at a scheduled time.
FreeBSD 12.2 UFS and ZFS base images are now available in the control panel and via the API. The image slug for the UFS FreeBSD image freebsd-12-x64 has been replaced with freebsd-12-x64-ufs. We will support 12.1 for three months after the release date of 12.2, which is currently scheduled for January 31, 2021.

FreeBSD 12.2 UFS and ZFS base images are now available in the control panel and via the API. The image slug for the UFS FreeBSD image freebsd-12-x64 has been replaced with freebsd-12-x64-ufs. We will support 12.1 for three months after the release date of 12.2, which is currently scheduled for January 31, 2021.
Released v1.51.0 of doctl, the official DigitalOcean CLI. This release add support for managing DigitalOcean Container Registry subscriptions.

Released v1.51.0 of doctl, the official DigitalOcean CLI. This release add support for managing DigitalOcean Container Registry subscriptions.
DigitalOcean Container Registry is now in General Availability. Highlights include:

Three subscription plans that offer different allowances for repositories, storage, and bandwidth
1-click method to configure DigitalOcean Kubernetes Cluster to use the registry
Garbage collection using the command line or the API
Ability to see the current storage usage in the control panel


DigitalOcean Container Registry is now in General Availability. Highlights include:
You can now integrate your DOCR registry with a Kubernetes cluster. When a registry is integrated with a Kubernetes cluster, we create docker registry type secrets in all the namespaces in the cluster. These secrets can be used with the workloads or added to the default service account in the namespace.
Additionally, we‚Äôve added DOCR integration support for our official clients, godo and doctl. Only versions of doctl 1.49.0 and godo 1.48.0 and above support docr integration for clusters.

You can now integrate your DOCR registry with a Kubernetes cluster. When a registry is integrated with a Kubernetes cluster, we create docker registry type secrets in all the namespaces in the cluster. These secrets can be used with the workloads or added to the default service account in the namespace.
Additionally, we‚Äôve added DOCR integration support for our official clients, godo and doctl. Only versions of doctl 1.49.0 and godo 1.48.0 and above support docr integration for clusters.
Released v1.50.0 of doctl, the official DigitalOcean CLI. This release incudes a number for new features and improvements. It includes new doctl apps sub-commands to retrieve information about App Platform pricing plan tiers, instance sizes, and regions. The doctl registry sub-command now supports managing garbage collection for container registries.

Released v1.50.0 of doctl, the official DigitalOcean CLI. This release incudes a number for new features and improvements. It includes new doctl apps sub-commands to retrieve information about App Platform pricing plan tiers, instance sizes, and regions. The doctl registry sub-command now supports managing garbage collection for container registries.
Released v1.49.0 of doctl, the official DigitalOcean CLI. This release incudes the ability to update the default VPC for a region, the ability to set an expiration time when downloading kubeconfig files, and more.

Released v1.49.0 of doctl, the official DigitalOcean CLI. This release incudes the ability to update the default VPC for a region, the ability to set an expiration time when downloading kubeconfig files, and more.
On Kubernetes 1.19 and later we now provision two fully-managed firewalls for each new Kubernetes cluster. One firewall manages the connection between worker nodes and control plane, and the other manages connections between worker nodes and the public internet.

On Kubernetes 1.19 and later we now provision two fully-managed firewalls for each new Kubernetes cluster. One firewall manages the connection between worker nodes and control plane, and the other manages connections between worker nodes and the public internet.
Fedora 33 base image is now available in the control panel and via the API.

Fedora 33 base image is now available in the control panel and via the API.
You can now change the default VPC network for a region. When you change the default VPC network for a region, the new default network will be automatically selected during applicable resource set ups unless otherwise specified.

You can now change the default VPC network for a region. When you change the default VPC network for a region, the new default network will be automatically selected during applicable resource set ups unless otherwise specified.
Added support to App Platform for configuring internal service ports, as well as internal-only services that are not internet-accessible.

Added support to App Platform for configuring internal service ports, as well as internal-only services that are not internet-accessible.
Added support to App Platform for configuring a catch-all document that can be used by static sites to rewrite all requests to pages that are not found, to the configured document. The catchall_document field is similar to error_document in that they both rewrite all requests to the specified document, and so they are mutually exclusive, only 1 can be set. Using catchall_document will result in 200 HTTP response codes for the rewritten requests, while error_document will result in 404 HTTP response codes.

Added support to App Platform for configuring a catch-all document that can be used by static sites to rewrite all requests to pages that are not found, to the configured document. The catchall_document field is similar to error_document in that they both rewrite all requests to the specified document, and so they are mutually exclusive, only 1 can be set. Using catchall_document will result in 200 HTTP response codes for the rewritten requests, while error_document will result in 404 HTTP response codes.
Ubuntu 20.10 base image is now available in the control panel and via the API.

Ubuntu 20.10 base image is now available in the control panel and via the API.
v2.0.0 of the DigitalOcean Terraform Provider is now available. This release uses v2.0.3 of the Terraform Plugin SDK and now only supports Terraform v0.12 and higher. It also includes a new digitalocean_records data source.

v2.0.0 of the DigitalOcean Terraform Provider is now available. This release uses v2.0.3 of the Terraform Plugin SDK and now only supports Terraform v0.12 and higher. It also includes a new digitalocean_records data source.
You can now update a VPC network to be the default VPC network for a region using the PUT /v2/vpcs/$VPC_ID and PATCH /v2/vpcs/$VPC_ID endpoints.
For example:
{
  "name": "renamed-new-vpc",
  "description": "A new description",
  "default": "true"
}
When you change the default VPC network for a region, all applicable resources are placed into the default VPC network unless otherwise specified during their creation.

You can now update a VPC network to be the default VPC network for a region using the PUT /v2/vpcs/$VPC_ID and PATCH /v2/vpcs/$VPC_ID endpoints.
For example:
When you change the default VPC network for a region, all applicable resources are placed into the default VPC network unless otherwise specified during their creation.
Added support to App Platform services and static sites for configuring an ingress CORS policy.

Added support to App Platform services and static sites for configuring an ingress CORS policy.
Added support to App Platform for configuring custom wildcard domains.

Added support to App Platform for configuring custom wildcard domains.
Added an App Platform environment variable binding with the CA certificate for managed databases in the users account.

Added an App Platform environment variable binding with the CA certificate for managed databases in the users account.
v1.23.0 of the DigitalOcean Terraform Provider is now available. This release introduces a new digitalocean_app resource with support for DigitalOcean App Platform.

v1.23.0 of the DigitalOcean Terraform Provider is now available. This release introduces a new digitalocean_app resource with support for DigitalOcean App Platform.
Added an App Platform the environment variable binding prefix _self that can be used to reference the current component without directly referencing it by component name.

Added an App Platform the environment variable binding prefix _self that can be used to reference the current component without directly referencing it by component name.
Launched the App Platform jobs component type, with support for running containerized operations before, after, and on failure of deploys.

Launched the App Platform jobs component type, with support for running containerized operations before, after, and on failure of deploys.
Released v1.48.0 of doctl, the official DigitalOcean CLI. This release promotes doctl apps commands with support for DigitalOcean App Platform to General Availability.

Released v1.48.0 of doctl, the official DigitalOcean CLI. This release promotes doctl apps commands with support for DigitalOcean App Platform to General Availability.
You can now apply taints to Kubernetes node pools using the DigitalOcean API. When you configure taints for a node pool, the taint automatically applies to all current nodes and any subsequently created nodes in the pool. For more information, see Kubernetes‚Äô documentation on taints and tolerations.

You can now apply taints to Kubernetes node pools using the DigitalOcean API. When you configure taints for a node pool, the taint automatically applies to all current nodes and any subsequently created nodes in the pool. For more information, see Kubernetes‚Äô documentation on taints and tolerations.
You can now apply taints to Kubernetes node pools. When you configure taints for a node pool, the taint automatically applies to all current nodes in the pool and any node you add to the pool thereafter. For more information about taints and tolerations, see Kubernetes‚Äô documentation.
Additionally, we‚Äôve added node taint support for our official clients, godo and doctl. Only versions of doctl 1.47.0 and godo 1.45.0 and above support persistent node pool taints.
You can define taints during a pool‚Äôs creation by submitting a POST request to the /v2/kubernetes/clusters/<cluster-id>/node_pools and the /v2/kubernetes/clusters endpoints, or you can update existing pools by submitting a PUT request to the /v2/kubernetes/clusters/<cluster-id>/node_pools/<node-pool-id> endpoint. For example, this request body defines two taints for a node pool.
{
  "name": "frontend",
  "size": 10,
  [...]
  "taints": [
    {
      "key": "priority",
      "value": "high",
      "effect": "NoSchedule",
    },
    {
      "key": "workloadKind",
      "value": "database",
      "effect": "NoExecute",
    }
  ]
}

You can now apply taints to Kubernetes node pools. When you configure taints for a node pool, the taint automatically applies to all current nodes in the pool and any node you add to the pool thereafter. For more information about taints and tolerations, see Kubernetes‚Äô documentation.
Additionally, we‚Äôve added node taint support for our official clients, godo and doctl. Only versions of doctl 1.47.0 and godo 1.45.0 and above support persistent node pool taints.
You can define taints during a pool‚Äôs creation by submitting a POST request to the /v2/kubernetes/clusters/<cluster-id>/node_pools and the /v2/kubernetes/clusters endpoints, or you can update existing pools by submitting a PUT request to the /v2/kubernetes/clusters/<cluster-id>/node_pools/<node-pool-id> endpoint. For example, this request body defines two taints for a node pool.
App Platform, our new platform as a service (PaaS) offering, is now in General Availability. Hook a GitHub repo to DigitalOcean and let App Platform automatically build and deploy your commits live to the cloud. Read the quickstart or try it now.

App Platform, our new platform as a service (PaaS) offering, is now in General Availability. Hook a GitHub repo to DigitalOcean and let App Platform automatically build and deploy your commits live to the cloud. Read the quickstart or try it now.
Released v1.47.0 of doctl, the official DigitalOcean CLI. This release adds support for setting Kubernetes node pool taints.

Released v1.47.0 of doctl, the official DigitalOcean CLI. This release adds support for setting Kubernetes node pool taints.
All Droplets created after 1 October 2020 are placed into a VPC network by default. You can no longer manually enable VPC networking on existing Droplets. You can migrate existing Droplets into VPC networks using Snapshots.

All Droplets created after 1 October 2020 are placed into a VPC network by default. You can no longer manually enable VPC networking on existing Droplets. You can migrate existing Droplets into VPC networks using Snapshots.
All Droplets created after 1 October 2020 are placed into a VPC network by default. The enable_private_networking action and private_network parameter have been deprecated. Use the vpc_uuid parameter to specify the VPC network for your Droplets.
You can migrate existing Droplets into VPC networks using Snapshots.

All Droplets created after 1 October 2020 are placed into a VPC network by default. The enable_private_networking action and private_network parameter have been deprecated. Use the vpc_uuid parameter to specify the VPC network for your Droplets.
You can migrate existing Droplets into VPC networks using Snapshots.
We have updated capacity in FRA1 and have resumed the creation of Spaces in that region.

We have updated capacity in FRA1 and have resumed the creation of Spaces in that region.
We have temporarily disabled the creation of new Spaces in SGP1 while we update capacity in this region.

We have temporarily disabled the creation of new Spaces in SGP1 while we update capacity in this region.
Learn more on Creation of New Spaces in SGP1 Disabled Until 2021.
Dedicated CPU Droplet plans now offer more SSD size options. Each plan contains SSD size variants that you can choose upon creation or when resizing a dedicated CPU Droplet.

Dedicated CPU Droplet plans now offer more SSD size options. Each plan contains SSD size variants that you can choose upon creation or when resizing a dedicated CPU Droplet.
Fedora 30 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 30 image from our platform in 30 days.

Fedora 30 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Fedora 30 image from our platform in 30 days.
Standard Droplet plans have been replaced with Basic Droplet plans. We have added one new plan, s-8vcpu-16gb, and deprecated the following plans:

s-1vcpu-3gb
s-3vcpu-1gb
s-6vcpu-16gb 
s-8vcpu-32gb
s-12vcpu-48gb
s-16vcpu-64gb
s-20vcpu-96gb
s-24vcpu-128gb
s-32vcpu-192gb

These deprecated plans are now unavailable in the control panel, but you can still create Droplets with those plans using the API or doctl.

Standard Droplet plans have been replaced with Basic Droplet plans. We have added one new plan, s-8vcpu-16gb, and deprecated the following plans:
These deprecated plans are now unavailable in the control panel, but you can still create Droplets with those plans using the API or doctl.
Account security history now only displays events after 17 August 2019. If you need data from an earlier date, open a support ticket.

Account security history now only displays events after 17 August 2019. If you need data from an earlier date, open a support ticket.
We have released a Droplet metadata endpoint which returns whether or not a Droplet is scheduled for a live migration. The impact of live migrations on Droplets is minimal, so users now only receive direct notifications for migrations that require us to power down a Droplet, which (except in emergencies) we send 7 days in advance.

We have released a Droplet metadata endpoint which returns whether or not a Droplet is scheduled for a live migration. The impact of live migrations on Droplets is minimal, so users now only receive direct notifications for migrations that require us to power down a Droplet, which (except in emergencies) we send 7 days in advance.
Ubuntu 19.10 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 19.10 image from our platform in 30 days.

Ubuntu 19.10 has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the Ubuntu 19.10 image from our platform in 30 days.
Memory-Optimized Droplets are now available for the BLR1 datacenter region.

Memory-Optimized Droplets are now available for the BLR1 datacenter region.
App Platform is now in beta.

App Platform is now in beta.
FreeBSD 11.4 UFS and ZFS base images are now available in the control panel and via the API.

FreeBSD 11.4 UFS and ZFS base images are now available in the control panel and via the API.
v1.22.0 of the DigitalOcean Terraform Provider is now available. This release includes auto_upgrade and surge_upgrade support for the digitalocean_kubernetes_cluster resource.

v1.22.0 of the DigitalOcean Terraform Provider is now available. This release includes auto_upgrade and surge_upgrade support for the digitalocean_kubernetes_cluster resource.
Memory-Optimized Droplets are now in general availability and are available in the SFO3 and TOR1 datacenter regions.

Memory-Optimized Droplets are now in general availability and are available in the SFO3 and TOR1 datacenter regions.
We have reenabled the creation of Spaces in NYC3 now that the datacenter‚Äôs capacity upgrade is complete. The ability to create new Spaces in FRA1 remains disabled while we finish that datacenter capacity upgrade.

We have reenabled the creation of Spaces in NYC3 now that the datacenter‚Äôs capacity upgrade is complete. The ability to create new Spaces in FRA1 remains disabled while we finish that datacenter capacity upgrade.
Released v1.46.0 of doctl, the official DigitalOcean CLI. This release includes support for install Kubernetes 1-Click Apps when creating a cluster, surge upgrade support for Kubernetes clusters, and more.

Released v1.46.0 of doctl, the official DigitalOcean CLI. This release includes support for install Kubernetes 1-Click Apps when creating a cluster, surge upgrade support for Kubernetes clusters, and more.
v1.21.0 of the DigitalOcean Terraform Provider is now available. This release includes the addition of https to the list of acceptable healthcheck protocols for the digitalocean_loadbalancer resource.

v1.21.0 of the DigitalOcean Terraform Provider is now available. This release includes the addition of https to the list of acceptable healthcheck protocols for the digitalocean_loadbalancer resource.
Load balancer health checks now support the HTTPS protocol. You can now configure load balancers to verify the health of your Droplets‚Äô HTTPS endpoints.

Load balancer health checks now support the HTTPS protocol. You can now configure load balancers to verify the health of your Droplets‚Äô HTTPS endpoints.
PostgreSQL 12 is now available for database clusters. You can also now perform in-place upgrades for PostgreSQL clusters to newer versions without any downtime. We currently support PostgreSQL 10, 11, and 12.

PostgreSQL 12 is now available for database clusters. You can also now perform in-place upgrades for PostgreSQL clusters to newer versions without any downtime. We currently support PostgreSQL 10, 11, and 12.
Load balancer health checks now support the HTTPS protocol. You can now configure load balancers to verify the health of your Droplets‚Äô HTTPS endpoints.

Load balancer health checks now support the HTTPS protocol. You can now configure load balancers to verify the health of your Droplets‚Äô HTTPS endpoints.
Tax collection for Saudi Arabia has begun. Charges will appear on the August invoice.

Tax collection for Saudi Arabia has begun. Charges will appear on the August invoice.
State tax collection for Arizona, Hawaii, New York, Pennsylvania, Washington, and West Virginia has begun. Charges will appear on the August invoice. Learn more about tax for the United States of America.

State tax collection for Arizona, Hawaii, New York, Pennsylvania, Washington, and West Virginia has begun. Charges will appear on the August invoice. Learn more about tax for the United States of America.
You can now remove all global SQL modes from MySQL database clusters. Global SQL modes affect the SQL syntax MySQL supports and the data validation checks it performs.

You can now remove all global SQL modes from MySQL database clusters. Global SQL modes affect the SQL syntax MySQL supports and the data validation checks it performs.
v1.20.0 of the DigitalOcean Terraform Provider is now available. This release includes the addition of a digitalocean_tags data source and improvements to other tag-related resources.

v1.20.0 of the DigitalOcean Terraform Provider is now available. This release includes the addition of a digitalocean_tags data source and improvements to other tag-related resources.
Released v1.45.1 of doctl, the official DigitalOcean CLI. This release updates doctl‚Äôs Snap packaging that allow Snap users to log in to DigitalOcean Container Registry using the doctl registry login command. To grant access doctl access to your Docker configuration, run snap connect doctl:dot-docker.

Released v1.45.1 of doctl, the official DigitalOcean CLI. This release updates doctl‚Äôs Snap packaging that allow Snap users to log in to DigitalOcean Container Registry using the doctl registry login command. To grant access doctl access to your Docker configuration, run snap connect doctl:dot-docker.
We have temporarily disabled the creation of new Spaces in FRA1 and NYC3 while we update capacity in these regions. Learn more about Spaces in FRA1 and NYC3.

We have temporarily disabled the creation of new Spaces in FRA1 and NYC3 while we update capacity in these regions. Learn more about Spaces in FRA1 and NYC3.
Learn more on Creation of New Spaces in FRA1 and NYC3 Disabled Until Late 2020.
Listing records for a domain now supports filtering by both name and type using query parameters. For example, to only include A records for a domain, send a GET request to /v2/domains/$DOMAIN_NAME/records?type=ATo only include records matching sub.example.com, send a GET request to /v2/domains/$DOMAIN_NAME/records?name=sub.example.com. name must be a fully qualified record name. Both name and type may be used together to further filter the records returned.

Listing records for a domain now supports filtering by both name and type using query parameters. For example, to only include A records for a domain, send a GET request to /v2/domains/$DOMAIN_NAME/records?type=ATo only include records matching sub.example.com, send a GET request to /v2/domains/$DOMAIN_NAME/records?name=sub.example.com. name must be a fully qualified record name. Both name and type may be used together to further filter the records returned.
The response body to POST requests creating multiple Droplets has been extended to include an actions link for each Droplet created. For example:
    "links": {
      "actions": [
        {
          "id": 24404896,
          "rel": "create",
          "href": "https://api.digitalocean.com/v2/actions/24404896"
        },
        {
          "id": 24404897,
          "rel": "create",
          "href": "https://api.digitalocean.com/v2/actions/24404897"
        }
      ]
    }
These can be used to check the status of each individual Droplet create event rather than polling each Droplet.

The response body to POST requests creating multiple Droplets has been extended to include an actions link for each Droplet created. For example:
These can be used to check the status of each individual Droplet create event rather than polling each Droplet.
Beginning 4 June 2020, you are required to create a primary key for each new table in any DigitalOcean Managed MySQL Database to improve cluster performance.

Beginning 4 June 2020, you are required to create a primary key for each new table in any DigitalOcean Managed MySQL Database to improve cluster performance.
v1.19.0 of the DigitalOcean Terraform Provider is now available. This release includes initial support the DigitalOcean Container Registry.

v1.19.0 of the DigitalOcean Terraform Provider is now available. This release includes initial support the DigitalOcean Container Registry.
Released v1.44.0 of doctl, the official DigitalOcean CLI. This release includes support for specifying a non-default VPC when creating Droplets, load balancers, and Kubernetes clusters. It also adds the ability to set an expiration time for container registry credentials. This can be useful when calling doctl registry login as part of a CI/CD process. A new doctl 1-click list subcommand is now also available.

Released v1.44.0 of doctl, the official DigitalOcean CLI. This release includes support for specifying a non-default VPC when creating Droplets, load balancers, and Kubernetes clusters. It also adds the ability to set an expiration time for container registry credentials. This can be useful when calling doctl registry login as part of a CI/CD process. A new doctl 1-click list subcommand is now also available.
Released v1.45.0 of doctl, the official DigitalOcean CLI. This release includes new doctl kubernetes 1-click list and doctl compute droplet 1-click list subcommands.

Released v1.45.0 of doctl, the official DigitalOcean CLI. This release includes new doctl kubernetes 1-click list and doctl compute droplet 1-click list subcommands.
Tax collection for Chile has begun. Charges will appear on the July invoice. Learn more about tax for Chile.

Tax collection for Chile has begun. Charges will appear on the July invoice. Learn more about tax for Chile.
CoreOS Container Linux has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the CoreOS image from our platform in 30 days.

CoreOS Container Linux has reached end of life. Per our image deprecation policy, this image is now only available via the API. We will remove the CoreOS image from our platform in 30 days.
The retention period for Droplet performance metrics has been decreased from 30 days to 14 days.

The retention period for Droplet performance metrics has been decreased from 30 days to 14 days.
Unassigned floating IP charges will now appear on invoices. The first charge will appear on July 2020 invoices for all floating IPs that were not assigned to Droplets during the month of June.

Unassigned floating IP charges will now appear on invoices. The first charge will appear on July 2020 invoices for all floating IPs that were not assigned to Droplets during the month of June.
Users can now search for Marketplace apps directly from the Droplet Create page.

Users can now search for Marketplace apps directly from the Droplet Create page.
The SFO3 datacenter region is now available.

The SFO3 datacenter region is now available.
The ability to choose a root password during Droplet creation has been reinstated.

The ability to choose a root password during Droplet creation has been reinstated.
It is now possible to adjust the behavior of the OAuth authorization flow by specifying a prompt and/or max_auth_age query parameter:

The prompt query parameter can be used to specify how the authorizing user should be authenticated.
The max_auth_age query parameter can be used to determine a deadline (in seconds) after which a user must re-authenticate on the control panel.

For more details, consult the OAuth documentation.

It is now possible to adjust the behavior of the OAuth authorization flow by specifying a prompt and/or max_auth_age query parameter:
For more details, consult the OAuth documentation.
v1.18.0 of the DigitalOcean Terraform Provider is now available. This release includes support for the backend keepalive option for the load balancer resource and data source.

v1.18.0 of the DigitalOcean Terraform Provider is now available. This release includes support for the backend keepalive option for the load balancer resource and data source.
Users can now use the API to destroy select resources associated with a Droplet when destroying a Droplet. You can destroy snapshots, volumes, or volume snapshots associated with a Droplet.

Users can now use the API to destroy select resources associated with a Droplet when destroying a Droplet. You can destroy snapshots, volumes, or volume snapshots associated with a Droplet.
It is now possible to destroy snapshots, volumes, and volume snapshots associated with a Droplet while destroying the Droplet itself in a single request. A number of new related endpoints are now available:

To list the resources that can be destroyed along with the Droplet, send a GET request to the /v2/droplets/$DROPLET_ID/destroy_with_associated_resources endpoint.
To destroy a Droplet along and a sub-set of its associated resources, send a DELETE request to the /v2/droplets/$DROPLET_ID/destroy_with_associated_resources/selective endpoint.
To destroy a Droplet along with all of its associated resources, send a DELETE request to the /v2/droplets/$DROPLET_ID/destroy_with_associated_resources/dangerous endpoint.
To check on the status of a request to destroy a Droplet with its associated resources, send a GET request to the /v2/droplets/$DROPLET_ID/destroy_with_associated_resources/status endpoint.
If the status reported any errors, the destroy request can be retried by sending a POST request to the /v2/droplets/$DROPLET_ID/destroy_with_associated_resources/retry endpoint.


It is now possible to destroy snapshots, volumes, and volume snapshots associated with a Droplet while destroying the Droplet itself in a single request. A number of new related endpoints are now available:
The DigitalOcean Virtual Private Cloud (VPC) service is now available for all customers. VPC replaces the private networking service. Existing private networks will continue to function as normal but with the enhanced security and features of the VPC service. See the description of VPC features for more information.

The DigitalOcean Virtual Private Cloud (VPC) service is now available for all customers. VPC replaces the private networking service. Existing private networks will continue to function as normal but with the enhanced security and features of the VPC service. See the description of VPC features for more information.
v1.17.0 of the DigitalOcean Terraform Provider is now available. This release includes bug fixes and new Spaces data sources.

v1.17.0 of the DigitalOcean Terraform Provider is now available. This release includes bug fixes and new Spaces data sources.
Load balancers now allow you to set a keepalive option for target Droplets.

Load balancers now allow you to set a keepalive option for target Droplets.
Released v1.43.0 of doctl, the official DigitalOcean CLI. This release updates container registry features from beta to early access.

Released v1.43.0 of doctl, the official DigitalOcean CLI. This release updates container registry features from beta to early access.
Fedora 32 base image is now available in the control panel and via the API.

Fedora 32 base image is now available in the control panel and via the API.
DigitalOcean Load balancers now allow you to set a keepalive option for forwarding rules. Enabling this option allows the load balancer to use fewer active TCP connections to send and receive HTTP requests between the load balancer and your target Droplets.

DigitalOcean Load balancers now allow you to set a keepalive option for forwarding rules. Enabling this option allows the load balancer to use fewer active TCP connections to send and receive HTTP requests between the load balancer and your target Droplets.
Released v1.42.0 of doctl, the official DigitalOcean CLI. This release includes a number of small UI improvements and support for additional container registry beta features.

Released v1.42.0 of doctl, the official DigitalOcean CLI. This release includes a number of small UI improvements and support for additional container registry beta features.
Ubuntu 20.04 LTS base image is now available in the control panel and via the API.

Ubuntu 20.04 LTS base image is now available in the control panel and via the API.
The option to set your own root password during Droplet creation has been temporarily removed. You can still choose to connect to your Droplet using an SSH key or an automatically-generated password via email.

The option to set your own root password during Droplet creation has been temporarily removed. You can still choose to connect to your Droplet using an SSH key or an automatically-generated password via email.
You can now choose a root password during Droplet creation rather than receiving an automatically-generated password via email.

You can now choose a root password during Droplet creation rather than receiving an automatically-generated password via email.
v1.16.0 of the DigitalOcean Terraform Provider is now available. This release includes VPC support and expanded Spaces support.

v1.16.0 of the DigitalOcean Terraform Provider is now available. This release includes VPC support and expanded Spaces support.
The login page now provides quick access to your last-used login method.

The login page now provides quick access to your last-used login method.
Released v1.41.0 of doctl, the official DigitalOcean CLI. This release fixes the link to create a new API token when running doctl auth init.

Released v1.41.0 of doctl, the official DigitalOcean CLI. This release fixes the link to create a new API token when running doctl auth init.
We began the incremental release of the DigitalOcean Virtual Private Cloud (VPC) service. It will be available for all customers soon. VPC replaces the private networking service.

We began the incremental release of the DigitalOcean Virtual Private Cloud (VPC) service. It will be available for all customers soon. VPC replaces the private networking service.
Released v1.40.0 of doctl, the official DigitalOcean CLI. This release includes a support for VPCs and viewing billing history for an account.

Released v1.40.0 of doctl, the official DigitalOcean CLI. This release includes a support for VPCs and viewing billing history for an account.
Starting today, we have begun the incremental release of DigitalOcean VPC, including its API. VPCs (virtual private clouds) allow you to create virtual networks containing resources that can communicate with each other in full isolation using private IP addresses. The VPC service will be available for all customers soon. It replaces the existing private networking service.
When enabled on your account, you will be able to create, configure, list, and delete custom VPCs as well as retrieve information about the resources assigned to them. For example, to create a new VPC, make a POST to the /v2/vpcs endpoint with a JSON body like:
{
  "name": "staging-vpc",
  "description": "VPC for the staging environment"
  "region": "nyc1"
}
For the more details, see the full API reference documentation for DigitalOcean VPCs.
Related Changes
This release contains related functionality for a number of other DigitalOcean resources.
When VPC is enabled on your account, the private_networking attribute previously used to enable private networking while creating a Droplet will now provision the Droplet inside of your account‚Äôs default VPC for the region. Use the new vpc_uuid attribute to specify a different VPC.
Kubernetes clusters, load balancers, and database clusters will also be provisioned inside of your account‚Äôs default VPC for the region when enabled. To specify a non-default VPC, set the appropriate attribute in the JSON body of the create request:



Resource
Attribute




Droplet
vpc_uuid


Kubernetes cluster
vpc_uuid


Load balancer
vpc_uuid


Database cluster
private_networking_uuid




Starting today, we have begun the incremental release of DigitalOcean VPC, including its API. VPCs (virtual private clouds) allow you to create virtual networks containing resources that can communicate with each other in full isolation using private IP addresses. The VPC service will be available for all customers soon. It replaces the existing private networking service.
When enabled on your account, you will be able to create, configure, list, and delete custom VPCs as well as retrieve information about the resources assigned to them. For example, to create a new VPC, make a POST to the /v2/vpcs endpoint with a JSON body like:
For the more details, see the full API reference documentation for DigitalOcean VPCs.
This release contains related functionality for a number of other DigitalOcean resources.
When VPC is enabled on your account, the private_networking attribute previously used to enable private networking while creating a Droplet will now provision the Droplet inside of your account‚Äôs default VPC for the region. Use the new vpc_uuid attribute to specify a different VPC.
Kubernetes clusters, load balancers, and database clusters will also be provisioned inside of your account‚Äôs default VPC for the region when enabled. To specify a non-default VPC, set the appropriate attribute in the JSON body of the create request:
Business customers in Iceland can now enter their VAT IDs on the billing page. This removes tax charges on future invoices. Learn more about Iceland taxes.

Business customers in Iceland can now enter their VAT IDs on the billing page. This removes tax charges on future invoices. Learn more about Iceland taxes.
The Spaces CDN now has separate caches for unique URLs, including query strings.

The Spaces CDN now has separate caches for unique URLs, including query strings.
v1.15.0 of the DigitalOcean Terraform Provider is now available. This release includes new data sources for accessing information about DigitalOcean regions, images, and projects, a new resource for adding resources to projects not created via Terraform, and a number of other improvements.

v1.15.0 of the DigitalOcean Terraform Provider is now available. This release includes new data sources for accessing information about DigitalOcean regions, images, and projects, a new resource for adding resources to projects not created via Terraform, and a number of other improvements.
Released v1.39.0 of doctl, the official DigitalOcean CLI. This release includes support for listing and retrieving invoices as well as expanded help output for all commands.

Released v1.39.0 of doctl, the official DigitalOcean CLI. This release includes support for listing and retrieving invoices as well as expanded help output for all commands.
Updated the Droplet-based Marketplace WordPress 1-Click application to now use PHP 7.4 and MySQL server 8.0.19.

Updated the Droplet-based Marketplace WordPress 1-Click application to now use PHP 7.4 and MySQL server 8.0.19.
The sign-in experience has been redesigned to help streamline two-factor authentication workflows and enable special announcements.

The sign-in experience has been redesigned to help streamline two-factor authentication workflows and enable special announcements.
Users who create a 1-Click application can now access the instructions for their app by clicking the Get Started link next to their Droplet on the project homepage.

Users who create a 1-Click application can now access the instructions for their app by clicking the Get Started link next to their Droplet on the project homepage.
You can now only rebuild Droplets from images that use an OS that resides in the same OS family as the Droplet being rebuilt. For example, a Droplet running Ubuntu 16 can be rebuilt from an image running Ubuntu 18, but it cannot be built from an image running Debian.

You can now only rebuild Droplets from images that use an OS that resides in the same OS family as the Droplet being rebuilt. For example, a Droplet running Ubuntu 16 can be rebuilt from an image running Ubuntu 18, but it cannot be built from an image running Debian.
Users can now upload custom images using FTP URLs.

Users can now upload custom images using FTP URLs.
We have finished expanding the AMS3 datacenter to address capacity and load issues with Spaces in that region. As a result, we have reenabled the creation of new Spaces in AMS3. The allowance and rate limits on uploads to Spaces in AMS3 will stay in place to ensure high performance.

We have finished expanding the AMS3 datacenter to address capacity and load issues with Spaces in that region. As a result, we have reenabled the creation of new Spaces in AMS3. The allowance and rate limits on uploads to Spaces in AMS3 will stay in place to ensure high performance.
Learn more on Restrictions on Spaces in AMS3 Applied Until Datacenter Expansion Planned for Early 2020.
Debian 10.3 and 9.12 base images are now available in the control panel and via the API.

Debian 10.3 and 9.12 base images are now available in the control panel and via the API.
Users can now destroy select resources associated with a Droplet when destroying a Droplet.

Users can now destroy select resources associated with a Droplet when destroying a Droplet.
Users can now set legacy MySQL 5x password encryption for MySQL 8+ Managed Databases from the control panel and API.

Users can now set legacy MySQL 5x password encryption for MySQL 8+ Managed Databases from the control panel and API.
The DigitalOcean managed databases API now supports configuring the user
authentication plug-in for both new and existing MySQL users. This is useful
when needing to connect to a MySQL 8.0 cluster using an application or older
MySQL client that does not support the default caching_sha2_password
authentication plug-in.
For example, to create a new MySQL user
using the mysql_native_password authentication plug-in, send a POST request
to /v2/databases/$DATABASE_ID/users with a JSON body like:
{
  "name": "php-app-01",
  "mysql_settings": {
    "auth_plugin": "mysql_native_password"
  }
}
For more details, see the full reference documentation for the managed databases API.

The DigitalOcean managed databases API now supports configuring the user
authentication plug-in for both new and existing MySQL users. This is useful
when needing to connect to a MySQL 8.0 cluster using an application or older
MySQL client that does not support the default caching_sha2_password
authentication plug-in.
For example, to create a new MySQL user
using the mysql_native_password authentication plug-in, send a POST request
to /v2/databases/$DATABASE_ID/users with a JSON body like:
For more details, see the full reference documentation for the managed databases API.
Tax collection for Iceland has begun. Charges will appear on the April invoice.

Tax collection for Iceland has begun. Charges will appear on the April invoice.
We began the incremental release of a feature that allows users to destroy select resources associated with a Droplet when they destroy the Droplet.

We began the incremental release of a feature that allows users to destroy select resources associated with a Droplet when they destroy the Droplet.
We have renamed the Limited Availability (LA) product lifecycle stage to Early Availability (EA) to better represent the status of products in that stage. Products in Early Availability are fully functional but not yet production-ready, and may be enabled only for specific user groups as part of an incremental roll-out strategy.

We have renamed the Limited Availability (LA) product lifecycle stage to Early Availability (EA) to better represent the status of products in that stage. Products in Early Availability are fully functional but not yet production-ready, and may be enabled only for specific user groups as part of an incremental roll-out strategy.
To provide a better service for all customers we are introducing burst request rate limits to our public API. Now clients will be rate limited if they consume more than 5% of their total requests for an hour over a 1 minute period (going over 250 requests in a minute). This only affects clients making their requests in large bursts, clients that spread their requests over time will not be affected. Check the rate limits documentation for more information about it.

To provide a better service for all customers we are introducing burst request rate limits to our public API. Now clients will be rate limited if they consume more than 5% of their total requests for an hour over a 1 minute period (going over 250 requests in a minute). This only affects clients making their requests in large bursts, clients that spread their requests over time will not be affected. Check the rate limits documentation for more information about it.
v1.14.0 of the DigitalOcean Terraform Provider is now available. This release includes a bug fix for projects containing many resources and exposes the Droplet IDs for individual nodes in Kubernetes clusters.

v1.14.0 of the DigitalOcean Terraform Provider is now available. This release includes a bug fix for projects containing many resources and exposes the Droplet IDs for individual nodes in Kubernetes clusters.
Released several improvements for seeking support, including a new support starting page that allows you to search DigitalOcean‚Äôs product documentation, Marketplace, and community tutorials from a single location.

Released several improvements for seeking support, including a new support starting page that allows you to search DigitalOcean‚Äôs product documentation, Marketplace, and community tutorials from a single location.
Released v1.38.0 of doctl, the official DigitalOcean CLI. This release adds the ability to set Kubernetes node pool labels as well as support for deleting multiple Kubernetes clusters with a single command.

Released v1.38.0 of doctl, the official DigitalOcean CLI. This release adds the ability to set Kubernetes node pool labels as well as support for deleting multiple Kubernetes clusters with a single command.
Our managed DigitalOcean Kubernetes product DOKS now supports setting Kubernetes labels on node pools. Once assigned, they will propagate to the associated pool nodes, both existing and new ones. This way, customers may reliably reference groups of nodes through label selectors that Kubernetes provides.
Labels can be set on node pool API objects that are accessible on multiple endpoints. For instance, updating an existing node pool by the labels service=web and priority=high is done by submitting a PUT request to the /v2/kubernetes/clusters/<cluster ID>/node_pools/<node ID> endpoint with the following JSON body:
{
  "name": "web",
  "count": 10,
  "labels": {
    "service": "web",
    "priority": "high"
  }
}
For details, see the available operations on the Kubernetes API.

Our managed DigitalOcean Kubernetes product DOKS now supports setting Kubernetes labels on node pools. Once assigned, they will propagate to the associated pool nodes, both existing and new ones. This way, customers may reliably reference groups of nodes through label selectors that Kubernetes provides.
Labels can be set on node pool API objects that are accessible on multiple endpoints. For instance, updating an existing node pool by the labels service=web and priority=high is done by submitting a PUT request to the /v2/kubernetes/clusters/<cluster ID>/node_pools/<node ID> endpoint with the following JSON body:
For details, see the available operations on the Kubernetes API.
Ubuntu 19.04, FreeBSD 12.0, Fedora 28, Fedora 28 Atomic, and Feodra 29 have reached end of life. Per our image deprecation policy, these images are now only available via the API. We will remove the these images from our platform in 30 days.

Ubuntu 19.04, FreeBSD 12.0, Fedora 28, Fedora 28 Atomic, and Feodra 29 have reached end of life. Per our image deprecation policy, these images are now only available via the API. We will remove the these images from our platform in 30 days.
v1.13.0 of the DigitalOcean Terraform Provider is now available. This release adds support for tagging Managed Databases clusters.

v1.13.0 of the DigitalOcean Terraform Provider is now available. This release adds support for tagging Managed Databases clusters.
RancherOS 1.5.5 base image is now available in the control panel and via the API.

RancherOS 1.5.5 base image is now available in the control panel and via the API.
CentOS 8.1 base image is now available in the control panel and via the API.

CentOS 8.1 base image is now available in the control panel and via the API.
Released v1.37.0 of doctl, the official DigitalOcean CLI. This release adds the ability to retrieve account balances.

Released v1.37.0 of doctl, the official DigitalOcean CLI. This release adds the ability to retrieve account balances.
Our API has been extended with a new endpoint enabling you to retrieve your account balance. For more information, see the balance endpoint in the API reference documentation.

Our API has been extended with a new endpoint enabling you to retrieve your account balance. For more information, see the balance endpoint in the API reference documentation.
Our API has been extended with a new endpoint enabling you to retrieve balance
information for an account. To do so, make a GET request to /v2/customers/my/balance.
The response will be a JSON body including your balance details. For example:
{
  "month_to_date_balance": "23.44",
  "account_balance": "12.23",
  "month_to_date_usage": "11.21",
  "generated_at": "2019-07-09T15:01:12Z"
}
For all the details, see the balance endpoint
in the full API reference documentation.

Our API has been extended with a new endpoint enabling you to retrieve balance
information for an account. To do so, make a GET request to /v2/customers/my/balance.
The response will be a JSON body including your balance details. For example:
For all the details, see the balance endpoint
in the full API reference documentation.
Tax collection for Malaysia has begun.

Tax collection for Malaysia has begun.
Tax collection for Singapore has begun.

Tax collection for Singapore has begun.
Released v1.12.0 of the DigitalOcean Terraform Provider. This release contains improvements to Managed Database support including a new resource for configuring trusted sources and the ability to set the global SQL mode or Redis eviction policy on a cluster. There is also a new data source for finding supported Kubernetes versions. Learn more on the Terraform Changelog.

Released v1.12.0 of the DigitalOcean Terraform Provider. This release contains improvements to Managed Database support including a new resource for configuring trusted sources and the ability to set the global SQL mode or Redis eviction policy on a cluster. There is also a new data source for finding supported Kubernetes versions. Learn more on the Terraform Changelog.
Users can now specify the payment method and amount when making payments on the billing page.

Users can now specify the payment method and amount when making payments on the billing page.
Fedora 31 base image is now available in the control panel and via the API.

Fedora 31 base image is now available in the control panel and via the API.
Bandwidth billing for Managed Databases, originally slated to begin in January 2020, has been postponed to 2021. Egress bandwidth for Managed Databases clusters will continue to be waived until then.

Bandwidth billing for Managed Databases, originally slated to begin in January 2020, has been postponed to 2021. Egress bandwidth for Managed Databases clusters will continue to be waived until then.
Debian 10.2 base image is now available in the control panel and via the API.

Debian 10.2 base image is now available in the control panel and via the API.
Our referral program offer has changed from $50 for 30 days to $100 for 60 days. This change applies only to new referrals. Existing users with referral credits will retain their current balance and credit expiration dates.

Our referral program offer has changed from $50 for 30 days to $100 for 60 days. This change applies only to new referrals. Existing users with referral credits will retain their current balance and credit expiration dates.
FreeBSD 12.1 (ufs & zfs) base image is now available in the control panel and via the API.

FreeBSD 12.1 (ufs & zfs) base image is now available in the control panel and via the API.
The DigitalOcean API currently offers the ability to retrieve a report of all
Droplets co-located on the same physical hardware by sending a GET request to
the /v2/reports/droplet_neighbors endpoint. This endpoint has been deprecated
and will begin responding with an HTTP status of 410 (Gone) on December 17th, 2019.
Today, in its place, a new endpoint is now available: /v2/reports/droplet_neighbors_ids.
Rather than listing the full Droplet object, responses from this endpoint only
contain sets of Droplet IDs co-located on the same physical hardware. For example:
{
  "neighbor_ids": [
    [168671828,168663509,168671815],
    [168671883,168671750]
  ]
}
This implementation is more performant and better able to scale for users with
many Droplets. For all the information, find the full API reference documentation here.
The API also continues to offer the ability to list ‚Äúneighbors‚Äù for a specific
Droplet by sending a GET request to /v2/droplets/$DROPLET_ID/neighbors. This
endpoint will continue to function without change.
We apologize for the inconvenience. If you need guidance on this transition, reach out to the team by opening a support ticket.

The DigitalOcean API currently offers the ability to retrieve a report of all
Droplets co-located on the same physical hardware by sending a GET request to
the /v2/reports/droplet_neighbors endpoint. This endpoint has been deprecated
and will begin responding with an HTTP status of 410 (Gone) on December 17th, 2019.
Today, in its place, a new endpoint is now available: /v2/reports/droplet_neighbors_ids.
Rather than listing the full Droplet object, responses from this endpoint only
contain sets of Droplet IDs co-located on the same physical hardware. For example:
This implementation is more performant and better able to scale for users with
many Droplets. For all the information, find the full API reference documentation here.
The API also continues to offer the ability to list ‚Äúneighbors‚Äù for a specific
Droplet by sending a GET request to /v2/droplets/$DROPLET_ID/neighbors. This
endpoint will continue to function without change.
We apologize for the inconvenience. If you need guidance on this transition, reach out to the team by opening a support ticket.
Tax collection for Belarus has begun. Charges will appear on the January invoice.

Tax collection for Belarus has begun. Charges will appear on the January invoice.
MySQL managed database clusters now support setting the global SQL mode via the control panel and the API. See How to Set Global SQL Mode on MySQL Clusters for more information.

MySQL managed database clusters now support setting the global SQL mode via the control panel and the API. See How to Set Global SQL Mode on MySQL Clusters for more information.
Users can now use the DigitalOcean API to set and modify trusted sources for managed databases to restrict incoming connections.

Users can now use the DigitalOcean API to set and modify trusted sources for managed databases to restrict incoming connections.
DigitalOcean‚Äôs API now supports managing a database cluster‚Äôs firewall rules (known as ‚Äútrusted sources‚Äù in the control panel) as well as the ability to configure the SQL mode used by MySQL clusters.
Using the /v2/databases/$DATABASE_ID/firewall endpoint, you can specify which resources should be able to open connections to your database. You may limit connections to specific Droplets, Kubernetes clusters, or external IP addresses. When a tag is provided, any Droplet or Kubernetes node with that tag applied to it will have access. For example, the body a PUT request might look like:
{
  "rules": [
    {"type": "ip_addr", "value": "192.168.1.1"},
    {"type": "droplet", "value": "163973392"},
    {"type": "k8s", "value": "ff2a6c52-5a44-4b63-b99c-0e98e7a63d61"},
    {"type": "tag", "value": "backend"}
  ]
}
To configure the SQL modes for a MySQL cluster, use the /v2/databases/$DATABASE_ID/sql_mode endpoint. For example, the body a PUT request might look like:
{
  "sql_mode": "ANSI,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE"
}
For more information, see the full API reference documentation for managed databases.

DigitalOcean‚Äôs API now supports managing a database cluster‚Äôs firewall rules (known as ‚Äútrusted sources‚Äù in the control panel) as well as the ability to configure the SQL mode used by MySQL clusters.
Using the /v2/databases/$DATABASE_ID/firewall endpoint, you can specify which resources should be able to open connections to your database. You may limit connections to specific Droplets, Kubernetes clusters, or external IP addresses. When a tag is provided, any Droplet or Kubernetes node with that tag applied to it will have access. For example, the body a PUT request might look like:
To configure the SQL modes for a MySQL cluster, use the /v2/databases/$DATABASE_ID/sql_mode endpoint. For example, the body a PUT request might look like:
For more information, see the full API reference documentation for managed databases.
DigitalOcean Container Registry has been released in Beta. To request early access, visit the homepage for Container Registry.

DigitalOcean Container Registry has been released in Beta. To request early access, visit the homepage for Container Registry.
DigitalOcean Kubernetes users can run our cluster linter before upgrading their cluster to a new minor version. The linter automatically finds issues with your cluster and links to recommended fixes.

DigitalOcean Kubernetes users can run our cluster linter before upgrading their cluster to a new minor version. The linter automatically finds issues with your cluster and links to recommended fixes.
DigitalOcean Kubernetes has added native support for the Kubernetes Dashboard for all DOKS clusters.

DigitalOcean Kubernetes has added native support for the Kubernetes Dashboard for all DOKS clusters.
Team owners can now require secure sign-in for teams.

Team owners can now require secure sign-in for teams.
Released v1.11.0 of the DigitalOcean Terraform Provider. Learn more in the Terraform Changelog.

Released v1.11.0 of the DigitalOcean Terraform Provider. Learn more in the Terraform Changelog.
Memory-Optimized Droplets are now available in the SGP1 datacenter region. See Choosing the Right Droplet Plan for more information.

Memory-Optimized Droplets are now available in the SGP1 datacenter region. See Choosing the Right Droplet Plan for more information.
General Purpose Droplets are now available in the LON1 datacenter region. See Choosing the Right Droplet Plan for more information.

General Purpose Droplets are now available in the LON1 datacenter region. See Choosing the Right Droplet Plan for more information.
State tax collection for the United States of America has begun. Charges will appear on the December invoice.

State tax collection for the United States of America has begun. Charges will appear on the December invoice.
Released v1.10.0 of the DigitalOcean Terraform Provider. Learn more in the Terraform Changelog.

Released v1.10.0 of the DigitalOcean Terraform Provider. Learn more in the Terraform Changelog.
Memory-Optimized Droplets are now in general availability and are available in the NYC1 and SFO2 datacenter regions.

Memory-Optimized Droplets are now in general availability and are available in the NYC1 and SFO2 datacenter regions.
DigitalOcean Load Balancers no longer support downgrading TLS connections to TLS 1.1.

DigitalOcean Load Balancers no longer support downgrading TLS connections to TLS 1.1.
Ubuntu 19.10 base image is now available in the control panel and via the API.

Ubuntu 19.10 base image is now available in the control panel and via the API.
Users can now see the sign-in method (email, email + 2FA, Google, or GitHub) for team members on the team account page in the control panel.

Users can now see the sign-in method (email, email + 2FA, Google, or GitHub) for team members on the team account page in the control panel.
The DigitalOcean Kubernetes (DOKS) October release is now available, and contains the following new features:


Cluster autoscaling.


Connecting to clusters with OAuth tokens.


Support for minor version upgrades.



The DigitalOcean Kubernetes (DOKS) October release is now available, and contains the following new features:
Cluster autoscaling.
Connecting to clusters with OAuth tokens.
Support for minor version upgrades.
Released v1.8.0 of the DigitalOcean Terraform Provider. Learn more in the Terraform Changelog.

Released v1.8.0 of the DigitalOcean Terraform Provider. Learn more in the Terraform Changelog.
Redis Managed Databases are now in General Availability with the addition of monitoring insights.

Redis Managed Databases are now in General Availability with the addition of monitoring insights.
DigitalOcean now supports 3-D Secure (3DS) second-factor payment authentication, allowing us to accept payment from banks that require it.

DigitalOcean now supports 3-D Secure (3DS) second-factor payment authentication, allowing us to accept payment from banks that require it.
The Billing page in the control panel now splits the costs displayed between payment due and the amount not yet billed for the active billing cycle.

The Billing page in the control panel now splits the costs displayed between payment due and the amount not yet billed for the active billing cycle.
Memory-Optimized Droplets are now in Limited Availability in the NYC3 and AMS3 regions. See Introducing Memory-Optimized Droplets with 8 GB RAM for Each Dedicated vCPU to learn more.

Memory-Optimized Droplets are now in Limited Availability in the NYC3 and AMS3 regions. See Introducing Memory-Optimized Droplets with 8 GB RAM for Each Dedicated vCPU to learn more.
The OpenEBS (Kubernetes) One-Click Application has been released.

The OpenEBS (Kubernetes) One-Click Application has been released.
Managed Databases for MySQL and Redis are now available in SGP1, BLR1, and TOR1, and MySQL is now in General Availability. Learn more in the MySQL and Redis announcement blog post.

Managed Databases for MySQL and Redis are now available in SGP1, BLR1, and TOR1, and MySQL is now in General Availability. Learn more in the MySQL and Redis announcement blog post.
The Chamilo One-Click Application has been released.

The Chamilo One-Click Application has been released.
Value Added Tax (VAT) collection for South Korea and Quebec Sales Tax (QST) collection for Quebec, Canada have begun. Charges will appear on the October invoice.

Value Added Tax (VAT) collection for South Korea and Quebec Sales Tax (QST) collection for Quebec, Canada have begun. Charges will appear on the October invoice.
Released Version 1.7.0 of the DigitalOcean Terraform provider.

Released Version 1.7.0 of the DigitalOcean Terraform provider.
Managed Databases for MySQL and Redis in Limited Availability are now available in the AMS3, LON1, and NYC3 datacenter regions. Learn more in the MySQL and Redis announcement blog post.

Managed Databases for MySQL and Redis in Limited Availability are now available in the AMS3, LON1, and NYC3 datacenter regions. Learn more in the MySQL and Redis announcement blog post.
Volume limits for verified accounts have been raised from 10 volumes per account/500 GB of volume data per region to 100 volumes per account/16 TB per region. Unverified accounts are still limited to 10 volumes/500 GB. Learn more about account verification.

Volume limits for verified accounts have been raised from 10 volumes per account/500 GB of volume data per region to 100 volumes per account/16 TB per region. Unverified accounts are still limited to 10 volumes/500 GB. Learn more about account verification.
Users can now specify an account address within their Billing Settings. This address will be reflected on invoices and will also be used to determine tax location. Learn more about tax locations.

Users can now specify an account address within their Billing Settings. This address will be reflected on invoices and will also be used to determine tax location. Learn more about tax locations.
The /v2/volumes?name=$VOLUME_NAME endpoint now lists all volumes that match the specified name as a query parameter. For more information, see the API v2 reference documentation on list volumes filtered by name.

The /v2/volumes?name=$VOLUME_NAME endpoint now lists all volumes that match the specified name as a query parameter. For more information, see the API v2 reference documentation on list volumes filtered by name.
Managed Databases for MySQL and Redis have been released in early availability in the NYC1, FRA1, and SFO2 datacenter regions. Learn more in the MySQL and Redis announcement blog post.

Managed Databases for MySQL and Redis have been released in early availability in the NYC1, FRA1, and SFO2 datacenter regions. Learn more in the MySQL and Redis announcement blog post.
DigitalOcean Load Balancers no longer support downgrading TLS connections to TLS 1.0. We will stop supporting TLS 1.1 later this year.

DigitalOcean Load Balancers no longer support downgrading TLS connections to TLS 1.0. We will stop supporting TLS 1.1 later this year.
Today DigitalOcean‚Äôs Managed Database service launched support for two new database engines, MySQL and Redis. Both are currently in Limited Availability and can initially be used in the NYC1, FRA1, and SFO2 regions.
When creating a new database cluster using the API, you must specify the engine attribute to select which type of database to use (mysql for MySQL or redis for Redis). For example, to create a new Redis cluster, make a POST to the /v2/databases endpoint with a JSON body like:
{
  "name": "cache-01",
  "engine": "redis",
  "version": "5",
  "region": "nyc1",
  "size": "db-s-1vcpu-2gb",
  "num_nodes": 2
}
See the full API reference documentation for all the details. For more information about DigitalOcean Managed Databases including the roll-out plan for additional regions, check out the blog post announcing the release.

Today DigitalOcean‚Äôs Managed Database service launched support for two new database engines, MySQL and Redis. Both are currently in Limited Availability and can initially be used in the NYC1, FRA1, and SFO2 regions.
When creating a new database cluster using the API, you must specify the engine attribute to select which type of database to use (mysql for MySQL or redis for Redis). For example, to create a new Redis cluster, make a POST to the /v2/databases endpoint with a JSON body like:
See the full API reference documentation for all the details. For more information about DigitalOcean Managed Databases including the roll-out plan for additional regions, check out the blog post announcing the release.
You can now create a maximum of one snapshot of a volume every 10 minutes. See the snapshots overview for more details.

You can now create a maximum of one snapshot of a volume every 10 minutes. See the snapshots overview for more details.
Floating IP Address rate limit information was added to the DigitalOcean API Documentation.

Floating IP Address rate limit information was added to the DigitalOcean API Documentation.
Began the incremental release of new block storage volume limits. By the end of the release, all verified accounts will be able to create up to 100 volumes or use a total of 16 TB of volume data per region. Unverified accounts will be allowed 10 volumes or to use a total of 500 GB per region.

Began the incremental release of new block storage volume limits. By the end of the release, all verified accounts will be able to create up to 100 volumes or use a total of 16 TB of volume data per region. Unverified accounts will be allowed 10 volumes or to use a total of 500 GB per region.
The default Ubuntu x64 base image has been updated from 18.04.1 to 18.04.3. For details about 18.04.3, see the Ubuntu release notes.

The default Ubuntu x64 base image has been updated from 18.04.1 to 18.04.3. For details about 18.04.3, see the Ubuntu release notes.
The credit card input form on the billing page in the control panel has been modified to simplify billing address entry.

The credit card input form on the billing page in the control panel has been modified to simplify billing address entry.
Released version 1.6.0 of the DigitalOcean Terraform provider.

Released version 1.6.0 of the DigitalOcean Terraform provider.
Released doctl version 1.24.1, which is also now available in Docker Hub. You can download it with docker pull digitalocean/doctl.

Released doctl version 1.24.1, which is also now available in Docker Hub. You can download it with docker pull digitalocean/doctl.
We‚Äôve disabled creating new Spaces in AMS3 until we complete maintenance on the datacenter as part of addressing Spaces performance concerns. Learn more about Spaces AMS3 availability.

We‚Äôve disabled creating new Spaces in AMS3 until we complete maintenance on the datacenter as part of addressing Spaces performance concerns. Learn more about Spaces AMS3 availability.
doctl version 1.22 was released.

doctl version 1.22 was released.
FreeBSD 11.3 base image is now available in the control panel and via the API.

FreeBSD 11.3 base image is now available in the control panel and via the API.
FreeBSD 12 (ufs & zfs) images have been updated to fix a bug related to private networking and SSH keys.

FreeBSD 12 (ufs & zfs) images have been updated to fix a bug related to private networking and SSH keys.
DigitalOcean users can now sign up and sign in to DigitalOcean with GitHub OAuth. Users can switch their login type between password-based, Google OAuth, and GitHub OAuth.

DigitalOcean users can now sign up and sign in to DigitalOcean with GitHub OAuth. Users can switch their login type between password-based, Google OAuth, and GitHub OAuth.
Downloadable CSV invoices available on the control panel billing page have been updated to include project names for each Droplet.

Downloadable CSV invoices available on the control panel billing page have been updated to include project names for each Droplet.
Debian 10 (buster) base image is now available in the control panel and via the API.

Debian 10 (buster) base image is now available in the control panel and via the API.
The installation repo for the metrics agent has been moved to DigitalOcean Spaces.

The installation repo for the metrics agent has been moved to DigitalOcean Spaces.
Kubernetes Monitoring Stack (Beta), FASTPANEL, SolidInvoice, and OpenCart third-party One-Click applications were released.

Kubernetes Monitoring Stack (Beta), FASTPANEL, SolidInvoice, and OpenCart third-party One-Click applications were released.
The Prometheus 2.9.2, RethinkDB 2.4.0, Mattermost 5.12.0, and Buddy third-party One-Click applications were released.

The Prometheus 2.9.2, RethinkDB 2.4.0, Mattermost 5.12.0, and Buddy third-party One-Click applications were released.
The ttl of a domain record now has a minimum value of 30 seconds, and if not set, the default value has changed from 1800 to the ttl of the SOA record.

The ttl of a domain record now has a minimum value of 30 seconds, and if not set, the default value has changed from 1800 to the ttl of the SOA record.
6-hour and 1-day alert policies for Droplets and Kubernetes worker nodes have been deprecated. No new alert policies with these intervals can be created. Existing alert policies using these intervals will remain in place until 1 August 2019, at which point they will be modified to reflect a 1-hour interval.

6-hour and 1-day alert policies for Droplets and Kubernetes worker nodes have been deprecated. No new alert policies with these intervals can be created. Existing alert policies using these intervals will remain in place until 1 August 2019, at which point they will be modified to reflect a 1-hour interval.
RancherOS v1.5.2 base images have replaced RancherOS v1.5.1 base images in the control panel and API.

RancherOS v1.5.2 base images have replaced RancherOS v1.5.1 base images in the control panel and API.
DigitalOcean Managed Databases now provide support for private networking. All new database clusters will be provisioned with private networking enabled. Existing clusters will require an update to connect over the private network. This can be triggered in the control panel.
Databases, read-only replicas, and connection pools will now contain a new private_connection object holding the information needed to access the resource via the private network. Its attributes are identical to the existing connection object, but the values for private_connection.uri and private_connection.host will contain FQDNs only accessible from resources (e.g. Droplets or Kubernetes clusters) within your account and in the same region.
For more information, see the full Managed Databases API documentation.

DigitalOcean Managed Databases now provide support for private networking. All new database clusters will be provisioned with private networking enabled. Existing clusters will require an update to connect over the private network. This can be triggered in the control panel.
Databases, read-only replicas, and connection pools will now contain a new private_connection object holding the information needed to access the resource via the private network. Its attributes are identical to the existing connection object, but the values for private_connection.uri and private_connection.host will contain FQDNs only accessible from resources (e.g. Droplets or Kubernetes clusters) within your account and in the same region.
For more information, see the full Managed Databases API documentation.
DigitalOcean Managed Databases now support private networking. New database clusters will provision with private networking enabled. Existing clusters will require an update to connect over the private network.

DigitalOcean Managed Databases now support private networking. New database clusters will provision with private networking enabled. Existing clusters will require an update to connect over the private network.
FreeBSD 12.0 (ufs & zfs) base images are now available in the control panel and via the API.

FreeBSD 12.0 (ufs & zfs) base images are now available in the control panel and via the API.
FreeBSD 10.4 (ufs & zfs) reached end of life and was removed from the control panel.

FreeBSD 10.4 (ufs & zfs) reached end of life and was removed from the control panel.
Fedora 27 reached end of life and was removed from the control panel.

Fedora 27 reached end of life and was removed from the control panel.
DigitalOcean Kubernetes is now Generally Available. Highlights include:


Availability in SGP1 and TOR1.


Support for patch version upgrades.


Configurable maintenance window and automatic upgrade options.


Delete node feature, which removes a specific node from a worker pool.


Basic and advanced monitoring insights for resource utilization and deployment status metrics.



DigitalOcean Kubernetes is now Generally Available. Highlights include:
Availability in SGP1 and TOR1.
Support for patch version upgrades.
Configurable maintenance window and automatic upgrade options.
Delete node feature, which removes a specific node from a worker pool.
Basic and advanced monitoring insights for resource utilization and deployment status metrics.
SOA records are now returned in record results, and you can update the TTL on a SOA record as you would with other records. This allows you to control the negative caching of your domain. SOA records cannot be manually deleted or created on a domain, they are created when the domain is created, and cleaned up on the domain deletion.

SOA records are now returned in record results, and you can update the TTL on a SOA record as you would with other records. This allows you to control the negative caching of your domain. SOA records cannot be manually deleted or created on a domain, they are created when the domain is created, and cleaned up on the domain deletion.
Today, we are promoting the Kubernetes API to General Availability. As part of this release, we have also extended the API with additional functionality:


When creating or updating a cluster, you may now configure a maintenance window policy specifying the day of the week and time of day that updates should take place for the cluster. Additionally, setting a cluster‚Äôs auto_upgrade attribute to true will specify that the cluster can be automatically upgraded to new Kubernetes patch releases (e.g.  1.13.1 to 1.13.2) during its maintenance window.


An upgrade endpoint is now available to imminently trigger an upgrade to a newer patch release of Kubernetes at your own convienience. You may list available upgrades for your cluster using the upgrades endpoint.


In order to give users finer control over individual nodes, the recycle endpoint has been deprecated. Instead, we now offer the ability to delete or replace specific nodes in a node pool. By default, workloads will be drained from the node before deletion. Appending the skip_drain=1 query parameter to the request will cause the node to be imminently deleted. Appending the replace=1 query parameter to the request will cause the node to be replaced by a new one after it has been deleted.


For the full details, see the API reference documentation for Kubernetes.
Thank you to everyone who took the time to provide us with feedback.

Today, we are promoting the Kubernetes API to General Availability. As part of this release, we have also extended the API with additional functionality:
When creating or updating a cluster, you may now configure a maintenance window policy specifying the day of the week and time of day that updates should take place for the cluster. Additionally, setting a cluster‚Äôs auto_upgrade attribute to true will specify that the cluster can be automatically upgraded to new Kubernetes patch releases (e.g.  1.13.1 to 1.13.2) during its maintenance window.
An upgrade endpoint is now available to imminently trigger an upgrade to a newer patch release of Kubernetes at your own convienience. You may list available upgrades for your cluster using the upgrades endpoint.
In order to give users finer control over individual nodes, the recycle endpoint has been deprecated. Instead, we now offer the ability to delete or replace specific nodes in a node pool. By default, workloads will be drained from the node before deletion. Appending the skip_drain=1 query parameter to the request will cause the node to be imminently deleted. Appending the replace=1 query parameter to the request will cause the node to be replaced by a new one after it has been deleted.
For the full details, see the API reference documentation for Kubernetes.
Thank you to everyone who took the time to provide us with feedback.
Our referral program offer has changed from $100 for 60 days to $50 for 30 days. This change applies only to new referrals. Existing users with referral credits will retain their current balance and credit expiration dates.

Our referral program offer has changed from $100 for 60 days to $50 for 30 days. This change applies only to new referrals. Existing users with referral credits will retain their current balance and credit expiration dates.
Managed Databases are now in General Availability. New features include enhanced monitoring insights, support for projects and tags, and availability in the Singapore (SGP1) region.

Managed Databases are now in General Availability. New features include enhanced monitoring insights, support for projects and tags, and availability in the Singapore (SGP1) region.
Released v1.3.0 of the DigitalOcean Terraform Provider. Learn more on the Terraform Changelog.

Released v1.3.0 of the DigitalOcean Terraform Provider. Learn more on the Terraform Changelog.
Spaces are now available in the Frankfurt (FRA1) region.

Spaces are now available in the Frankfurt (FRA1) region.
Creating Spaces in NYC3 is now re-enabled.

Creating Spaces in NYC3 is now re-enabled.
Kubernetes version 1.14.1 is now available for cluster creation in DOKS.

Kubernetes version 1.14.1 is now available for cluster creation in DOKS.
Value Added Tax (VAT) collection for Norway, South Africa, and the United Arab Emirates and Good and Services Tax (GST) collection for New Zealand have begun. Charges will appear on the June invoice.

Value Added Tax (VAT) collection for Norway, South Africa, and the United Arab Emirates and Good and Services Tax (GST) collection for New Zealand have begun. Charges will appear on the June invoice.
Fedora 30 base images are now available in the control panel and via the API using the slug fedora-30-x64.

Fedora 30 base images are now available in the control panel and via the API using the slug fedora-30-x64.
Ubuntu 14.04 reached end of life and was removed from the control panel.

Ubuntu 14.04 reached end of life and was removed from the control panel.
DOKS node pools can now be named at creation time.

DOKS node pools can now be named at creation time.
DOKS master nodes now automatically rotate logs to avoid disk space issues.

DOKS master nodes now automatically rotate logs to avoid disk space issues.
Released v1.2.0 of the DigitalOcean Terraform Provider. Learn more: https://do.co/terraform-changelog

Released v1.2.0 of the DigitalOcean Terraform Provider. Learn more: https://do.co/terraform-changelog
The control panel billing page now includes a breakdown of your spending and a downloadable PDF of your invoice.

The control panel billing page now includes a breakdown of your spending and a downloadable PDF of your invoice.
Ubuntu 19.04 base images are now available in the control panel and via the API using the slug ubuntu-19-04-x64.

Ubuntu 19.04 base images are now available in the control panel and via the API using the slug ubuntu-19-04-x64.
The /v2/volumes/$volume_id/snapshots endpoint now accepts tags at creation time, and these are reflected on the /v2/snapshots endpoint. Volume snapshot tags may now be managed with the /v2/tags endpoint as well. For more information, see the API reference documentation for both volumes and tags.

The /v2/volumes/$volume_id/snapshots endpoint now accepts tags at creation time, and these are reflected on the /v2/snapshots endpoint. Volume snapshot tags may now be managed with the /v2/tags endpoint as well. For more information, see the API reference documentation for both volumes and tags.
The ONLYOFFICE third-party One-Click application was released.

The ONLYOFFICE third-party One-Click application was released.
The new metrics agent is fully released into production. Highlights include:


A simpler way to contribute custom metrics


A new load average plot


Fedora 27 support


Process name collection opt-out


This will be the default agent used by our managed databases and Kubernetes products. All agent installations on or after this date will receive the new agent by default. On 8 July 2019, the legacy metrics agent will be deprecated, meaning users will no longer be able to view metrics from Droplets running the legacy agent. You can upgrade to the new agent at any time.

The new metrics agent is fully released into production. Highlights include:
A simpler way to contribute custom metrics
A new load average plot
Fedora 27 support
Process name collection opt-out
This will be the default agent used by our managed databases and Kubernetes products. All agent installations on or after this date will receive the new agent by default. On 8 July 2019, the legacy metrics agent will be deprecated, meaning users will no longer be able to view metrics from Droplets running the legacy agent. You can upgrade to the new agent at any time.
Spaces, DigitalOcean‚Äôs object storage solution, includes a built-in CDN. Today we‚Äôve added the ability to use custom subdomains with your CDN endpoints. When configuring your CDN via the API, you can now set the custom_domain attribute to use a subdomain with the endpoint. When a custom subdomain is in use, the certificate_id attribute is also required. Its value must be the ID of a DigitalOcean managed SSL certificate. For example, the body of your request to enable a CDN might look like:
{
  "origin": "static-images.nyc3.digitaloceanspaces.com",
  "certificate_id": "892071a0-bb95-49bc-8021-3afd67a210bf",
  "custom_domain": "static.example.com"
}
See here for more information about using the API to configure Spaces CDN endpoints.

Spaces, DigitalOcean‚Äôs object storage solution, includes a built-in CDN. Today we‚Äôve added the ability to use custom subdomains with your CDN endpoints. When configuring your CDN via the API, you can now set the custom_domain attribute to use a subdomain with the endpoint. When a custom subdomain is in use, the certificate_id attribute is also required. Its value must be the ID of a DigitalOcean managed SSL certificate. For example, the body of your request to enable a CDN might look like:
See here for more information about using the API to configure Spaces CDN endpoints.
The OpenVPN and GrandNode third-party One-Click applications were released.

The OpenVPN and GrandNode third-party One-Click applications were released.
General Purpose Performance Droplet plans are now in General Availability with the addition of SFO2, AMS3, and SGP1.

General Purpose Performance Droplet plans are now in General Availability with the addition of SFO2, AMS3, and SGP1.
To help customers track their credits, beginning in April we will send invoice emails when customers use any resources during a billing period, regardless of an account‚Äôs outstanding balance. Previously, invoices were sent only when the outstanding balance exceeded the threshold for automatic payments.

To help customers track their credits, beginning in April we will send invoice emails when customers use any resources during a billing period, regardless of an account‚Äôs outstanding balance. Previously, invoices were sent only when the outstanding balance exceeded the threshold for automatic payments.
The Zabbix and Mastodon third-party One-Click applications were released.

The Zabbix and Mastodon third-party One-Click applications were released.
Debian 8 has reached end of life and has been removed from the control panel and API.

Debian 8 has reached end of life and has been removed from the control panel and API.
The Acra, Gladius Accelerator, and Selenoid third-party One-Click applications were released.

The Acra, Gladius Accelerator, and Selenoid third-party One-Click applications were released.
DOKS customers will now be able to see the cost of their Kubernetes nodes and load balancers aggregated by cluster name within a Kubernetes clusters group on their invoice. Volumes and volume snapshots used in a DOKS cluster are not yet included in the cluster aggregation.

DOKS customers will now be able to see the cost of their Kubernetes nodes and load balancers aggregated by cluster name within a Kubernetes clusters group on their invoice. Volumes and volume snapshots used in a DOKS cluster are not yet included in the cluster aggregation.
DigitalOcean Load Balancers now support PROXY protocol version 1.

DigitalOcean Load Balancers now support PROXY protocol version 1.
DigitalOcean Load Balancers now support using PROXY Protocol to pass information like origin IP addresses and port numbers from connecting client requests along to the backend service. This can be configured using the API by setting the new enable_proxy_protocol attribute to true when creating a new Load Balancer or updating an existing one.
See here for more information about using PROXY Protocol with DigitalOcean Load Balancers.

DigitalOcean Load Balancers now support using PROXY Protocol to pass information like origin IP addresses and port numbers from connecting client requests along to the backend service. This can be configured using the API by setting the new enable_proxy_protocol attribute to true when creating a new Load Balancer or updating an existing one.
See here for more information about using PROXY Protocol with DigitalOcean Load Balancers.
The Akaunting and Caprover third-party One-Click applications were released.

The Akaunting and Caprover third-party One-Click applications were released.
The DigitalOcean Marketplace is now in General Availability.

The DigitalOcean Marketplace is now in General Availability.
The Microweber third-party One-Click application was released.

The Microweber third-party One-Click application was released.
The following third-party One-Click applications were released: CloudBees, Jenkins, cPanel, Passbolt, Directus, and Nimbella.

The following third-party One-Click applications were released: CloudBees, Jenkins, cPanel, Passbolt, Directus, and Nimbella.
The Bitwarden and Redash third-party One-Click applications were released.

The Bitwarden and Redash third-party One-Click applications were released.
Public beta was opened for the new metrics agent. See how to update your metrics agent here.

Public beta was opened for the new metrics agent. See how to update your metrics agent here.
The third-party InfluxDB One-Click application was released.

The third-party InfluxDB One-Click application was released.
The GitLab One-Click application maintained by DigitalOcean was replaced in the control panel by a GitLab Enterprise Edition maintained by GitLab. The corresponding API slug, gitlab-18-04, is deprecated and will be removed in 90 days. The new slug, gitlab-ee-18-04 is available now.

The GitLab One-Click application maintained by DigitalOcean was replaced in the control panel by a GitLab Enterprise Edition maintained by GitLab. The corresponding API slug, gitlab-18-04, is deprecated and will be removed in 90 days. The new slug, gitlab-ee-18-04 is available now.
General Purpose Performance Droplet plans were released.

General Purpose Performance Droplet plans were released.
As announced on September 5, 2018, the last_tagged attribute returned in response to GET requests to the /v2/tags or /v2/tags/$TAG_NAME endpoints has been deprecated. Beginning March 1st, 2019, last_tagged will no longer be populated in favor of the last_tagged_uri attribute.
For example, a GET request to /v2/tags/frontend currently might return:
{
  "tag": {
    "name": "frontend",
    "resources": {
      "count": 3,
      "last_tagged_uri": "https://api.digitalocean.com/v2/droplets/132000916",
      "droplets": {
        "count": 3,
        "last_tagged": {
          "id": 132000916,
          "name": "suspicious-bhabha-u8zq",
          "memory": 2048,
          "vcpus": 2,
          "disk": 60,
          "locked": false,
          "status": "active",
          "kernel": null,
          "created_at": "2019-02-13T05:29:52Z",
          "features": [
            "private_networking"
          ],
          "backup_ids": [],
          "next_backup_window": null,
          "snapshot_ids": [],
          "image": {
            "id": 43509743,
            "name": "do-kube-1.12.3",
            "distribution": "Debian",
            "slug": null,
            "public": false,
            "regions": [
              "ams2",
              "ams3",
              "blr1",
              "fra1",
              "lon1",
              "nyc1",
              "nyc2",
              "nyc3",
              "sfo1",
              "sfo2",
              "sgp1",
              "tor1"
            ],
            "created_at": "2019-02-11T20:38:04Z",
            "min_disk_size": 20,
            "type": "snapshot",
            "size_gigabytes": 2.99
          },
          "volume_ids": [],
          "size": {
            "slug": "s-2vcpu-2gb",
            "memory": 2048,
            "vcpus": 2,
            "disk": 60,
            "transfer": 3,
            "price_monthly": 15,
            "price_hourly": 0.02232,
            "regions": [
              "ams2",
              "ams3",
              "blr1",
              "fra1",
              "lon1",
              "nyc1",
              "nyc2",
              "nyc3",
              "sfo1",
              "sfo2",
              "sgp1",
              "tor1"
            ],
            "available": true
          },
          "size_slug": "s-2vcpu-2gb",
          "networks": {
            "v4": [
              {
                "ip_address": "192.0.2.255",
                "netmask": "255.255.240.0",
                "gateway": "192.0.2.1",
                "type": "public"
              },
              {
                "ip_address": "10.136.121.81",
                "netmask": "255.255.0.0",
                "gateway": "10.136.0.1",
                "type": "private"
              }
            ],
            "v6": []
          },
          "region": {
            "name": "New York 1",
            "slug": "nyc1",
            "sizes": [
              "s-1vcpu-3gb",
              "s-1vcpu-1gb",
              "s-3vcpu-1gb",
              "s-1vcpu-2gb",
              "s-2vcpu-2gb",
              "s-2vcpu-4gb",
              "s-4vcpu-8gb",
              "s-16vcpu-64gb",
              "s-6vcpu-16gb",
              "s-8vcpu-32gb",
              "s-12vcpu-48gb",
              "s-20vcpu-96gb",
              "s-24vcpu-128gb",
              "s-32vcpu-192gb"
            ],
            "features": [
              "private_networking",
              "backups",
              "ipv6",
              "metadata",
              "install_agent",
              "server_id",
              "management_networking"
            ],
            "available": true
          },
          "tags": [
            "frontend"
          ]
        },
        "last_tagged_uri": "https://api.digitalocean.com/v2/droplets/132000916"
      },
      "images": {
        "count": 1,
        "last_tagged_uri": "https://api.digitalocean.com/v2/images/42991114"
      },
      "volumes": {
        "count": 0
      }
    }
  }
}
Following this change, the new response would look like:
{
  "tag": {
    "name": "frontend",
    "resources": {
      "count": 3,
      "last_tagged_uri": "https://api.digitalocean.com/v2/droplets/132000916",
      "droplets": {
        "count": 3,
        "last_tagged_uri": "https://api.digitalocean.com/v2/droplets/132000916"
      },
      "images": {
        "count": 1,
        "last_tagged_uri": "https://api.digitalocean.com/v2/images/42991114"
      },
      "volumes": {
        "count": 0
      }
    }
  }
}
For additional information, see the full API reference documentation for tags.

As announced on September 5, 2018, the last_tagged attribute returned in response to GET requests to the /v2/tags or /v2/tags/$TAG_NAME endpoints has been deprecated. Beginning March 1st, 2019, last_tagged will no longer be populated in favor of the last_tagged_uri attribute.
For example, a GET request to /v2/tags/frontend currently might return:
Following this change, the new response would look like:
For additional information, see the full API reference documentation for tags.
DigitalOcean Managed Databases were released with support for PostgreSQL v10 and v11.

DigitalOcean Managed Databases were released with support for PostgreSQL v10 and v11.
Today DigitalOcean‚Äôs Managed Database service, including its API, has entered Limited Availability. In order to access these new endpoints, you must first enable Managed Databases on your account by opting-in via the cloud control panel. Once enabled, you will be able to create, scale, and manage your database clusters via the API. For example, to create a new database cluster, make a POST to the /v2/databases endpoint with a JSON body like:
{
  "name": "backend",
  "engine": "pg",
  "version": "10",
  "region": "nyc3",
  "size": "db-s-1vcpu-2gb",
  "num_nodes": 2
}
The response will include a full JSON representation of the database cluster. The initial value of the cluster‚Äôs status attribute will be ‚Äúcreating.‚Äù When the cluster is ready for use, this will transition to ‚Äúonline.‚Äù
For the all the details, see the full API reference documentation for DigitalOcean Managed Databases.

Today DigitalOcean‚Äôs Managed Database service, including its API, has entered Limited Availability. In order to access these new endpoints, you must first enable Managed Databases on your account by opting-in via the cloud control panel. Once enabled, you will be able to create, scale, and manage your database clusters via the API. For example, to create a new database cluster, make a POST to the /v2/databases endpoint with a JSON body like:
The response will include a full JSON representation of the database cluster. The initial value of the cluster‚Äôs status attribute will be ‚Äúcreating.‚Äù When the cluster is ready for use, this will transition to ‚Äúonline.‚Äù
For the all the details, see the full API reference documentation for DigitalOcean Managed Databases.
The third-party Helpy One-Click application was released.

The third-party Helpy One-Click application was released.
The third-party Cloudron One-Click application was released.

The third-party Cloudron One-Click application was released.
Users with credits will automatically receive an email notification when account usage exceeds their promo code credit and any prepay balance.

Users with credits will automatically receive an email notification when account usage exceeds their promo code credit and any prepay balance.
The /v2/volumes endpoint now displays tags and supports adding them to volumes at creation time. Volume tags may now be managed with the /v2/tags endpoint as well. For more information, see the API reference documentation for both volumes and tags.

The /v2/volumes endpoint now displays tags and supports adding them to volumes at creation time. Volume tags may now be managed with the /v2/tags endpoint as well. For more information, see the API reference documentation for both volumes and tags.
Added the Droplet name to the subject line in metrics alert email notifications.

Added the Droplet name to the subject line in metrics alert email notifications.
The third-party Grafana One-Click application was released.

The third-party Grafana One-Click application was released.
The third-party NKN Full Node One-Click application was released.

The third-party NKN Full Node One-Click application was released.
The third-party Fathom Analytics One-Click application was released.

The third-party Fathom Analytics One-Click application was released.
The third-party OpenFaaS One-Click application was released.

The third-party OpenFaaS One-Click application was released.
To ensure the accuracy of reported metrics, the top processes graphs were removed from Monitoring. Instead, you can monitor resource-consuming processes with tools like top.

To ensure the accuracy of reported metrics, the top processes graphs were removed from Monitoring. Instead, you can monitor resource-consuming processes with tools like top.
The deprecated 16.04 One-Click LAMP slug, lamp-16-04, was removed from the API.

The deprecated 16.04 One-Click LAMP slug, lamp-16-04, was removed from the API.
The third-party OpenLiteSpeed Django One-Click application was released.

The third-party OpenLiteSpeed Django One-Click application was released.
The third-party OpenLiteSpeed NodeJS One-Click application was released.

The third-party OpenLiteSpeed NodeJS One-Click application was released.
Monthly billing emails now include a PDF invoice attachment.

Monthly billing emails now include a PDF invoice attachment.
Value Added Tax (VAT) collection for Russia has begun. Charges will appear on the February 1 invoice.

Value Added Tax (VAT) collection for Russia has begun. Charges will appear on the February 1 invoice.
The third-party OpenLiteSpeed CyberPanel and Countly Analytics One-Click applications were released.

The third-party OpenLiteSpeed CyberPanel and Countly Analytics One-Click applications were released.
Released v1.1.0 of the DigitalOcean Terraform Provider. Learn more on the Terraform Changelog.

Released v1.1.0 of the DigitalOcean Terraform Provider. Learn more on the Terraform Changelog.
The third-party Open Source Social Network One-Click application was released.

The third-party Open Source Social Network One-Click application was released.
The following updates were released for DigitalOcean Kubernetes:

Any user can opt into Kubernetes during early availability via the control panel.
Users can rename clusters.
Users can edit tags associated with clusters and worker pool.
A guided walkthrough helps users set up their kubectl and kubeconfig properly and provides example manifests to make it easier to get started.
The node pool settings to add, remove, and scale node pools has been moved to the ‚ÄòNodes‚Äô tab for a more streamlined experience.
The cluster provisioning status bar more accurately reflects the cluster creation progress.
Users can view the estimated monthly cost for their clusters.
Users can view a breakdown of total cluster capacity including CPU, Memory, Disk cluster wide.
Users can create and manage Kubernetes clusters, worker pools, and configuration using doctl.


The following updates were released for DigitalOcean Kubernetes:
Today we opened up access to the DigitalOcean Kubernetes service for all users. As part of this release, the API is now also available to all. While still in Limited Availability, you must first enable Kubernetes on your account by opting-in via the cloud control panel to access these endpoints.
Once enabled, you will be able to list, create, or delete clusters as well as scale node pools up and down, recycle individual nodes, and retrieve the kubeconfig file for use with a cluster via the API. For example, to create a new cluster with a node pool using three s-2vcpu-2gb Droplets, make a POST to the /v2/kubernetes/clusters endpoint with a JSON body like:
{
  "name": "prod-cluster-01",
  "region": "nyc1",
  "version": "1.12.1-do.2",
  "tags": ["production"],
  "node_pools": [
    {
      "size": "s-2vcpu-2gb",
      "count": 3,
      "name": "woker-pool"
    }
  ]
}
The response will include a full JSON representation of the cluster. The initial value of the cluster‚Äôs status.state attribute will be ‚Äúprovisioning.‚Äù When the cluster is ready for use, this will transition to ‚Äúrunning.‚Äù You can use the /v2/kubernetes/options endpoint to find the available versions of Kubernetes as well as the supported regions and Droplet sizes.
Once ready, you can retrieve the credentials for use with the cluster by sending a GET request to /v2/kubernetes/clusters/$K8S_CLUSTER_ID/kubeconfig. The response will be a kubeconfig file in yaml format. This file can be used to connect to and administer the cluster using the Kubernetes command line tool, kubectl. For more information, see ‚ÄúHow to Connect to a DigitalOcean Kubernetes Cluster with kubectl.‚Äù
For the all the details, see the full API reference documentation for DigitalOcean Kubernetes.

Today we opened up access to the DigitalOcean Kubernetes service for all users. As part of this release, the API is now also available to all. While still in Limited Availability, you must first enable Kubernetes on your account by opting-in via the cloud control panel to access these endpoints.
Once enabled, you will be able to list, create, or delete clusters as well as scale node pools up and down, recycle individual nodes, and retrieve the kubeconfig file for use with a cluster via the API. For example, to create a new cluster with a node pool using three s-2vcpu-2gb Droplets, make a POST to the /v2/kubernetes/clusters endpoint with a JSON body like:
The response will include a full JSON representation of the cluster. The initial value of the cluster‚Äôs status.state attribute will be ‚Äúprovisioning.‚Äù When the cluster is ready for use, this will transition to ‚Äúrunning.‚Äù You can use the /v2/kubernetes/options endpoint to find the available versions of Kubernetes as well as the supported regions and Droplet sizes.
Once ready, you can retrieve the credentials for use with the cluster by sending a GET request to /v2/kubernetes/clusters/$K8S_CLUSTER_ID/kubeconfig. The response will be a kubeconfig file in yaml format. This file can be used to connect to and administer the cluster using the Kubernetes command line tool, kubectl. For more information, see ‚ÄúHow to Connect to a DigitalOcean Kubernetes Cluster with kubectl.‚Äù
For the all the details, see the full API reference documentation for DigitalOcean Kubernetes.
The minimum size for a Kubernetes node was changed to the 2 GB Memory / 1 vCPU plan.

The minimum size for a Kubernetes node was changed to the 2 GB Memory / 1 vCPU plan.
The first version of monthly billing emails with attached PDF invoices was released to a small group of beta customers.

The first version of monthly billing emails with attached PDF invoices was released to a small group of beta customers.
The Projects API was released to general availability.

The Projects API was released to general availability.
Today, we are promoting the Projects API to General Availability. For the full details, see the API reference documentation for both Projects and Project Resources.
Thank you to everyone who took the time to provide us with feedback.

Today, we are promoting the Projects API to General Availability. For the full details, see the API reference documentation for both Projects and Project Resources.
Thank you to everyone who took the time to provide us with feedback.
The third-party OpenLiteSpeed WordPress One-Click application is now available in the control panel.

The third-party OpenLiteSpeed WordPress One-Click application is now available in the control panel.
Ubuntu 16.04 One-Click application images were removed from the API.

Ubuntu 16.04 One-Click application images were removed from the API.
Value Added Tax (VAT) collection for Switzerland and Turkey has begun. Charges will appear on the December invoice.

Value Added Tax (VAT) collection for Switzerland and Turkey has begun. Charges will appear on the December invoice.
The third-party Hasura One-Click application is now available in the control panel.

The third-party Hasura One-Click application is now available in the control panel.
The third-party Plesk One-Click application is now available in the control panel.

The third-party Plesk One-Click application is now available in the control panel.
Droplets created from custom images now support snapshots and backups.

Droplets created from custom images now support snapshots and backups.
The third-party Sourcegraph One-Click application is now available in the control panel.

The third-party Sourcegraph One-Click application is now available in the control panel.
Ubuntu 18.10 base images are now available in the control panel and via the API using the slug ubuntu-18-10-x64.

Ubuntu 18.10 base images are now available in the control panel and via the API using the slug ubuntu-18-10-x64.
Today, we are launching a beta of our new Projects API. Projects enable you to group your resources in ways that align with the applications you host on DigitalOcean, and now you can do so via our API as well. This initial release includes the ability to:

Create, list, retrieve, update, and delete Projects
Assign existing resources to a Project
List resources in a Project

Additionally, we‚Äôve added beta support for Projects to our official clients (Droplet Kit, godo, and doctl).
You can create a new project by sending a POST request to the /v2/projects endpoint including a body like:
{
  "name": "my-web-api",
  "description": "My website API",
  "purpose": "Service or API",
  "environment": "Production"
}

To assign resources to a project, send a POST request to /v2/projects/$PROJECT_ID/resources including a list of those resources in the body:
{
  "resources": [
    "do:droplet:123456",
    "do:floatingip:192.168.99.100",
    "do:space:static-assets",
    "do:volume:0e250b2a-8a01-11e8-96ae-0242ad114410"
   ]
}

Resources are identified by uniform resource names or URNs, a string consisting of the type of resource and its unique identifier. A valid URN has the following format: do:resource_type:resource_id. For the full details, see the API reference documentation for both Projects and Project Resources.
Note that as this is a beta release, we may make additional changes based on your feedback. So let us know how you‚Äôre using projects, and follow along with the API changelog for updates.

Today, we are launching a beta of our new Projects API. Projects enable you to group your resources in ways that align with the applications you host on DigitalOcean, and now you can do so via our API as well. This initial release includes the ability to:
Additionally, we‚Äôve added beta support for Projects to our official clients (Droplet Kit, godo, and doctl).
You can create a new project by sending a POST request to the /v2/projects endpoint including a body like:
To assign resources to a project, send a POST request to /v2/projects/$PROJECT_ID/resources including a list of those resources in the body:
Resources are identified by uniform resource names or URNs, a string consisting of the type of resource and its unique identifier. A valid URN has the following format: do:resource_type:resource_id. For the full details, see the API reference documentation for both Projects and Project Resources.
Note that as this is a beta release, we may make additional changes based on your feedback. So let us know how you‚Äôre using projects, and follow along with the API changelog for updates.
The DigitalOcean feature request portal has been migrated to https://ideas.digitalocean.com.

The DigitalOcean feature request portal has been migrated to https://ideas.digitalocean.com.
Released v1.0.2 of the DigitalOcean Terraform Provider. Learn more on the Terraform Changelog.

Released v1.0.2 of the DigitalOcean Terraform Provider. Learn more on the Terraform Changelog.
Released v1.0.1 of the DigitalOcean Terraform Provider. Learn more on the Terraform Changelog.

Released v1.0.1 of the DigitalOcean Terraform Provider. Learn more on the Terraform Changelog.
Pricing for load balancers has decreased from $20/month to $10/month.

Pricing for load balancers has decreased from $20/month to $10/month.
DigitalOcean Kubernetes is now in early availability. Learn more.

DigitalOcean Kubernetes is now in early availability. Learn more.
Released v1.0.0 of the DigitalOcean Terraform Provider, including new attachment resources for volumes and floating IPs, support for Let‚Äôs Encrypt certificates, auto-formatting for volumes, and CAA domain records, and more. Learn more: https://do.co/terraform-changelog

Released v1.0.0 of the DigitalOcean Terraform Provider, including new attachment resources for volumes and floating IPs, support for Let‚Äôs Encrypt certificates, auto-formatting for volumes, and CAA domain records, and more. Learn more: https://do.co/terraform-changelog
Released the Spaces content delivery network (CDN).

Released the Spaces content delivery network (CDN).
Deprecated the Spaces free trial.

Deprecated the Spaces free trial.
Today‚Äôs release brings Content Delivery Network (CDN) support to Spaces, DigitalOcean‚Äôs object storage solution. This can be configured and managed using our API. By sending requests to /v2/cdn/endpoints, you can list, create, or delete CDN endpoints as well as purge cached content.
To enable the CDN for your Space, send a POST request to /v2/cdn/endpoints. In the JSON body of your request, specify the origin of your content and the desired TTL. For example:
{
  "origin": "static-images.nyc3.digitaloceanspaces.com",
  "ttl": 3600
}

Currently, the origin must be a DigitalOcean Space.
To purge cached content from a CDN endpoint, send a DELETE request to /v2/cdn/endpoints/$ENDPOINT_ID/cache. The body of the request should include a files attribute containing a list of cached file paths to be purged. A path may be for a single file or may contain a wildcard (*) to recursively purge all files under a directory. When only a wildcard is provided, all cached files will be purged. For example, the body of your request might look like:
{
  "files": [
    "assets/img/hero.png",
    "assets/css/*"
  ]
}

For additional details, see the API reference documentation for managing CDN endpoints.

Today‚Äôs release brings Content Delivery Network (CDN) support to Spaces, DigitalOcean‚Äôs object storage solution. This can be configured and managed using our API. By sending requests to /v2/cdn/endpoints, you can list, create, or delete CDN endpoints as well as purge cached content.
To enable the CDN for your Space, send a POST request to /v2/cdn/endpoints. In the JSON body of your request, specify the origin of your content and the desired TTL. For example:
Currently, the origin must be a DigitalOcean Space.
To purge cached content from a CDN endpoint, send a DELETE request to /v2/cdn/endpoints/$ENDPOINT_ID/cache. The body of the request should include a files attribute containing a list of cached file paths to be purged. A path may be for a single file or may contain a wildcard (*) to recursively purge all files under a directory. When only a wildcard is provided, all cached files will be purged. For example, the body of your request might look like:
For additional details, see the API reference documentation for managing CDN endpoints.
Released custom image support which allows customers to upload their Linux and Unix-like images to their DigitalOcean account and use them to create Droplets.

Released custom image support which allows customers to upload their Linux and Unix-like images to their DigitalOcean account and use them to create Droplets.
Today DigitalOcean released support for uploading custom images, enabling you to create Droplets based on your own Linux virtual machine images. Our image management API has been extended with support as well. By sending a POST to the /v2/images endpoint, you can create a new custom image. The request must contain a url attribute pointing to where the image can be downloaded. The image itself may be in the raw, qcow2, vhdx, vdi, or vmdk format. It can be compressed using gzip or bzip2 but must be smaller that 100 GB after being decompressed. For example, the body of you request might look like:
{
  "name": "ubuntu-18.04-minimal",
  "url": "http://cloud-images.ubuntu.com/minimal/releases/bionic/release/ubuntu-18.04-minimal-cloudimg-amd64.img",
  "distribution": "Ubuntu",
  "region": "nyc3",
  "description": "Cloud-optimized image w/ small footprint",
  "tags": [
    "base-image",
    "prod"
  ]
}

To make organizing your images easier, we‚Äôve also extended tagging support to custom images as well as Droplet snapshots. For additional details, see the API reference documentation for creating custom images and tagging resources.

Today DigitalOcean released support for uploading custom images, enabling you to create Droplets based on your own Linux virtual machine images. Our image management API has been extended with support as well. By sending a POST to the /v2/images endpoint, you can create a new custom image. The request must contain a url attribute pointing to where the image can be downloaded. The image itself may be in the raw, qcow2, vhdx, vdi, or vmdk format. It can be compressed using gzip or bzip2 but must be smaller that 100 GB after being decompressed. For example, the body of you request might look like:
To make organizing your images easier, we‚Äôve also extended tagging support to custom images as well as Droplet snapshots. For additional details, see the API reference documentation for creating custom images and tagging resources.
Removed deprecated Machine Learning/Artificial Intelligence and MEAN One-Clicks from the control panel.

Removed deprecated Machine Learning/Artificial Intelligence and MEAN One-Clicks from the control panel.
New One-Click Application Droplets that you create with the control panel will be based on Ubuntu 18.04 LTS. For the next 60 days (through November 12, 2018 11:59pm EST), Ubuntu 16.04 LTS-based One-Clicks will continue to be available alongside the 18.04 version through the API.

New One-Click Application Droplets that you create with the control panel will be based on Ubuntu 18.04 LTS. For the next 60 days (through November 12, 2018 11:59pm EST), Ubuntu 16.04 LTS-based One-Clicks will continue to be available alongside the 18.04 version through the API.
Removed the list of environments (Production, Staging, Development) from the list of project purposes when creating projects. Added them as a separate dropdown on the Project Settings page.

Removed the list of environments (Production, Staging, Development) from the list of project purposes when creating projects. Added them as a separate dropdown on the Project Settings page.
Fixed a bug where a project containing only domains displayed an empty state that required users to scroll to access their domains.

Fixed a bug where a project containing only domains displayed an empty state that required users to scroll to access their domains.
Fixed a bug where domains with capital letters were not displaying the project on the Domain page.

Fixed a bug where domains with capital letters were not displaying the project on the Domain page.
Began incremental release of invoice aggregation for users with more than 3000 invoice line items.

Began incremental release of invoice aggregation for users with more than 3000 invoice line items.
Spaces are now available in the SFO2 region.

Spaces are now available in the SFO2 region.
Ubuntu 18.04 is now the default image when creating new Droplets from the control panel.

Ubuntu 18.04 is now the default image when creating new Droplets from the control panel.
When listing or getting tags by sending a GET request to /v2/tags or /v2/tags/$TAG_NAME, the response payload currently includes a last_tagged value inside the tag‚Äôs resources.droplets containing a full representation of the resource. This payload is considerably nested and adds additional overhead to the request. In order to improve performance as well as lay the groundwork for bring tagging support to additional resources, this attribute is being deprecated. Beginning March 1st, 2019 last_tagged will no longer be populated in favor of the new last_tagged_uri attribute introduced today.
For all resources (and each resource type supported), the last_tagged_uri attribute contains a string indicating the URI which can be used to retrieve details about that specific resource. If you need information about the last tagged resource specifically, issuing another call to that URI will provide you with all the data for that resource.
Additionally, a count attribute describing how many resources overall have been tagged with the tag in question has been added. Each individual resource type will continue providing a count attribute.
If you need guidance on transitioning from using last_tagged to using of the new last_tagged_uri attribute, reach out to the team by opening a support ticket.

When listing or getting tags by sending a GET request to /v2/tags or /v2/tags/$TAG_NAME, the response payload currently includes a last_tagged value inside the tag‚Äôs resources.droplets containing a full representation of the resource. This payload is considerably nested and adds additional overhead to the request. In order to improve performance as well as lay the groundwork for bring tagging support to additional resources, this attribute is being deprecated. Beginning March 1st, 2019 last_tagged will no longer be populated in favor of the new last_tagged_uri attribute introduced today.
For all resources (and each resource type supported), the last_tagged_uri attribute contains a string indicating the URI which can be used to retrieve details about that specific resource. If you need information about the last tagged resource specifically, issuing another call to that URI will provide you with all the data for that resource.
Additionally, a count attribute describing how many resources overall have been tagged with the tag in question has been added. Each individual resource type will continue providing a count attribute.
If you need guidance on transitioning from using last_tagged to using of the new last_tagged_uri attribute, reach out to the team by opening a support ticket.
Customers with multiple credit cards on file can now choose which one is billed by default on the billing page.

Customers with multiple credit cards on file can now choose which one is billed by default on the billing page.
Discontinued the CPU-optimized Droplet 2 GB/1vCPU plan.

Discontinued the CPU-optimized Droplet 2 GB/1vCPU plan.
The Recovery Console now supports pasting information into the console.

The Recovery Console now supports pasting information into the console.
The GitLab One-Click application has been updated with the following changes:



Updated
From
To




GitLab Community Edition
11.0.0
11.1.4




The GitLab One-Click application has been updated with the following changes:
Updated all product documentation to reflect the release of DigitalOcean Projects, control panel side navigation, and the restructuring of the Accounts section.

Updated all product documentation to reflect the release of DigitalOcean Projects, control panel side navigation, and the restructuring of the Accounts section.
The 192 GB Standard Droplet plan has been enabled in AMS3, BLR1, FRA1, LON1, NYC3, NYC1, SGP1, SFO2, and TOR1.

The 192 GB Standard Droplet plan has been enabled in AMS3, BLR1, FRA1, LON1, NYC3, NYC1, SGP1, SFO2, and TOR1.
The default Ubuntu x64 base image has been updated from 16.04.4 to 18.04.1. For details about 18.04.1, see the Ubuntu release notes.

The default Ubuntu x64 base image has been updated from 16.04.4 to 18.04.1. For details about 18.04.1, see the Ubuntu release notes.
Released the following control panel updates:

Users can organize their resources into projects to fit the way they work. Projects allow users to group their Droplets, Spaces, Load Balancers, domains, and Floating IPs to align with the applications, environments, clients, and projects that they host on DigitalOcean
The main navigation of the control panel moved from top navigation to left navigation with updated styles.

The changes are scheduled to reach all users by July 28.

Released the following control panel updates:
The changes are scheduled to reach all users by July 28.
You can now edit the card holder name, expiration date, CVC code, and billing address for existing credit card on the Account Billing page.

You can now edit the card holder name, expiration date, CVC code, and billing address for existing credit card on the Account Billing page.
The WordPress One-Click application has been updated:



Updated
From
To




Wordpress
4.9.1
4.9.7


MySQL
5.7.2
5.7.22




The WordPress One-Click application has been updated:
The Ghost One-Click application has been updated:



Updated
From
To




Ghost
1.21.1
1.24.9


Ghost-CLI
1.5.2
1.8.1




The Ghost One-Click application has been updated:
Ubuntu 17.10 reached end of life today and was removed from the control panel and API.

Ubuntu 17.10 reached end of life today and was removed from the control panel and API.
With the release of private networking isolation in NYC3, private networks are restricted to each user account in all regions.

With the release of private networking isolation in NYC3, private networks are restricted to each user account in all regions.
Private networking isolation was released in the NYC2 datacenter.

Private networking isolation was released in the NYC2 datacenter.
Private networking isolation was released in the NYC1 datacenter.

Private networking isolation was released in the NYC1 datacenter.
The Debian 9 base image was updated from 9.4 to 9.5. The image is available in the control panel and via the API using the slug debian-9-x64.

The Debian 9 base image was updated from 9.4 to 9.5. The image is available in the control panel and via the API using the slug debian-9-x64.
Private networking isolation was released in the SFO1 and SGP1 datacenters.

Private networking isolation was released in the SFO1 and SGP1 datacenters.
Private networking isolation continued with releases in the LON1, AMS3, FRA1, and SFO2 datacenters.

Private networking isolation continued with releases in the LON1, AMS3, FRA1, and SFO2 datacenters.
Local disk size for the 1vCPU-Optimized Droplet plan (c-1vcpu-2gb) increased from 20 GB to 25 GB.

Local disk size for the 1vCPU-Optimized Droplet plan (c-1vcpu-2gb) increased from 20 GB to 25 GB.
Private networking isolation was released in the AMS2, BLR1, and TOR1 datacenters. Communication over the private network in those datacenters is now restricted to other resources within an account or team.

Private networking isolation was released in the AMS2, BLR1, and TOR1 datacenters. Communication over the private network in those datacenters is now restricted to other resources within an account or team.
FreeBSD 11.2 is now available through the control panel and through the API using the slug freebsd-11-2-x64-zfs.

FreeBSD 11.2 is now available through the control panel and through the API using the slug freebsd-11-2-x64-zfs.
The changes to Droplet bandwidth billing announced on April 24 were put into effect.

The changes to Droplet bandwidth billing announced on April 24 were put into effect.
Released new documentation site for the DigitalOcean Control Panel with updated content and product-specific navigation and search to help readers more readily learn how to use DigitalOcean.

Released new documentation site for the DigitalOcean Control Panel with updated content and product-specific navigation and search to help readers more readily learn how to use DigitalOcean.
Spaces will send billing data for active users within 2 hours of usage, down from a 4-5 day processing time. Previously, some users who were not billed for overages because of the processing delay may see their bill go up based on their actual usage.

Spaces will send billing data for active users within 2 hours of usage, down from a 4-5 day processing time. Previously, some users who were not billed for overages because of the processing delay may see their bill go up based on their actual usage.
GitLab One-Click application has been updated with the following changes:



Updated
From
To




Kernel
4.4.0-119-generic
4.4.0-128-generic


GitLab Community Edition
10.6.4 dee2c87
11.0.0 b84bfb5




GitLab One-Click application has been updated with the following changes:
New Domain resources can now be created via the DigitalOcean v2 API without providing an IP address. The previous behavior, which would automatically create an A record pointing to the apex domain, will be retained for backwards-compatibility when an IP address is provided.
This example demonstrates how to create a new domain without providing an IP address:
curl -X POST -H "Content-Type: application/json" \
    -H "Authorization: Bearer $DIGITALOCEAN_API_TOKEN" \
    -d '{"name":"example.com"}' \
    "https://api.digitalocean.com/v2/domains"
For more information, see the full Domains API documentation.

New Domain resources can now be created via the DigitalOcean v2 API without providing an IP address. The previous behavior, which would automatically create an A record pointing to the apex domain, will be retained for backwards-compatibility when an IP address is provided.
This example demonstrates how to create a new domain without providing an IP address:
For more information, see the full Domains API documentation.
Ruby on Rails One-Click application has been updated with the following changes:



Updated
From
To




Kernel
4.4.0-72-generic
4.4.0-128-generic


Ruby
2.4.0
2.4.1


Rails
5.0.2
5.2.0


Nginx
1.10.0
1.10.3




Ruby on Rails One-Click application has been updated with the following changes:
Debian 7 reached end of life and has been removed from the control panel.

Debian 7 reached end of life and has been removed from the control panel.
Updated RancherOS container image from 12.0 to 14.0. Customers can learn more about the new version on Rancher‚Äôs release page.

Updated RancherOS container image from 12.0 to 14.0. Customers can learn more about the new version on Rancher‚Äôs release page.
Expanded Droplet View allows customers using the Dashboard to click on a Droplet and quickly view additional information about the Droplet without having to go to the Droplet Page. It also updates the list of Droplets to display at a glance whether Backups are on/off and if a Floating IP is attached.

Expanded Droplet View allows customers using the Dashboard to click on a Droplet and quickly view additional information about the Droplet without having to go to the Droplet Page. It also updates the list of Droplets to display at a glance whether Backups are on/off and if a Floating IP is attached.
Released new Droplet feature to allow customers to boot Droplets from a Recovery ISO.  Learn more in How To Recover from File System Corruption Using Fsck and a Recovery ISO.

Released new Droplet feature to allow customers to boot Droplets from a Recovery ISO.  Learn more in How To Recover from File System Corruption Using Fsck and a Recovery ISO.
Volumes for Ubuntu, Fedora, Debian 8+, CentOS, and Fedora Atomic can be automatically formatted and mounted when they are created.

Volumes for Ubuntu, Fedora, Debian 8+, CentOS, and Fedora Atomic can be automatically formatted and mounted when they are created.
The /v2/volumes endpoint has been updated to support automatically formatting the filesystem of newly created volumes. Volume resources now expose two new attributes: filesystem_type and filesystem_label. They can be used to specify the filesystem and the label to be applied. Currently, the available filesytem types are ext4 and xfs.
For example, here is a request creating a new volume formatted with an EXT4 filesystem:
  curl -X POST \
  -d '{"name":"volume-nyc3-01","region":"nyc3","filesystem_type":"ext4","filesystem_label":"example","size_gigabytes": 100}' \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $DIGITALOCEAN_TOKEN" \
  https://api.digitalocean.com/v2/volumes
Additionally, Ubuntu, Debian, Fedora, Fedora Atomic, and CentOS Droplets created on or after April 26, 2018 will now automatically mount volumes with pre-formatted filesystems when attached. Attaching pre-formatted volumes to other Droplets is not recommended. When the filesystem_type attribute is not provided, volumes will continue to be presented as raw block devices and require additional configuration.
When retrieving an existing volume, filesystem_type and filesystem_label will reflect the current filesystem and label used on the volume even if these were applied manually.
For more information, see the full API documentation for Volumes.

The /v2/volumes endpoint has been updated to support automatically formatting the filesystem of newly created volumes. Volume resources now expose two new attributes: filesystem_type and filesystem_label. They can be used to specify the filesystem and the label to be applied. Currently, the available filesytem types are ext4 and xfs.
For example, here is a request creating a new volume formatted with an EXT4 filesystem:
Additionally, Ubuntu, Debian, Fedora, Fedora Atomic, and CentOS Droplets created on or after April 26, 2018 will now automatically mount volumes with pre-formatted filesystems when attached. Attaching pre-formatted volumes to other Droplets is not recommended. When the filesystem_type attribute is not provided, volumes will continue to be presented as raw block devices and require additional configuration.
When retrieving an existing volume, filesystem_type and filesystem_label will reflect the current filesystem and label used on the volume even if these were applied manually.
For more information, see the full API documentation for Volumes.
Turned on Droplet Search in the top navigation for all users. Allows users to quickly search for Droplets by name or IP address and go directly to the Droplet Page.

Turned on Droplet Search in the top navigation for all users. Allows users to quickly search for Droplets by name or IP address and go directly to the Droplet Page.
Enabled users to sign up and sign in with their Google accounts. DigitalOcean users can switch their accounts back and forth between password-based and Google-based authentication.

Enabled users to sign up and sign in with their Google accounts. DigitalOcean users can switch their accounts back and forth between password-based and Google-based authentication.
Fixed issue with 2FA QR code generation for authenticator applications. Users with usernames over 26 characters were unable to generate a QR code. We now render up to 65 characters of a user‚Äôs entire email address and truncate if it is longer. This prevents errors for users with long email addresses, and renders more information in authenticator applications to help users with multiple accounts have more context.

Fixed issue with 2FA QR code generation for authenticator applications. Users with usernames over 26 characters were unable to generate a QR code. We now render up to 65 characters of a user‚Äôs entire email address and truncate if it is longer. This prevents errors for users with long email addresses, and renders more information in authenticator applications to help users with multiple accounts have more context.
Load Balancers v1.5 is released to general availability in all regions, including backend upgrades, Let‚Äôs Encrypt Integration, and HTTP/2 Support.

Load Balancers v1.5 is released to general availability in all regions, including backend upgrades, Let‚Äôs Encrypt Integration, and HTTP/2 Support.
Today, DigitalOcean released a number of Load Balancer improvements including support for using SSL/TLS certificates automatically generated by Let‚Äôs Encrypt. Our Certificate management API has been updated to support automatically generating Let‚Äôs Encrypt certificates in addition to uploading custom, user-generated certificates.
A request to generate a new SSL/TLS certificate using Let‚Äôs Encrypt would look like:
    curl -X POST \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $DO_TOKEN" \
    -d '{"name": "le-cert-01", "type": "lets_encrypt", "dns_names": ["www.example.com","example.com"]}' \
    "https://api.digitalocean.com/v2/certificates"```
The new type attribute must be set to lets_encrypt when using Let‚Äôs Encrypt. If omitted, it will default to custom in order to maintain backwards compatibility. For additional details, see the Certificate management API reference documentation.
For more information on how to use Let‚Äôs Encrypt with DigitalOcean Load Balancers, see this tutorial on our community site.

Today, DigitalOcean released a number of Load Balancer improvements including support for using SSL/TLS certificates automatically generated by Let‚Äôs Encrypt. Our Certificate management API has been updated to support automatically generating Let‚Äôs Encrypt certificates in addition to uploading custom, user-generated certificates.
A request to generate a new SSL/TLS certificate using Let‚Äôs Encrypt would look like:
The new type attribute must be set to lets_encrypt when using Let‚Äôs Encrypt. If omitted, it will default to custom in order to maintain backwards compatibility. For additional details, see the Certificate management API reference documentation.
For more information on how to use Let‚Äôs Encrypt with DigitalOcean Load Balancers, see this tutorial on our community site.
Fedora 28 base image has been released using the slug fedora-28-x64 and fedora-28-x64-atomic. The images are now public to all users.

Fedora 28 base image has been released using the slug fedora-28-x64 and fedora-28-x64-atomic. The images are now public to all users.
Burst support rolled out to all nine block storage regions.
Performance Expectations



Droplet Type
IOPS
Throughput




Standard
5K
200 MB/s


Std (Burst)
7.5K
300 MB/s


Optimized
7.5K
300 MB/s


Optimized (Burst)
10K
350 MB/s




Burst support rolled out to all nine block storage regions.
Performance Expectations
Ubuntu 18.04 is now available through the control panel and via our API using the slug ubuntu-18-04-x64

Ubuntu 18.04 is now available through the control panel and via our API using the slug ubuntu-18-04-x64
Changes to Droplet Bandwidth Billing announced. The new billing plan goes into effect June 1. Charges for June, if any, will appear on the July 1 bill.  Customers can view usage and billing information on their billing page.

Changes to Droplet Bandwidth Billing announced. The new billing plan goes into effect June 1. Charges for June, if any, will appear on the July 1 bill.  Customers can view usage and billing information on their billing page.
Debit cards from any country can be used for payment once a temporary pre-authorization charge of $1 is successful.

Debit cards from any country can be used for payment once a temporary pre-authorization charge of $1 is successful.
Spaces users no longer need to cancel their Spaces subscription via the Spaces UI when they want to stop using Spaces. Now, any time a Spaces user destroys their last Space, their pro-rated $5/month billing (if not in the free trial period) stops. Overage charges still apply if they were incurred before deletion of the last Space.

Spaces users no longer need to cancel their Spaces subscription via the Spaces UI when they want to stop using Spaces. Now, any time a Spaces user destroys their last Space, their pro-rated $5/month billing (if not in the free trial period) stops. Overage charges still apply if they were incurred before deletion of the last Space.
1vCPU-Optimized Droplet launched.

1vCPU-Optimized Droplet launched.
Released the MEAN One-Click application on Ubuntu 16.04, configured to install using docker-compose.

Released the MEAN One-Click application on Ubuntu 16.04, configured to install using docker-compose.
Launched the new Dashboard experience to the control panel. The Dashboard replaces the Droplets page as the new default home page of the control panel. It provides at-a-glance visibility into active resources, like Droplets, Spaces, load balancers, floating IPs, and domains, month-to-date current billing usage, shortcuts to team management, and other common tasks without having to navigate to different and often hard-to-find sections of the control panel.

Launched the new Dashboard experience to the control panel. The Dashboard replaces the Droplets page as the new default home page of the control panel. It provides at-a-glance visibility into active resources, like Droplets, Spaces, load balancers, floating IPs, and domains, month-to-date current billing usage, shortcuts to team management, and other common tasks without having to navigate to different and often hard-to-find sections of the control panel.
Improved notifications of when credits will be applied to an account.

Improved notifications of when credits will be applied to an account.
Resolved an issue where some Spaces customers were being rate limited even though they were well below the rate limiting threshold.

Resolved an issue where some Spaces customers were being rate limited even though they were well below the rate limiting threshold.
Added improvements to reduce timeouts on the Spaces API.

Added improvements to reduce timeouts on the Spaces API.
Number of days left in your 60-day Spaces free trial is now shown on trial opt-in page and on details modal.

Number of days left in your 60-day Spaces free trial is now shown on trial opt-in page and on details modal.
The Ghost one-click was updated to Ghost version 1.21.1.

The Ghost one-click was updated to Ghost version 1.21.1.
Block storage cluster in Bangalore and London upgraded to Ceph Luminous, reducing median cluster latency by 50%

Block storage cluster in Bangalore and London upgraded to Ceph Luminous, reducing median cluster latency by 50%
Deletion of a Space via the control panel is disallowed if there are 100,000 or more objects in the Space. Once the Space has less than 100,000 objects within, it can be deleted by the control panel.

Deletion of a Space via the control panel is disallowed if there are 100,000 or more objects in the Space. Once the Space has less than 100,000 objects within, it can be deleted by the control panel.
Spaces now support:

Version 4 of pre-signed URLs, allowing for easier use of 3rd party S3 compatible libraries.
Scheduled deletion of objects via bucket lifecycle methods in the Spaces API.
Previews of image, audio, and video files that are moused over in the control panel file browser.


Spaces now support:
Spaces are now available in the Singapore (SGP1) region.

Spaces are now available in the Singapore (SGP1) region.
Static site hosting and custom domains for Spaces have been released in private beta. Email [email protected] to participate.

Static site hosting and custom domains for Spaces have been released in private beta. Email [email protected] to participate.
DigitalOcean upgrades Memory, SSD and vCPU across all Standard, Flexible and Optimized Droplet plans.

DigitalOcean upgrades Memory, SSD and vCPU across all Standard, Flexible and Optimized Droplet plans.
Today, we announced wide-ranging changes to our Droplet plans, bringing improved resources across the board. These new plans are now available via the API and can be referenced using their respective size slugs.
Size slugs are human-readable strings used to specify the type of Droplet in certain API requests. In the past, size slugs were typically based on the amount of RAM provided with the plan (e.g. 1gb). Moving forward, we are standardizing on a format comprised of the identifier for the Droplet‚Äôs class, the vCPU count, and the amount of RAM in order to provide more flexibility in the plans we are able to offer you. For example, our new $5 per month Standard Droplet comes with 1 vCPU and 1 GB of RAM. So its size slug is. s-1vcpu-1gb.
Applications and scripts with hard-coded size slugs must be updated to take advantage of these new plans. In order to provide a transition period, 1st Generation Droplet plans will continue to be available via the API using the legacy size slugs. We will provide additional notice before their removal.
The table below shows the new 2nd Generation Standard Droplet plans along with their corresponding size slug. For always up-to-date information on available plans and pricing, see our pricing page.



Class
Slug
vCPUs
RAM
Disk
Transfer
Monthly Price




Standard
s-1vcpu-1gb
1
1 GB
25 GB
1 TB
$5


Standard
s-1vcpu-2gb
1
2 GB
50 GB
2 TB
$10


Standard
s-1vcpu-3gb
1
3 GB
60 GB
3 TB
$15


Standard
s-2vcpu-2gb
2
2 GB
60 GB
3 TB
$15


Standard
s-3vcpu-1gb
3
1 GB
60 GB
3 TB
$15


Standard
s-2vcpu-4gb
2
4 GB
80 GB
4 TB
$20


Standard
s-4vcpu-8gb
4
8 GB
160 GB
5 TB
$40


Standard
s-6vcpu-16gb
6
16 GB
320 GB
6 TB
$80


Standard
s-8vcpu-32gb
8
32 GB
640 GB
7 TB
$160


Standard
s-12vcpu-48gb
12
48 GB
960 GB
8 TB
$240


Standard
s-16vcpu-64gb
16
64 GB
1,280 GB
9 TB
$320


Standard
s-20vcpu-96gb
20
96 GB
1,920 GB
10 TB
$480


Standard
s-24vcpu-128gb
24
128 GB
2,560 GB
11 TB
$640


Standard
s-32vcpu-192gb
32
192 GB
3,840 GB
12 TB
$960



Available Droplet plans, the resources they provide, and the size slug used to identify them can be accessed programmatically by querying the /v2/sizes endpoint.

Today, we announced wide-ranging changes to our Droplet plans, bringing improved resources across the board. These new plans are now available via the API and can be referenced using their respective size slugs.
Size slugs are human-readable strings used to specify the type of Droplet in certain API requests. In the past, size slugs were typically based on the amount of RAM provided with the plan (e.g. 1gb). Moving forward, we are standardizing on a format comprised of the identifier for the Droplet‚Äôs class, the vCPU count, and the amount of RAM in order to provide more flexibility in the plans we are able to offer you. For example, our new $5 per month Standard Droplet comes with 1 vCPU and 1 GB of RAM. So its size slug is. s-1vcpu-1gb.
Applications and scripts with hard-coded size slugs must be updated to take advantage of these new plans. In order to provide a transition period, 1st Generation Droplet plans will continue to be available via the API using the legacy size slugs. We will provide additional notice before their removal.
The table below shows the new 2nd Generation Standard Droplet plans along with their corresponding size slug. For always up-to-date information on available plans and pricing, see our pricing page.
Available Droplet plans, the resources they provide, and the size slug used to identify them can be accessed programmatically by querying the /v2/sizes endpoint.
With the release in AMS3, we now have volumes in all regions.

With the release in AMS3, we now have volumes in all regions.
Domain Record resources have been updated to add support for CAA records. As specified in RFC-6844, this record type can be used to specify which certificate authorities (CAs) are permitted to issue certificates for a domain.
For example, in order to restrict TLS/SSL certificate creation for example.com to letsencrypt.org, you would use a request like:
  curl -X POST \
  -d '{"type":"CAA","name":"@","data":"letsencrypt.org.","priority":null,"port":null,"ttl":1800,"flags":0,"tag":"issue"}' \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $DIGITALOCEAN_TOKEN" \
  https://api.digitalocean.com/v2/domains/example.com/records
For more information on how to use CAA records, see this tutorial on our community site.

Domain Record resources have been updated to add support for CAA records. As specified in RFC-6844, this record type can be used to specify which certificate authorities (CAs) are permitted to issue certificates for a domain.
For example, in order to restrict TLS/SSL certificate creation for example.com to letsencrypt.org, you would use a request like:
For more information on how to use CAA records, see this tutorial on our community site.
Our API has been extended to support configuring the TTL value for individual domain records. This can be done when creating a new record as well as when updating an existing one via a PUT request. See the domain record documentation for further information.

Our API has been extended to support configuring the TTL value for individual domain records. This can be done when creating a new record as well as when updating an existing one via a PUT request. See the domain record documentation for further information.
Our API currently offers the ability to ‚Äúrename‚Äù a tag by sending a PUT request to /v2/tags/$TAG_NAME. Due to low usage and operational complexities involved with its maintenance, we are deprecating this functionality. Beginning April 26th, 2017 all requests to this endpoint will respond with an HTTP status of 410 (Gone).
A tag‚Äôs name also serves as its unique identifier. We‚Äôve found that the ability to change a tag‚Äôs name introduces unneeded complexity. If you need guidance on this transition, reach out to the team by opening a support ticket.

Our API currently offers the ability to ‚Äúrename‚Äù a tag by sending a PUT request to /v2/tags/$TAG_NAME. Due to low usage and operational complexities involved with its maintenance, we are deprecating this functionality. Beginning April 26th, 2017 all requests to this endpoint will respond with an HTTP status of 410 (Gone).
A tag‚Äôs name also serves as its unique identifier. We‚Äôve found that the ability to change a tag‚Äôs name introduces unneeded complexity. If you need guidance on this transition, reach out to the team by opening a support ticket.
You may now pass tags as an attribute when creating one or more new Droplets. This optional parameter will create and apply the specified tag(s) to the newly created Droplet(s). For more information see create Droplet documentation.

You may now pass tags as an attribute when creating one or more new Droplets. This optional parameter will create and apply the specified tag(s) to the newly created Droplet(s). For more information see create Droplet documentation.
API v2 now supports volume snapshots, and exposes a unified snapshot endpoint for volume and Droplet snapshots.

API v2 now supports volume snapshots, and exposes a unified snapshot endpoint for volume and Droplet snapshots.
Size objects now contain a size\_gigabytes attribute which represents the physical size of the image in gigabytes. For more information see the images documentation.

Size objects now contain a size\_gigabytes attribute which represents the physical size of the image in gigabytes. For more information see the images documentation.
API v2 now supports tagging and managing tagged droplets.

API v2 now supports tagging and managing tagged droplets.
API v2 now supports creating multiple droplets simultaneously.

API v2 now supports creating multiple droplets simultaneously.
Account objects now contain a floating\_ip\_limit attribute which provides the maximum number of floating IPs that may be provisioned by the account. For more information, see the account documentation.

Account objects now contain a floating\_ip\_limit attribute which provides the maximum number of floating IPs that may be provisioned by the account. For more information, see the account documentation.
API V2 now supports enabling backups on a Droplet.

API V2 now supports enabling backups on a Droplet.
Account objects now contain status and status_message attributes, describing whether an account is locked, active or has a pending warning. For more information, see the account documentation.

Account objects now contain status and status_message attributes, describing whether an account is locked, active or has a pending warning. For more information, see the account documentation.
We‚Äôve deprecated final (temporary) snapshots and therefore temporary is no longer an acceptable value for type for a snapshot.

We‚Äôve deprecated final (temporary) snapshots and therefore temporary is no longer an acceptable value for type for a snapshot.
The access token response from the OAuth API now returns a canonical UUID for an account. This should be used to canonically identify a user.

The access token response from the OAuth API now returns a canonical UUID for an account. This should be used to canonically identify a user.
Since releasing version 2 of our API nearly a year ago and officially bringing it out of beta last month, we‚Äôve seen a tremendous uptake of usage by our community. As the ecosystem of tools and libraries continues to grow, we‚Äôve decided that it is time to sunset version 1 of the API.
Don‚Äôt worry! We‚Äôre not going to pull the rug out from under you. In order to give everyone time to port their tools, version 1 will not be turned off until Monday, November 9, 2015.
With its (more) RESTful interface and features like OAuth support, v2 is both powerful and easy to use. Our developer documentation should give you all the information you need to begin the transition. If you have questions, you can always ask on our Community site or on Twitter.

Since releasing version 2 of our API nearly a year ago and officially bringing it out of beta last month, we‚Äôve seen a tremendous uptake of usage by our community. As the ecosystem of tools and libraries continues to grow, we‚Äôve decided that it is time to sunset version 1 of the API.
Don‚Äôt worry! We‚Äôre not going to pull the rug out from under you. In order to give everyone time to port their tools, version 1 will not be turned off until Monday, November 9, 2015.
With its (more) RESTful interface and features like OAuth support, v2 is both powerful and easy to use. Our developer documentation should give you all the information you need to begin the transition. If you have questions, you can always ask on our Community site or on Twitter.
We are very pleased to announce that API v2 is no longer in beta. Thank you to everyone who helped report bugs and suggest features during the beta period. Read more about this release on our blog.

We are very pleased to announce that API v2 is no longer in beta. Thank you to everyone who helped report bugs and suggest features during the beta period. Read more about this release on our blog.
The Image action endpoint now responds to a convert attribute, that allows backups and temporary snapshots to be saved permanently as snapshots. For more information, see the image actions documentation.

The Image action endpoint now responds to a convert attribute, that allows backups and temporary snapshots to be saved permanently as snapshots. For more information, see the image actions documentation.
Images objects now return a type attribute, describing whether they are snapshots, backups or temporary images. For more information, see the images documentation.

Images objects now return a type attribute, describing whether they are snapshots, backups or temporary images. For more information, see the images documentation.
Size objects now expose an available boolean attribute, which represents whether new Droplets can be created with the size.

Size objects now expose an available boolean attribute, which represents whether new Droplets can be created with the size.
All action objects, i.e. those returned by the /v2/actions, /v2/droplets/$ID/actions and /v2/images/$ID/actions endpoint now return a region_slug attribute, in addition to a region attribute. At 00:01 March 20, 2015 UTC, API v2 will start returning an embedded region object at the region attribute, not a slug.
For example, the API request:
curl -X GET -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer $DIGITALOCEAN_TOKEN' \
    "https://api.digitalocean.com/v2/actions?page=1&per_page=1"
would return:
{
  "actions": [
    {
      "id": 36804636,
      "status": "completed",
      "type": "create",
      "started_at": "2014-11-14T16:29:21Z",
      "completed_at": "2014-11-14T16:30:06Z",
      "resource_id": 3164444,
      "resource_type": "droplet",
      "region": {
        "name": "New York 3",
        "slug": "nyc3",
        "sizes": [
          "32gb",
          "16gb",
          "2gb",
          "1gb",
          "4gb",
          "8gb",
          "512mb",
          "64gb",
          "48gb"
        ],
        "features": [
          "virtio",
          "private_networking",
          "backups",
          "ipv6",
          "metadata"
        ],
        "available": true
      },
      "region_slug": "nyc3"
    }
  ],
  "links": {
    "pages": {
      "last": "https://api.digitalocean.com/v2/actions?page=159&per_page=1",
      "next": "https://api.digitalocean.com/v2/actions?page=2&per_page=1"
    }
  },
  "meta": {
    "total": 159
  }
}

All action objects, i.e. those returned by the /v2/actions, /v2/droplets/$ID/actions and /v2/images/$ID/actions endpoint now return a region_slug attribute, in addition to a region attribute. At 00:01 March 20, 2015 UTC, API v2 will start returning an embedded region object at the region attribute, not a slug.
For example, the API request:
would return:
Two new endpoints in API v2 report if droplets are running on the same physical hardware. They exist for an individual droplet or for an entire account.

Two new endpoints in API v2 report if droplets are running on the same physical hardware. They exist for an individual droplet or for an entire account.
The maximum allowed rate limit per O-Auth token has been increased to 5,000 requests/hour.

The maximum allowed rate limit per O-Auth token has been increased to 5,000 requests/hour.
The images now supports a private filter which will allow you
to retrieve all images that are specific to your account (IE: backups and snapshots).
For more information, you can view the documentation for this endpoint here.

The images now supports a private filter which will allow you
to retrieve all images that are specific to your account (IE: backups and snapshots).
For more information, you can view the documentation for this endpoint here.
API V2 now validates SSH key IDs and identifiers passed into the Droplet create call. In addition, API V2 now validates that requested features are available for a Droplet (backups, private networking, IPv6 and user data).

API V2 now validates SSH key IDs and identifiers passed into the Droplet create call. In addition, API V2 now validates that requested features are available for a Droplet (backups, private networking, IPv6 and user data).
The API v2 now supports retrieving images by type, to retrieve an image by type, simply append:
GET /v2/images?type={distribution,application}
Change type to what you would like to retrieve and voil√†!
DropletKit (The Ruby API Client) also supports this functionality as well in Version 1.1.0
You can view the documentation for this feature here.

The API v2 now supports retrieving images by type, to retrieve an image by type, simply append:
Change type to what you would like to retrieve and voil√†!
DropletKit (The Ruby API Client) also supports this functionality as well in Version 1.1.0
You can view the documentation for this feature here.
The JSON object for a droplet no longer contains a nested Size object, but rather a slug called size_slug that references a Size object. See the droplet docs for the updated structure.
The Image JSON object now includes a min_disk_size attribute that contains the slug of the minimum size droplet required for that image. For example a snapshot of a 1 Gig droplet will have ‚Äú1gb‚Äù as it‚Äôs min_disk_size.

The JSON object for a droplet no longer contains a nested Size object, but rather a slug called size_slug that references a Size object. See the droplet docs for the updated structure.
The Image JSON object now includes a min_disk_size attribute that contains the slug of the minimum size droplet required for that image. For example a snapshot of a 1 Gig droplet will have ‚Äú1gb‚Äù as it‚Äôs min_disk_size.
Remove embedded action_ids from Droplet and Image.

Remove embedded action_ids from Droplet and Image.
Both price_monthly and price_hourly were previously strings. This made them harder to work with so we have turned them into floats.

Both price_monthly and price_hourly were previously strings. This made them harder to work with so we have turned them into floats.
We have tweaked the per_page limits to default to 20 and be a maximum of 200. We have found in our testing, so far, for this to be a good balance of requests versus results. Head on over and read up on pagination.

We have tweaked the per_page limits to default to 20 and be a maximum of 200. We have found in our testing, so far, for this to be a good balance of requests versus results. Head on over and read up on pagination.
API V2 now supports disabling backups on a Droplet.

API V2 now supports disabling backups on a Droplet.
API V2 now supports expanding a droplet‚Äôs disk size, along with other resources.

API V2 now supports expanding a droplet‚Äôs disk size, along with other resources.
Want to know which regions support IPv6 or Private Networking? It is now possible to check which features are enabled in each region.

Want to know which regions support IPv6 or Private Networking? It is now possible to check which features are enabled in each region.
It seems adding X- to custom HTTP headers is going out of style, so we have changed our RateLimit headers to no longer include the X.
They now look like this:
RateLimit-Limit:
RateLimit-Remaining:
RateLimit-Reset:

It seems adding X- to custom HTTP headers is going out of style, so we have changed our RateLimit headers to no longer include the X.
They now look like this:
Please try using alternative keywords or simplifying your search terms.
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
DigitalOcean research on trends in cloud, open source, and small and medium-sized businesses. 
deploy is back November 15‚Äì16, 2022!Get started, get connected, and get growing at DigitalOcean‚Äôs annual virtual conference, where the software builder (you) is central to everything we do.
From one developer to the next, we understand the complex environment when building and scaling applications. This year, we‚Äôll focus on how businesses can subtract complexity and add happiness to the developer experience.
We‚Äôll pay homage to our growing community of business builders and social impact developers by sharing their startup journeys and demonstrating ideal pathways to help you get from point A to B faster.
Join us for two days of learning, sharing, and networking with professional developers and technical business managers who understand the challenges of working with complex systems.
Sign up for updates and be the first to know about the speaker lineup, agenda, and more!
Subtract complexity -> Add developer happiness -> Create software that changes the world -> Subtract complexity -> Add developer happiness -> Create software that changes the world -> Subtract complexity -> Add developer happiness -> Create software that changes the world -> Subtract complexity -> Add developer happiness -> Create software that changes the world -> 
deploy by DigitalOcean attracts speakers that can create talks around solving complex problems with practical (and simple) strategies. Technologies and topics include:
Sessions are around 25 minutes long: 20 minutes for the talk, and 5 minutes for Q&A.
Submit your talk
Speakers will be required to:
With over 35,000 people in attendance representing 85+ countries, deploy continues to elevate the experience of virtual gatherings by providing best practices and simple solutions to challenges faced by SMBs and startups.
Apply to be a sponsor
November 15-16, 2022
Virtual conference for global development teams #DOdeploy
Join us on the mission to simplify the developer experience. Sign up for email updates!
Failed to fetch https://http://store.digitalocean.com/: HTTPSConnectionPool(host='http', port=443): Max retries exceeded with url: //store.digitalocean.com/ (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7e97a993b6d0>: Failed to resolve 'http' ([Errno -2] Name or service not known)"))
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
We‚Äôre looking for people like you to help us simplify cloud infrastructure and create an experience that users love!
From time to time, we‚Äôll invite you to participate in research. You might be asked to share your experience with one of our products, tell us about what you do, test new designs, or complete a survey. We‚Äôll give you all the details, so you can decide each time if you want to get involved.
Many sessions take place online. Others are held in person, across the globe at a DigitalOcean meetups or our Manhattan or Cambridge offices. You can limit the invitations to the venues that are right for you.
To join our Research Program, sign up here and we‚Äôll reach out the next time we‚Äôre running a relevant study.
The commitment for each research event varies. Often, we‚Äôll schedule a 30-60 minute appointment. Other times, you‚Äôll be invited to complete a survey or participate in a card sort that can be done on your own schedule. When you receive an invitation to a study, we‚Äôll always provide you with an estimate of the time involved.
No. We want input from the entire spectrum of people interested in cloud computing! No prior experience with cloud infrastructure in general or DigitalOcean specifically is required. We will use your answers to the questions on your profile to match your experience to the tasks at hand.
For each session you‚Äôre a part of, we‚Äôll compensate you for your time with your choice of DigitalOcean credit or a gift card.
No. Software development teams with people from all kinds of different roles use DigitalOcean: developers, data scientists, QA specialists, product owners, designers, system administrators and more. We welcome your participation!
All your personal information will be kept confidential. It will only be accessible by the DigitalOcean Research Team, consistent with our Terms of Service.
Of course! You can decline any invitation, and if you‚Äôd like to opt-out of our research program entirely, email us at research@digitalocean.com.
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
DigitalOcean is committed to ensuring that Community remains a welcoming destination for people from all walks of life to learn, share knowledge, and connect with one another. In order to maintain this environment, it is our expectation that Community members uphold the following code of conduct.
While here, you may encounter ideas, viewpoints, and technologies that are unfamiliar to you. You may even find some that you disagree with. The DigitalOcean Community welcomes multiple approaches to solve the same problem. As a platform for communal learning, please lead with generosity when interacting with others and avoid derailing the conversation and detracting from the educational value of content for others. We encourage members of the DigitalOcean Community to keep an open mind, and to commit to being thoughtful when sharing knowledge.
Content posted to Community must be on-topic and constructive. Promotional content, content designed to create backlinks, trolling, links to harmful websites, or content unrelated to the topic at hand may be removed.
Our goal is for everyone in our Community to feel welcome, which is why we encourage members to contribute in a way that is inclusive, supports learning, and promotes the welfare of others. To that end, malicious behavior, harassment, and offensive language that is in reference to (but not limited to) age, disability, ethnicity, gender identity or expression, level of experience, nationality, neurodiversity, personal appearance, race, religion, political affiliation, sexual orientation, socioeconomic status, or technology choices will not be tolerated.
The DigitalOcean Community is first and foremost a place for learning. Community members who purposely misinform others or who give advice in bad faith run the risk of their account being terminated.
DigitalOcean reserves the right to remove content that fails to meet the standards set forth in this code of conduct. Likewise, DigitalOcean reserves the right to prohibit individual users from participating in Community at its sole discretion. This policy applies to tutorials, comments, questions, answers, and reviews published on DigitalOcean Community.
Sign up for Infrastructure as a Newsletter.
Sign up
Working on improving health and education, reducing inequality, and spurring economic growth? We'd like to help.
Learn more
Get paid to write technical tutorials and select a tech-focused charity to receive a matching donation.
Learn more
DigitalOcean makes it simple to launch in the cloud and scale up as you grow ‚Äî whether you're running one virtual machine or ten thousand.
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
Get our roundup of sysadmin tutorials and open source happenings delivered to your inbox once you sign up and again every two weeks.
Sign up for Infrastructure as a Newsletter.
Sign up
Working on improving health and education, reducing inequality, and spurring economic growth? We'd like to help.
Learn more
Get paid to write technical tutorials and select a tech-focused charity to receive a matching donation.
Learn more
DigitalOcean makes it simple to launch in the cloud and scale up as you grow ‚Äî whether you're running one virtual machine or ten thousand.
But maybe not as awkward as when you go in for a high five and they go in for a fist bump.
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
DigitalOcean‚Äôs flexible SaaS hosting and cloud solutions are ideal for developers and businesses building Software as a Service (SaaS) solutions. DigitalOcean‚Äôs simple and intuitive tools, predictable pricing models, and robust community enable you to focus on building your SaaS while we focus on the infrastructure.
DigitalOcean‚Äôs features, products, and resources are tailored to the unique needs of SaaS builders, making it simple for you to build and grow your SaaS applications. 
DigitalOcean‚Äôs simple UI, CLI, and API enable small teams to get started quickly without the need to learn and manage complex infrastructure tools.
Our transparent pricing model, industry-leading price to performance ratio, and low bandwidth pricing enables you to scale without worrying about unexpected costs.
DigitalOcean includes all the cloud products SaaS builders need, from virtual machines to storage, managed databases, Platform as a Service, and managed Kubernetes.
Our community tutorials, documentation, and personal customer support mean you‚Äôre supported throughout your journey building a SaaS.
DigitalOcean has supported SaaS builders at all stages, from founders just starting out to rapidly scaling businesses. Our flexible SaaS development platform means SaaS solutions ranging from artificial intelligence systems, to marketing analytics platforms can grow with DigitalOcean.
DigitalOcean provides all of the infrastructure solutions that SaaS builders need, from compute to Kubernetes. 
Flexible, simple compute with Droplets virtual machines
DigitalOcean‚Äôs technical tutorials, product documentation, and library of resources can answer common questions about building a SaaS solution. 
Ready to get started on DigitalOcean? Sign up for an account and deploy your first Droplet virtual machine in just minutes, or contact our team using the form below if you need help with migrating your infrastructure. 
What SaaS building solution is right for me?
How much does your SaaS development platform cost?
What are 1-click Marketplace apps?
What support do you offer?
Was this page helpful?
Search for help from support articles, product documentation, community, and the DigitalOcean Marketplace.
Create a ticket with our support team.
Get solutions to common problems accessing your account.
Learn how to collaborate with DigitalOcean teams.
Check the current status of DigitalOcean services.
Contact sales for help with large deployments.
Please try using alternative keywords or simplifying your search terms.
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
We are on a mission to help our customers focus on testing their Ideas, building their businesses and realizing their dreams.  When we are working with a business we aim to make their migrations as smooth as possible and answer any questions ahead of time.
Immediate, on-demand, proof-of-concept support for your migration, so you‚Äôre never left wondering what‚Äôs going on or what will happen next.
Technical experts and dedicated account managers provide ongoing guidance and support.
Our transparent, predictable pricing ensures that you know what you‚Äôll always know what you‚Äôre paying. Period.
David Shifley
CTO, RouteTrust
Joe Puccio
Cofounder of Coursicle
Douglas Cherry
Cofounder of Snapt
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
DigitalOcean is a platform that's committed to a better Internet. If you know about, or are a victim of, abuse on a site hosted by DigitalOcean, you can use this form to report the problem to our SOC team.Explore our Security Center ->
If you need any other help with using DigitalOcean, please contact our support team; they are here to help.Sign In and Create Ticket ->
Resend OTP in:  seconds 

                    Didn't receive the OTP?
                    Resend OTP 

Resend OTP in: 30 seconds 

                      Didn't receive the OTP?
                      Resend OTP 

No incidents reported today.
No incidents reported.
No incidents reported.
No incidents reported.
No incidents reported.
No incidents reported.
No incidents reported.
No incidents reported.
No incidents reported.
No incidents reported.
No incidents reported.
Featured Products
Compute
Backups & Snapshots
Storage
Networking
Managed Databases
Developer Tools
AI/ML
Cloud Website Hosting
Cloudways
By industry
By use case
Questions?
Our community
Resources
Get Involved
Documentation
Grow your business
Resources
Featured articles
Contact
Failed to fetch https://twitter.com/digitalocean: 400 Client Error: Bad Request for url: https://twitter.com/digitalocean
Failed to fetch https://www.glassdoor.com/Overview/Working-at-DigitalOcean-EI_IE823482.11,23.htm: 403 Client Error: Forbidden for url: https://www.glassdoor.com/Overview/Working-at-DigitalOcean-EI_IE823482.11,23.htm
Colab paid products - Cancel contracts here

Node.js¬Æ is an open-source, cross-platform JavaScript runtime environment.
For information about supported releases, see the release schedule.
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting
Node.js¬Æ is an open-source, cross-platform JavaScript runtime environment.
For information about supported releases, see the release schedule.
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting
Node.js is an open-source and cross-platform JavaScript runtime environment. It is a popular tool for almost any kind of project!
Node.js runs the V8 JavaScript engine, the core of Google Chrome, outside of the browser. This allows Node.js to be very performant.
A Node.js app runs in a single process, without creating a new thread for every request. Node.js provides a set of asynchronous I/O primitives in its standard library that prevent JavaScript code from blocking and generally, libraries in Node.js are written using non-blocking paradigms, making blocking behavior the exception rather than the norm.
When Node.js performs an I/O operation, like reading from the network, accessing a database or the filesystem, instead of blocking the thread and wasting CPU cycles waiting, Node.js will resume the operations when the response comes back.
This allows Node.js to handle thousands of concurrent connections with a single server without introducing the burden of managing thread concurrency, which could be a significant source of bugs.
Node.js has a unique advantage because millions of frontend developers that write JavaScript for the browser are now able to write the server-side code in addition to the client-side code without the need to learn a completely different language.
In Node.js the new ECMAScript standards can be used without problems, as you don't have to wait for all your users to update their browsers - you are in charge of deciding which ECMAScript version to use by changing the Node.js version, and you can also enable specific experimental features by running Node.js with flags.
The most common example Hello World of Node.js is a web server:
To run this snippet, save it as a server.js file and run node server.js in your terminal.
This code first includes the Node.js http module.
Node.js has a fantastic standard library, including first-class support for networking.
The createServer() method of http creates a new HTTP server and returns it.
The server is set to listen on the specified port and host name. When the server is ready, the callback function is called, in this case informing us that the server is running.
Whenever a new request is received, the request event is called, providing two objects: a request (an http.IncomingMessage object) and a response (an http.ServerResponse object).
Those 2 objects are essential to handle the HTTP call.
The first provides the request details. In this simple example, this is not used, but you could access the request headers and request data.
The second is used to return data to the caller.
In this case with:
we set the statusCode property to 200, to indicate a successful response.
We set the Content-Type header:
and we close the response, adding the content as an argument to end():
See https://github.com/nodejs/examples for a list of Node.js examples that go beyond hello world.
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting
As an asynchronous event-driven JavaScript runtime, Node.js is designed to build
scalable network applications. In the following "hello world" example, many
connections can be handled concurrently. Upon each connection, the callback is
fired, but if there is no work to be done, Node.js will sleep.
This is in contrast to today's more common concurrency model, in which OS threads
are employed. Thread-based networking is relatively inefficient and very
difficult to use. Furthermore, users of Node.js are free from worries of
dead-locking the process, since there are no locks. Almost no function in
Node.js directly performs I/O, so the process never blocks except when the I/O is performed using
synchronous methods of Node.js standard library. Because nothing blocks, scalable systems are very
reasonable to develop in Node.js.
If some of this language is unfamiliar, there is a full article on
Blocking vs. Non-Blocking.
Node.js is similar in design to, and influenced by, systems like Ruby's
Event Machine and Python's Twisted. Node.js takes the event model a bit
further. It presents an event loop as a runtime construct instead of as a library. In other systems,
there is always a blocking call to start the event-loop.
Typically, behavior is defined through callbacks at the beginning of a script, and
at the end a server is started through a blocking call like EventMachine::run().
In Node.js, there is no such start-the-event-loop call. Node.js simply enters the event loop after executing the input script. Node.js
exits the event loop when there are no more callbacks to perform. This behavior
is like browser JavaScript ‚Äî the event loop is hidden from the user.
HTTP is a first-class citizen in Node.js, designed with streaming and low
latency in mind. This makes Node.js well suited for the foundation of a web
library or framework.
Node.js being designed without threads doesn't mean you can't take
advantage of multiple cores in your environment. Child processes can be spawned
by using our child_process.fork() API, and are designed to be easy to
communicate with. Built upon that same interface is the cluster module,
which allows you to share sockets between processes to enable load balancing
over your cores.
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting
Latest LTS Version: 20.11.1 (includes npm 10.2.4)
Download the Node.js source code or a pre-built installer for your platform, and start developing today.

node-v20.11.1-x86.msi
node-v20.11.1.pkg
node-v20.11.1.tar.gz
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting
Note: The Node.js Guides are being archived and incrementally being replaced by the "Learn" section of the Node.js Website. These Guides will remain available at this location until the transition is complete.
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting
Application development involves building applications for use in a wide variety of technologies. This could include web applications, mobile applications, cloud applications, desktop applications and more. Application development is less concerned with the architecture and maintenance of an operating system, and instead focuses on building new applications for that operating system.
Enroll Now
Get early access to the latest Linux Foundation Training news, tutorials and exclusive offers ‚Äì available only for monthly newsletter subscribers.
We won‚Äôt ever send you spam, promise.
¬© 2024 Linux Foundation - Training.  The Linux Foundation¬Æ. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our Trademark Usage page. Linux is a registered trademark of Linus Torvalds. Terms of Use | Privacy Policy | Bylaws | Trademark Usage | Antitrust Policy | Good Standing PolicyAccelerated by 
Stay up to date with the newest courses, certifications, and promotions from the LF training team.
Stay up to date with the newest courses, certifications, and promotions from the LF training team.
Thank you for your interest in Linux Foundation training and certification. We think we can better serve you from our China Training site. To access this site please click below. 
ÊÑüË∞¢ÊÇ®ÂØπLinux FoundationÂüπËÆ≠ÁöÑÂÖ≥Ê≥®„ÄÇ‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞‰∏∫ÊÇ®ÊúçÂä°ÔºåÊàë‰ª¨Â∞ÜÊÇ®ÈáçÂÆöÂêëÂà∞‰∏≠ÂõΩÂüπËÆ≠ÁΩëÁ´ô„ÄÇ
                Êàë‰ª¨ÊúüÂæÖÂ∏ÆÂä©ÊÇ®ÂÆûÁé∞Âú®‰∏≠ÂõΩÂå∫ÂÜÖÊâÄÊúâÁ±ªÂûãÁöÑÂºÄÊ∫êÂüπËÆ≠ÁõÆÊ†á„ÄÇ
Updates are now available for the v18.x, v20.x and v21.x Node.js release lines for the
following issues.
This security release includes the following dependency updates to address public vulnerabilities:
This security release includes an OpenSSL update to version 3.0.13+quic1 on all release lines to address advisories:
On Linux, Node.js ignores certain environment variables if those may have been set by an unprivileged user while the process is running with elevated privileges with the only exception of CAP_NET_BIND_SERVICE.
Due to a bug in the implementation of this exception, Node.js incorrectly applies this exception even when certain other capabilities have been set.
This allows unprivileged users to inject code that inherits the process's elevated privileges.
Impacts:
Thank you, to Tobias Nie√üen for reporting this vulnerability and for fixing it.
A vulnerability in Node.js HTTP servers allows an attacker to send a specially crafted HTTP request with chunked encoding,
leading to resource exhaustion and denial of service (DoS).
The server reads an unbounded number of bytes from a single connection, exploiting the lack of limitations on chunk extension bytes.
The issue can cause CPU and network bandwidth exhaustion, bypassing standard safeguards like timeouts and body size limits.
Impacts:
Thank you, to Bartek Nowotarski for reporting this vulnerability and thank you Paolo Insogna
for fixing it.
The permission model protects itself against path traversal attacks by calling path.resolve() on any paths given by the user.
If the path is to be treated as a Buffer, the implementation uses Buffer.from() to obtain a Buffer from the result of path.resolve().
By monkey-patching Buffer internals, namely, Buffer.prototype.utf8Write, the application can modify the result of path.resolve(), which leads to a path traversal vulnerability.
Impacts:
Please note that at the time this CVE was issued, the permission model is an experimental feature of Node.js.
Thank you, to Tobias Nie√üen for reporting this vulnerability and for fixing it.
setuid() does not affect libuv's internal io_uring operations if initialized before the call to setuid().
This allows the process to perform privileged operations despite presumably having dropped such privileges through a call to setuid().
Impacts:
Thank you, to valette for reporting this vulnerability and thank you Tobias Nie√üen
for fixing it.
A vulnerability in the privateDecrypt() API of the crypto library, allowed a covert timing side-channel during PKCS#1 v1.5 padding error handling.
The vulnerability revealed significant timing differences in decryption for valid and invalid ciphertexts.
This poses a serious threat as attackers could remotely exploit the vulnerability to decrypt captured RSA ciphertexts or forge signatures, especially in scenarios involving API endpoints processing Json Web Encryption messages.
Impacts:
Thank you, to hkario for reporting this vulnerability and thank you Michael Dawson
for fixing it.
Node.js depends on multiple built-in utility functions to normalize paths provided to node:fs functions, which can be overwitten with user-defined implementations leading to filesystem permission model bypass through path traversal attack.
Impacts:
Please note that at the time this CVE was issued, the permission model is an experimental feature of Node.js.
Thank you, to xion for reporting this vulnerability and thank you Rafael Gonzaga
for fixing it.
The Node.js Permission Model does not clarify in the documentation that wildcards should be only used as the last character of a file path.
For example: --allow-fs-read=/home/node/.ssh/*.pub will ignore pub and give access to everything after .ssh/.
Impacts:
Please note that at the time this CVE was issued, the permission model is an experimental feature of Node.js.
Thank you, to Tobias Nie√üen for reporting this vulnerability and thank you Rafael Gonzaga
for fixing it.
Due to a long pipeline test on this security release, additional time was required.
Therefore, the new target date is Wednesday 14th.
We have encounted an error in one of our patches, therefore, the release will take a bit longer than expected
and the Node.js Security Releases should be available on, or shortly after, Tuesday, February 13th, 2024.
Preparing the releases is taking us a bit longer than originally expected and the Node.js Security Releases
will be available on, or shortly after, Thursday, February 8th, 2024.
The Node.js project will release new versions of the 18.x, 20.x and, 21.x
releases lines on or shortly after, Tuesday February 6 2024 in order to address:
The 21.x release line of Node.js is vulnerable to 4 high severity issues, 4 medium severity issues, and 1 low severity issue.
The 20.x release line of Node.js is vulnerable to 4 high severity issues, 4 medium severity issues, and 1 low severity issue.
In addition, the 18.x release line of Node.js is vulnerable to 2 high severity issues, 2 medium severity issues, and 1 low severity issue.
Releases will be available on, or shortly after, Tuesday February 6 2024.
The current Node.js security policy can be found at https://nodejs.org/security/. Please follow the process outlined in https://github.com/nodejs/node/security/policy if you wish to report a vulnerability in Node.js.
Subscribe to the low-volume announcement-only nodejs-sec mailing list at https://groups.google.com/forum/#!forum/nodejs-sec to stay up to date on security vulnerabilities and security-related releases of Node.js and the projects maintained in the nodejs GitHub organization.
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting
WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
Latest LTS Version: 20.11.1 (includes npm 10.2.4)
Download the Node.js source code or a pre-built installer for your platform, and start developing today.

node-v20.11.1-x86.msi
node-v20.11.1.pkg
node-v20.11.1.tar.gz
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting

This is a security release.


The new flag --experimental-default-type can be used to flip the default\nmodule system used by Node.js. Input that is already explicitly defined as ES\nmodules or CommonJS, such as by a package.json \"type\" field or .mjs/.cjs\nfile extension or the --input-type flag, is unaffected. What is currently\nimplicitly CommonJS would instead be interpreted as ES modules under\n--experimental-default-type=module:
String input provided via --eval or STDIN, if --input-type is unspecified.
Files ending in .js or with no extension, if there is no package.json file\npresent in the same folder or any parent folder.
Files ending in .js or with no extension, if the nearest parent\npackage.json field lacks a type field; unless the folder is inside a\nnode_modules folder.
In addition, extensionless files are interpreted as Wasm if\n--experimental-wasm-modules is passed and the file contains the \"magic bytes\"\nWasm header.
Contributed by Geoffrey Booth in #49869.
The new flag --experimental-detect-module can be used to automatically run ES\nmodules when their syntax can be detected. For ‚Äúambiguous‚Äù files, which are\n.js or extensionless files with no package.json with a type field, Node.js\nwill parse the file to detect ES module syntax; if found, it will run the file\nas an ES module, otherwise it will run the file as a CommonJS module. The same\napplies to string input via --eval or STDIN.
We hope to make detection enabled by default in a future version of Node.js.\nDetection increases startup time, so we encourage everyone‚Äîespecially package\nauthors‚Äîto add a type field to package.json, even for the default\n\"type\": \"commonjs\". The presence of a type field, or explicit extensions\nsuch as .mjs or .cjs, will opt out of detection.
Contributed by Geoffrey Booth in #50096.
When writing to files, it is possible that data is not immediately flushed to\npermanent storage. This allows subsequent read operations to see stale data.\nThis PR adds a 'flush' option to the fs.writeFile family of functions which\nforces the data to be flushed at the end of a successful write operation.
Contributed by Colin Ihrig in #50009 and #50095.
Adds a --experimental-websocket flag that adds a WebSocket\nglobal, as standardized by WHATWG.
Contributed by Matthew Aitken in #49830.
Previously repeated compilation of the same source code using vm.Script\nstopped hitting the V8 compilation cache after v16.x when support for\nimportModuleDynamically was added to vm.Script, resulting in a performance\nregression that blocked users (in particular Jest users) from upgrading from\nv16.x.
The recent fixes allow the compilation cache to be hit again\nfor vm.Script when --experimental-vm-modules is not used even in the\npresence of the importModuleDynamically option, so that users affected by the\nperformance regression can now upgrade. Ongoing work is also being done to\nenable compilation cache support for vm.CompileFunction.
Contributed by Joyee Cheung in #49950\nand #50137.

This release marks the transition of Node.js 20.x into Long Term Support (LTS)\nwith the codename 'Iron'. The 20.x release line now moves into \"Active LTS\"\nand will remain so until October 2024. After that time, it will move into\n\"Maintenance\" until end of life in April 2026.
Collecting code coverage via the NODE_V8_COVERAGE environment variable may\nlead to a hang. This is not thought to be a regression in Node.js 20 (some\nreports are on Node.js 18). For more information, including some potential\nworkarounds, see issue #49344.

This is a security release.
The following CVEs are fixed in this release:
More detailed information on each of the vulnerabilities can be found in October 2023 Security Releases blog post.

Performance improvements to writable and readable streams, improving the creation and destruction by ¬±15% and reducing the memory overhead each stream takes in Node.js
Contributed by Benjamin Gruenbaum in #49745 and Raz Luvaton in #49834.
Performance improvements for readable webstream, improving readable stream async iterator consumption by ¬±140% and improving readable stream pipeTo consumption by ¬±60%
Contributed by Raz Luvaton in #49662 and #49690.
This rework addressed a series of long-standing memory leaks and use-after-free issues in the following APIs that support importModuleDynamically:
This should enable affected users to upgrade from older versions of Node.js.
Contributed by Joyee Cheung in #48510.



Starting from Node.js v20.6.0, Node.js supports .env files for configuring environment variables.
Your configuration file should follow the INI file format, with each line containing a key-value pair for an environment variable.\nTo initialize your Node.js application with predefined configurations, use the following CLI command: node --env-file=config.env index.js.
For example, you can access the following environment variable using process.env.PASSWORD when your application is initialized:
In addition to environment variables, this change allows you to define your NODE_OPTIONS directly in the .env file, eliminating the need to include it in your package.json.
This feature was contributed by Yagiz Nizipli in #48890.
In ES modules, import.meta.resolve(specifier) can be used to get an absolute URL string to which specifier resolves, similar to require.resolve in CommonJS. This aligns Node.js with browsers and other server-side runtimes.
This feature was contributed by Guy Bedford in #49028
There is a new API register available on node:module to specify a file that exports module customization hooks, and pass data to the hooks, and establish communication channels with them. The ‚Äúdefine the file with the hooks‚Äù part was previously handled by a flag --experimental-loader, but when the hooks moved into a dedicated thread in 20.0.0 there was a need to provide a way to communicate between the main (application) thread and the hooks thread. This can now be done by calling register from the main thread and passing data, including MessageChannel instances.
We encourage users to migrate to an approach that uses --import with register, such as:
Using --import ensures that the customization hooks are registered before any application code runs, even the entry point.
This feature was contributed by Jo√£o Lenon and Jacob Smith in #46826, Izaak Schroeder and Jacob Smith in #48842 and #48559
Authors of module customization hooks can how handle both ES module and CommonJS sources in the load hook. This works for CommonJS modules referenced via either import or require, so long as the main entry point of the application is handled by the ES module loader (such as because the entry point is an ES module file, or if the --import flag is passed). This should simplify the customization of the Node.js module loading process, as package authors can customize more of Node.js without relying on deprecated APIs such as require.extensions.
This feature was contributed by Antoine du Hamel in #47999
Now when Node.js starts up, it makes sure that there is a v8::CppHeap attached to the V8 isolate. This enables users to allocate in the v8::CppHeap using <cppgc/*> headers from V8, which are now also included into the Node.js headers available to addons. Note that since Node.js only bundles the cppgc library coming from V8, the ABI stability of cppgc is currently not guaranteed in semver-minor and -patch updates, but we do not expect the ABI to break often, as it has been stable and battle-tested in Chromium for years. We may consider including cppgc into the ABI stability guarantees when it gets enough adoption internally and externally.
To help addon authors create JavaScript-to-C++ references of which V8's garbage collector can be aware, a helper function node::SetCppgcReference(isolate, js_object, cppgc_object) has been added to node.h. V8 may provide a native alternative in the future, which could then replace this Node.js-specific helper. In the mean time, users can use this API to avoid having to hard-code the layout of JavaScript wrapper objects. An example of how to create garbage-collected C++ objects in the unified heap and wrap it in a JavaScript object can be found in the Node.js addon tests.
The existing node::ObjectWrap helper would continue to work, while cppgc-based object management serves as an alternative with some advantages mentioned in the V8 blog post about Oilpan.
This feature was contributed by Daryl Haresign and Joyee Cheung in #48660 and #45704.

This is a security release.
The following CVEs are fixed in this release:
More detailed information on each of the vulnerabilities can be found in August 2023 Security Releases blog post.


The new feature allows developers to write more reliable and predictable tests for time-dependent functionality.\nIt includes MockTimers with the ability to mock setTimeout, setInterval from globals, node:timers, and node:timers/promises.
The feature provides a simple API to advance time, enable specific timers, and release all timers.
This feature was contributed by Erick Wendel in #47775.
Node is adding support to the explicit resource management\nproposal to its resources allowing users of TypeScript/babel to use using/await using with\nV8 support for everyone else on the way.
This feature was contributed by Moshe Atlow and Benjamin Gruenbaum in #48518.

This is a security release.
The following CVEs are fixed in this release:
More detailed information on each of the vulnerabilities can be found in June 2023 Security Releases blog post.




We're excited to announce the release of Node.js 20! Highlights include the new Node.js Permission Model,\na synchronous import.meta.resolve, a stable test_runner, updates of the V8 JavaScript engine to 11.3, Ada to 2.0,\nand more!
As a reminder, Node.js 20 will enter long-term support (LTS) in October, but until then, it will be the \"Current\" release for the next six months.\nWe encourage you to explore the new features and benefits offered by this latest release and evaluate their potential impact on your applications.
Node.js now has an experimental feature called the Permission Model.\nIt allows developers to restrict access to specific resources during program execution, such as file system operations,\nchild process spawning, and worker thread creation.\nThe API exists behind a flag --experimental-permission which when enabled will restrict access to all available permissions.\nBy using this feature, developers can prevent their applications from accessing or modifying sensitive data or running potentially harmful code.\nMore information about the Permission Model can be found in the Node.js documentation.
The Permission Model was a contribution by Rafael Gonzaga in #44004.
ESM hooks supplied via loaders (--experimental-loader=foo.mjs) now run in a dedicated thread, isolated from the main thread.\nThis provides a separate scope for loaders and ensures no cross-contamination between loaders and application code.
Synchronous import.meta.resolve()
In alignment with browser behavior, this function now returns synchronously.\nDespite this, user loader resolve hooks can still be defined as async functions (or as sync functions, if the author prefers).\nEven when there are async resolve hooks loaded, import.meta.resolve will still return synchronously for application code.
Contributed by Anna Henningsen, Antoine du Hamel, Geoffrey Booth, Guy Bedford, Jacob Smith, and Micha√´l Zasso in #44710
The V8 engine is updated to version 11.3, which is part of Chromium 113.\nThis version includes three new features to the JavaScript API:
The V8 update was a contribution by Micha√´l Zasso in #47251.
The recent update to Node.js, version 20, includes an important change to the test_runner module. The module has been marked as stable after a recent update.\nPreviously, the test_runner module was experimental, but this change marks it as a stable module that is ready for production use.
Contributed by Colin Ihrig in #46983
Node.js v20 comes with the latest version of the URL parser, Ada. This update brings significant performance improvements\nto URL parsing, including enhancements to the url.domainToASCII and url.domainToUnicode functions in node:url.
Ada 2.0 has been integrated into the Node.js codebase, ensuring that all parts of the application can benefit from the\nimproved performance. Additionally, Ada 2.0 features a significant performance boost over its predecessor, Ada 1.0.4,\nwhile also eliminating the need for the ICU requirement for URL hostname parsing.
Contributed by Yagiz Nizipli and Daniel Lemire in #47339
Building a single executable app now requires injecting a blob prepared by\nNode.js from a JSON config instead of injecting the raw JS file.\nThis opens up the possibility of embedding multiple co-existing resources into the SEA (Single Executable Apps).
Contributed by Joyee Cheung in #47125
Web Crypto API functions' arguments are now coerced and validated as per their WebIDL definitions like in other Web Crypto API implementations.\nThis further improves interoperability with other implementations of Web Crypto API.
This change was made by Filip Skokan in #46067.
Node.js now includes binaries for ARM64 Windows, allowing for native execution on the platform.\nThe MSI, zip/7z packages, and executable are available from the Node.js download site along with all other platforms.\nThe CI system was updated and all changes are now fully tested on ARM64 Windows, to prevent regressions and ensure compatibility.
ARM64 Windows was upgraded to tier 2 support by Stefan Stojanovic in #47233.
When new WASI() is called, the version option is now required and has no default value.\nAny code that relied on the default for the version will need to be updated to request a specific version.
This change was made by Michael Dawson in #47391.
url.parse() accepts URLs with ports that are not numbers. This behavior might result in host name spoofing with unexpected input.\nThese URLs will throw an error in future versions of Node.js, as the WHATWG URL API does already.\nStarting with Node.js 20, these URLS cause url.parse() to emit a warning.
WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
Latest Current Version: 21.6.2 (includes npm 10.2.4)
Download the Node.js source code or a pre-built installer for your platform, and start developing today.

node-v21.6.2-x86.msi
node-v21.6.2.pkg
node-v21.6.2.tar.gz
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting

This is a security release.

This release fixes a bug in undici using WebStreams

Three new events were added in the net.createConnection flow:
Additionally, a previous bug has been fixed where a new connection attempt could have been started after a previous one failed and after the connection was destroyed by the user.\nThis led to a failed assertion.
Contributed by Paolo Insogna in #51045.
Node.js 21.6.0 comes with several fixes for the experimental permission model and two new semver-minor commits.\nWe're adding a new flag --allow-addons to enable addon usage when using the Permission Model.
Contributed by Rafael Gonzaga in #51183
And relative paths are now supported through the --allow-fs-* flags.\nTherefore, with this release one can use:
To give only read access to the entrypoint of the application.
Contributed by Rafael Gonzaga and Carlos Espa in #50758
We are adding a new flag --build-snapshot-config to configure snapshots through a custom JSON configuration file.
When using this flag, additional script files provided on the command line will\nnot be executed and instead be interpreted as regular command line arguments.
These changes were contributed by Joyee Cheung and Anna Henningsen in #50453


This release fixes a regression introduced in v21.3.0 that caused the fs.writeFileSync\nmethod to throw when called with 'utf8' encoding, no flag option, and if the target file didn't exist yet.

This version adds a new --disable-warning option that allows users to disable specific warnings either by code\n(i.e. DEP0025) or type (i.e. DeprecationWarning, ExperimentalWarning).
This option works alongside existing --warnings and --no-warnings.
For example, the following script will not emit DEP0025 require('node:sys') when executed with\nnode --disable-warning=DEP0025:
Contributed by Ethan-Arrowood in #50661
This is the certdata.txt from NSS 3.95, released on 2023-11-16.
This is the version of NSS that will ship in Firefox 121 on\n2023-12-19.
Certificates added:
Certificates removed:
Enhanced writeFileSync functionality by implementing a highly efficient fast path primarily in C++ for UTF8-encoded string data.\nAdditionally, optimized the appendFileSync method by leveraging the improved writeFileSync functionality.\nFor simplicity and performance considerations, the current implementation supports only string data,\nas benchmark results raise concerns about the efficacy of using Buffer for this purpose.\nFuture optimizations and expansions may be explored, but for now, the focus is on maximizing efficiency for string data operations.
Contributed by CanadaHonk in #49884.


The new flag --experimental-detect-module can be used to automatically run\nES modules when their syntax can be detected. For ‚Äúambiguous‚Äù files, which are\n.js or extensionless files with no package.json with a type field, Node.js\nwill parse the file to detect ES module syntax; if found, it will run the file\nas an ES module, otherwise it will run the file as a CommonJS module.\nThe same applies to string input via --eval or STDIN.
We hope to make detection enabled by default in a future version of Node.js.\nDetection increases startup time, so we encourage everyone ‚Äî especially package\nauthors ‚Äî to add a type field to package.json, even for the default\n\"type\": \"commonjs\". The presence of a type field, or explicit extensions\nsuch as .mjs or .cjs, will opt out of detection.
Contributed by Geoffrey Booth in #50096.
Previously repeated compilation of the same source code using vm.Script\nstopped hitting the V8 compilation cache after v16.x when support for\nimportModuleDynamically was added to vm.Script, resulting in a performance\nregression that blocked users (in particular Jest users) from upgrading from\nv16.x.
The recent fixes landed in v21.1.0 allow the compilation cache to be hit again\nfor vm.Script when --experimental-vm-modules is not used even in the\npresence of the importModuleDynamically option, so that users affected by the\nperformance regression can now upgrade. Ongoing work is also being done to\nenable compilation cache support for vm.CompileFunction.
Contributed by Joyee Cheung in #50137.

We're excited to announce the release of Node.js 21! Highlights include updates of the V8 JavaScript engine to 11.8,\nstable fetch and WebStreams, a new experimental flag to change the interpretation of ambiguous code\nfrom CommonJS to ES modules (--experimental-default-type), many updates to our test runner, and more!
Node.js 21 will replace Node.js 20 as our ‚ÄòCurrent‚Äô release line when Node.js 20 enters long-term support (LTS) later this month.\nAs per the release schedule, Node.js 21 will be ‚ÄòCurrent' release for the next 6 months, until April 2024.
Major Node.js versions enter Current release status for six months, which gives library authors time to add support for them.
After six months, odd-numbered releases (9, 11, etc.) become unsupported, and even-numbered releases (10, 12, etc.) move to Active LTS status and are ready for general use.
LTS release status is "long-term support", which typically guarantees that critical bugs will be fixed for a total of 30 months.
Production applications should only use Active LTS or Maintenance LTS releases.

Full details regarding Node.js release schedule are available on GitHub.
Copyright OpenJS Foundation and Node.js contributors. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.
The OpenJS Foundation | Trademark Policy | Privacy Policy | Code of Conduct | Security Reporting
OpenJS promotes the widespread adoption and continued development of key JavaScript technologies worldwide.
Home to projects that are changing the web
Browse all projects
The OpenJS Foundation is the central place to support collaborative development of JavaScript and web technologies. Interested in joining? Learn more about membership benefits and how to join!
Our members support the community
The OpenJS Foundation recognizes the critical supporting role of these organizations, and thanks them for their ongoing support of our project communities.
Stay up to date with the latest news and updates from the OpenJS.
Copyright ¬© OpenJS Foundation. All rights reserved. The OpenJS Foundation has registered trademarks and uses trademarks. For a list of trademarks of the OpenJS Foundation, please see our Trademark Policy and Trademark List. Trademarks and logos not indicated on the list of OpenJS Foundation trademarks are trademarks‚Ñ¢ or registered¬Æ trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.OpenJS Foundation | Terms of Use | Privacy Policy | Bylaws | Code of Conduct | Trademark Policy | Trademark List | Cookie Policy
WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
Streaming output truncated to the last 5000 lines.
event (unless emitClose is set to false). After this call, the readable
stream will release any internal resources and subsequent calls to push()
will be ignored.
Once destroy() has been called any further calls will be a no-op and no
further errors except from _destroy() may be emitted as 'error'.
Implementors should not override this method, but instead implement
readable._destroy().
Is true after 'close' has been emitted.
Is true after readable.destroy() has been called.
The readable.isPaused() method returns the current operating state of the
Readable. This is used primarily by the mechanism that underlies the
readable.pipe() method. In most typical cases, there will be no reason to
use this method directly.
The readable.pause() method will cause a stream in flowing mode to stop
emitting 'data' events, switching out of flowing mode. Any data that
becomes available will remain in the internal buffer.
The readable.pause() method has no effect if there is a 'readable'
event listener.
The readable.pipe() method attaches a Writable stream to the readable,
causing it to switch automatically into flowing mode and push all of its data
to the attached Writable. The flow of data will be automatically managed
so that the destination Writable stream is not overwhelmed by a faster
Readable stream.
The following example pipes all of the data from the readable into a file
named file.txt:
It is possible to attach multiple Writable streams to a single Readable
stream.
The readable.pipe() method returns a reference to the destination stream
making it possible to set up chains of piped streams:
By default, stream.end() is called on the destination Writable
stream when the source Readable stream emits 'end', so that the
destination is no longer writable. To disable this default behavior, the end
option can be passed as false, causing the destination stream to remain open:
One important caveat is that if the Readable stream emits an error during
processing, the Writable destination is not closed automatically. If an
error occurs, it will be necessary to manually close each stream in order
to prevent memory leaks.
The process.stderr and process.stdout Writable streams are never
closed until the Node.js process exits, regardless of the specified options.
The readable.read() method reads data out of the internal buffer and
returns it. If no data is available to be read, null is returned. By default,
the data is returned as a Buffer object unless an encoding has been
specified using the readable.setEncoding() method or the stream is operating
in object mode.
The optional size argument specifies a specific number of bytes to read. If
size bytes are not available to be read, null will be returned unless
the stream has ended, in which case all of the data remaining in the internal
buffer will be returned.
If the size argument is not specified, all of the data contained in the
internal buffer will be returned.
The size argument must be less than or equal to 1 GiB.
The readable.read() method should only be called on Readable streams
operating in paused mode. In flowing mode, readable.read() is called
automatically until the internal buffer is fully drained.
Each call to readable.read() returns a chunk of data, or null. The chunks
are not concatenated. A while loop is necessary to consume all data
currently in the buffer. When reading a large file .read() may return null,
having consumed all buffered content so far, but there is still more data to
come not yet buffered. In this case a new 'readable' event will be emitted
when there is more data in the buffer. Finally the 'end' event will be
emitted when there is no more data to come.
Therefore to read a file's whole contents from a readable, it is necessary
to collect chunks across multiple 'readable' events:
A Readable stream in object mode will always return a single item from
a call to readable.read(size), regardless of the value of the
size argument.
If the readable.read() method returns a chunk of data, a 'data' event will
also be emitted.
Calling stream.read([size]) after the 'end' event has
been emitted will return null. No runtime error will be raised.
Is true if it is safe to call readable.read(), which means
the stream has not been destroyed or emitted 'error' or 'end'.


Returns whether the stream was destroyed or errored before emitting 'end'.


Returns whether 'data' has been emitted.
Getter for the property encoding of a given Readable stream. The encoding
property can be set using the readable.setEncoding() method.
Becomes true when 'end' event is emitted.
Returns error if the stream has been destroyed with an error.
This property reflects the current state of a Readable stream as described
in the Three states section.
Returns the value of highWaterMark passed when creating this Readable.
This property contains the number of bytes (or objects) in the queue
ready to be read. The value provides introspection data regarding
the status of the highWaterMark.
Getter for the property objectMode of a given Readable stream.
The resume() has no effect if there is a 'readable' event listening.
Added in: v0.9.4
The readable.resume() method causes an explicitly paused Readable stream to
resume emitting 'data' events, switching the stream into flowing mode.
The readable.resume() method can be used to fully consume the data from a
stream without actually processing any of that data:
The readable.resume() method has no effect if there is a 'readable'
event listener.
The readable.setEncoding() method sets the character encoding for
data read from the Readable stream.
By default, no encoding is assigned and stream data will be returned as
Buffer objects. Setting an encoding causes the stream data
to be returned as strings of the specified encoding rather than as Buffer
objects. For instance, calling readable.setEncoding('utf8') will cause the
output data to be interpreted as UTF-8 data, and passed as strings. Calling
readable.setEncoding('hex') will cause the data to be encoded in hexadecimal
string format.
The Readable stream will properly handle multi-byte characters delivered
through the stream that would otherwise become improperly decoded if simply
pulled from the stream as Buffer objects.
The readable.unpipe() method detaches a Writable stream previously attached
using the stream.pipe() method.
If the destination is not specified, then all pipes are detached.
If the destination is specified, but no pipe is set up for it, then
the method does nothing.
The chunk argument can now be a Uint8Array instance.
Added in: v0.9.11
Passing chunk as null signals the end of the stream (EOF) and behaves the
same as readable.push(null), after which no more data can be written. The EOF
signal is put at the end of the buffer and any buffered data will still be
flushed.
The readable.unshift() method pushes a chunk of data back into the internal
buffer. This is useful in certain situations where a stream is being consumed by
code that needs to "un-consume" some amount of data that it has optimistically
pulled out of the source, so that the data can be passed on to some other party.
The stream.unshift(chunk) method cannot be called after the 'end' event
has been emitted or a runtime error will be thrown.
Developers using stream.unshift() often should consider switching to
use of a Transform stream instead. See the API for stream implementers
section for more information.
Unlike stream.push(chunk), stream.unshift(chunk) will not
end the reading process by resetting the internal reading state of the stream.
This can cause unexpected results if readable.unshift() is called during a
read (i.e. from within a stream._read() implementation on a
custom stream). Following the call to readable.unshift() with an immediate
stream.push('') will reset the reading state appropriately,
however it is best to simply avoid calling readable.unshift() while in the
process of performing a read.
Prior to Node.js 0.10, streams did not implement the entire node:stream
module API as it is currently defined. (See Compatibility for more
information.)
When using an older Node.js library that emits 'data' events and has a
stream.pause() method that is advisory only, the
readable.wrap() method can be used to create a Readable stream that uses
the old stream as its data source.
It will rarely be necessary to use readable.wrap() but the method has been
provided as a convenience for interacting with older Node.js applications and
libraries.
Symbol.asyncIterator support is no longer experimental.
Added in: v10.0.0
If the loop terminates with a break, return, or a throw, the stream will
be destroyed. In other terms, iterating over a stream will consume the stream
fully. The stream will be read in chunks of size equal to the highWaterMark
option. In the code example above, data will be in a single chunk if the file
has less then 64 KiB of data because no highWaterMark option is provided to
fs.createReadStream().


Calls readable.destroy() with an AbortError and returns
a promise that fulfills when the stream is finished.


See stream.compose for more information.


The iterator created by this method gives users the option to cancel the
destruction of the stream if the for await...of loop is exited by return,
break, or throw, or if the iterator should destroy the stream if the stream
emitted an error during iteration.
added highWaterMark in options.
Added in: v17.4.0, v16.14.0


This method allows mapping over the stream. The fn function will be called
for every chunk in the stream. If the fn function returns a promise - that
promise will be awaited before being passed to the result stream.
added highWaterMark in options.
Added in: v17.4.0, v16.14.0


This method allows filtering the stream. For each chunk in the stream the fn
function will be called and if it returns a truthy value, the chunk will be
passed to the result stream. If the fn function returns a promise - that
promise will be awaited.


This method allows iterating a stream. For each chunk in the stream the
fn function will be called. If the fn function returns a promise - that
promise will be awaited.
This method is different from for await...of loops in that it can optionally
process chunks concurrently. In addition, a forEach iteration can only be
stopped by having passed a signal option and aborting the related
AbortController while for await...of can be stopped with break or
return. In either case the stream will be destroyed.
This method is different from listening to the 'data' event in that it
uses the readable event in the underlying machinary and can limit the
number of concurrent fn calls.


This method allows easily obtaining the contents of a stream.
As this method reads the entire stream into memory, it negates the benefits of
streams. It's intended for interoperability and convenience, not as the primary
way to consume streams.


This method is similar to Array.prototype.some and calls fn on each chunk
in the stream until the awaited return value is true (or any truthy value).
Once an fn call on a chunk awaited return value is truthy, the stream is
destroyed and the promise is fulfilled with true. If none of the fn
calls on the chunks return a truthy value, the promise is fulfilled with
false.


This method is similar to Array.prototype.find and calls fn on each chunk
in the stream to find a chunk with a truthy value for fn. Once an fn call's
awaited return value is truthy, the stream is destroyed and the promise is
fulfilled with value for which fn returned a truthy value. If all of the
fn calls on the chunks return a falsy value, the promise is fulfilled with
undefined.


This method is similar to Array.prototype.every and calls fn on each chunk
in the stream to check if all awaited return values are truthy value for fn.
Once an fn call on a chunk awaited return value is falsy, the stream is
destroyed and the promise is fulfilled with false. If all of the fn calls
on the chunks return a truthy value, the promise is fulfilled with true.


This method returns a new stream by applying the given callback to each
chunk of the stream and then flattening the result.
It is possible to return a stream or another iterable or async iterable from
fn and the result streams will be merged (flattened) into the returned
stream.


This method returns a new stream with the first limit chunks dropped.


This method returns a new stream with the first limit chunks.
Using the asIndexedPairs method emits a runtime warning that it will be removed in a future version.
Added in: v17.5.0, v16.15.0


This method returns a new stream with chunks of the underlying stream paired
with a counter in the form [index, chunk]. The first index value is 0 and it
increases by 1 for each chunk produced.


This method calls fn on each chunk of the stream in order, passing it the
result from the calculation on the previous element. It returns a promise for
the final value of the reduction.
If no initial value is supplied the first chunk of the stream is used as the
initial value. If the stream is empty, the promise is rejected with a
TypeError with the ERR_INVALID_ARGS code property.
The reducer function iterates the stream element-by-element which means that
there is no concurrency parameter or parallelism. To perform a reduce
concurrently, you can extract the async function to readable.map method.
Instances of Duplex now return true when checking instanceof stream.Writable.
Added in: v0.9.4
Duplex streams are streams that implement both the Readable and
Writable interfaces.
Examples of Duplex streams include:
If false then the stream will automatically end the writable side when the
readable side ends. Set initially by the allowHalfOpen constructor option,
which defaults to true.
This can be changed manually to change the half-open behavior of an existing
Duplex stream instance, but must be changed before the 'end' event is
emitted.
Transform streams are Duplex streams where the output is in some way
related to the input. Like all Duplex streams, Transform streams
implement both the Readable and Writable interfaces.
Examples of Transform streams include:
Work as a no-op on a stream that has already been destroyed.
Added in: v8.0.0
Destroy the stream, and optionally emit an 'error' event. After this call, the
transform stream would release any internal resources.
Implementors should not override this method, but instead implement
readable._destroy().
The default implementation of _destroy() for Transform also emit 'close'
unless emitClose is set in false.
Once destroy() has been called, any further calls will be a no-op and no
further errors except from _destroy() may be emitted as 'error'.
Added support for ReadableStream and WritableStream.
The signal option was added.
The finished(stream, cb) will wait for the 'close' event before invoking the callback. The implementation tries to detect legacy streams and only apply this behavior to streams which are expected to emit 'close'.
Emitting 'close' before 'end' on a Readable stream will cause an ERR_STREAM_PREMATURE_CLOSE error.
Callback will be invoked on streams which have already finished before the call to finished(stream, cb).
Added in: v10.0.0
A readable and/or writable stream/webstream.
options <Object>
callback <Function> A callback function that takes an optional error
argument.
Returns: <Function> A cleanup function which removes all registered
listeners.
A function to get notified when a stream is no longer readable, writable
or has experienced an error or a premature close event.
Especially useful in error handling scenarios where a stream is destroyed
prematurely (like an aborted HTTP request), and will not emit 'end'
or 'finish'.
The finished API provides promise version.
stream.finished() leaves dangling event listeners (in particular
'error', 'end', 'finish' and 'close') after callback has been
invoked. The reason for this is so that unexpected 'error' events (due to
incorrect stream implementations) do not cause unexpected crashes.
If this is unwanted behavior then the returned cleanup function needs to be
invoked in the callback:
Added support for webstreams.
Passing an invalid callback to the callback argument now throws ERR_INVALID_ARG_TYPE instead of ERR_INVALID_CALLBACK.
The pipeline(..., cb) will wait for the 'close' event before invoking the callback. The implementation tries to detect legacy streams and only apply this behavior to streams which are expected to emit 'close'.
Add support for async generators.
Added in: v10.0.0
A module method to pipe between streams and generators forwarding errors and
properly cleaning up and provide a callback when the pipeline is complete.
The pipeline API provides a promise version.
stream.pipeline() will call stream.destroy(err) on all streams except:
stream.pipeline() leaves dangling event listeners on the streams
after the callback has been invoked. In the case of reuse of streams after
failure, this can cause event listener leaks and swallowed errors. If the last
stream is readable, dangling event listeners will be removed so that the last
stream can be consumed later.
stream.pipeline() closes all the streams when an error is raised.
The IncomingRequest usage with pipeline could lead to an unexpected behavior
once it would destroy the socket without sending the expected response.
See the example below:
Added support for stream class.
Added support for webstreams.
Added in: v16.9.0


Combines two or more streams into a Duplex stream that writes to the
first stream and reads from the last. Each provided stream is piped into
the next, using stream.pipeline. If any of the streams error then all
are destroyed, including the outer Duplex stream.
Because stream.compose returns a new stream that in turn can (and
should) be piped into other streams, it enables composition. In contrast,
when passing streams to stream.pipeline, typically the first stream is
a readable stream and the last a writable stream, forming a closed
circuit.
If passed a Function it must be a factory method taking a source
Iterable.
stream.compose can be used to convert async iterables, generators and
functions into streams.
See readable.compose(stream) for stream.compose as operator.
A utility method for creating readable streams out of iterators.
Calling Readable.from(string) or Readable.from(buffer) will not have
the strings or buffers be iterated to match the other streams semantics
for performance reasons.
If an Iterable object containing promises is passed as an argument,
it might result in unhandled rejection.




Returns whether the stream has been read from or cancelled.


Returns whether the stream has encountered an error.


Returns whether the stream is readable.






The src argument can now be a ReadableStream or WritableStream.
Added in: v16.8.0
A utility method for creating duplex streams.
If an Iterable object containing promises is passed as an argument,
it might result in unhandled rejection.




Added support for ReadableStream and WritableStream.
Added in: v15.4.0
A stream to attach a signal to.
Attaches an AbortSignal to a readable or writeable stream. This lets code
control stream destruction using an AbortController.
Calling abort on the AbortController corresponding to the passed
AbortSignal will behave the same way as calling .destroy(new AbortError())
on the stream, and controller.error(new AbortError()) for webstreams.
Or using an AbortSignal with a readable stream as an async iterable:
Or using an AbortSignal with a ReadableStream:
Returns the default highWaterMark used by streams.
Defaults to 16384 (16 KiB), or 16 for objectMode.
Sets the default highWaterMark used by streams.
The node:stream module API has been designed to make it possible to easily
implement streams using JavaScript's prototypal inheritance model.
First, a stream developer would declare a new JavaScript class that extends one
of the four basic stream classes (stream.Writable, stream.Readable,
stream.Duplex, or stream.Transform), making sure they call the appropriate
parent class constructor:
When extending streams, keep in mind what options the user
can and should provide before forwarding these to the base constructor. For
example, if the implementation makes assumptions in regard to the
autoDestroy and emitClose options, do not allow the
user to override these. Be explicit about what
options are forwarded instead of implicitly forwarding all options.
The new stream class must then implement one or more specific methods, depending
on the type of stream being created, as detailed in the chart below:
The implementation code for a stream should never call the "public" methods
of a stream that are intended for use by consumers (as described in the
API for stream consumers section). Doing so may lead to adverse side effects
in application code consuming the stream.
Avoid overriding public methods such as write(), end(), cork(),
uncork(), read() and destroy(), or emitting internal events such
as 'error', 'data', 'end', 'finish' and 'close' through .emit().
Doing so can break current and future stream invariants leading to behavior
and/or compatibility issues with other streams, stream utilities, and user
expectations.
For many simple cases, it is possible to create a stream without relying on
inheritance. This can be accomplished by directly creating instances of the
stream.Writable, stream.Readable, stream.Duplex, or stream.Transform
objects and passing appropriate methods as constructor options.
The stream.Writable class is extended to implement a Writable stream.
Custom Writable streams must call the new stream.Writable([options])
constructor and implement the writable._write() and/or writable._writev()
method.
support passing in an AbortSignal.
Change autoDestroy option default to true.
Add autoDestroy option to automatically destroy() the stream when it emits 'finish' or errors.
Add emitClose option to specify if 'close' is emitted on destroy.
Or, when using pre-ES6 style constructors:
Or, using the simplified constructor approach:
Calling abort on the AbortController corresponding to the passed
AbortSignal will behave the same way as calling .destroy(new AbortError())
on the writeable stream.
The _construct() method MUST NOT be called directly. It may be implemented
by child classes, and if so, will be called by the internal Writable
class methods only.
This optional function will be called in a tick after the stream constructor
has returned, delaying any _write(), _final() and _destroy() calls until
callback is called. This is useful to initialize state or asynchronously
initialize resources before the stream can be used.
_write() is optional when providing _writev().
All Writable stream implementations must provide a
writable._write() and/or
writable._writev() method to send data to the underlying
resource.
Transform streams provide their own implementation of the
writable._write().
This function MUST NOT be called by application code directly. It should be
implemented by child classes, and called by the internal Writable class
methods only.
The callback function must be called synchronously inside of
writable._write() or asynchronously (i.e. different tick) to signal either
that the write completed successfully or failed with an error.
The first argument passed to the callback must be the Error object if the
call failed or null if the write succeeded.
All calls to writable.write() that occur between the time writable._write()
is called and the callback is called will cause the written data to be
buffered. When the callback is invoked, the stream might emit a 'drain'
event. If a stream implementation is capable of processing multiple chunks of
data at once, the writable._writev() method should be implemented.
If the decodeStrings property is explicitly set to false in the constructor
options, then chunk will remain the same object that is passed to .write(),
and may be a string rather than a Buffer. This is to support implementations
that have an optimized handling for certain string data encodings. In that case,
the encoding argument will indicate the character encoding of the string.
Otherwise, the encoding argument can be safely ignored.
The writable._write() method is prefixed with an underscore because it is
internal to the class that defines it, and should never be called directly by
user programs.
This function MUST NOT be called by application code directly. It should be
implemented by child classes, and called by the internal Writable class
methods only.
The writable._writev() method may be implemented in addition or alternatively
to writable._write() in stream implementations that are capable of processing
multiple chunks of data at once. If implemented and if there is buffered data
from previous writes, _writev() will be called instead of _write().
The writable._writev() method is prefixed with an underscore because it is
internal to the class that defines it, and should never be called directly by
user programs.
The _destroy() method is called by writable.destroy().
It can be overridden by child classes but it must not be called directly.
Furthermore, the callback should not be mixed with async/await
once it is executed when a promise is resolved.
The _final() method must not be called directly. It may be implemented
by child classes, and if so, will be called by the internal Writable
class methods only.
This optional function will be called before the stream closes, delaying the
'finish' event until callback is called. This is useful to close resources
or write buffered data before a stream ends.
Errors occurring during the processing of the writable._write(),
writable._writev() and writable._final() methods must be propagated
by invoking the callback and passing the error as the first argument.
Throwing an Error from within these methods or manually emitting an 'error'
event results in undefined behavior.
If a Readable stream pipes into a Writable stream when Writable emits an
error, the Readable stream will be unpiped.
The following illustrates a rather simplistic (and somewhat pointless) custom
Writable stream implementation. While this specific Writable stream instance
is not of any real particular usefulness, the example illustrates each of the
required elements of a custom Writable stream instance:
Decoding buffers is a common task, for instance, when using transformers whose
input is a string. This is not a trivial process when using multi-byte
characters encoding, such as UTF-8. The following example shows how to decode
multi-byte strings using StringDecoder and Writable.
The stream.Readable class is extended to implement a Readable stream.
Custom Readable streams must call the new stream.Readable([options])
constructor and implement the readable._read() method.
support passing in an AbortSignal.
Change autoDestroy option default to true.
Add autoDestroy option to automatically destroy() the stream when it emits 'end' or errors.
Or, when using pre-ES6 style constructors:
Or, using the simplified constructor approach:
Calling abort on the AbortController corresponding to the passed
AbortSignal will behave the same way as calling .destroy(new AbortError())
on the readable created.
The _construct() method MUST NOT be called directly. It may be implemented
by child classes, and if so, will be called by the internal Readable
class methods only.
This optional function will be scheduled in the next tick by the stream
constructor, delaying any _read() and _destroy() calls until callback is
called. This is useful to initialize state or asynchronously initialize
resources before the stream can be used.
This function MUST NOT be called by application code directly. It should be
implemented by child classes, and called by the internal Readable class
methods only.
All Readable stream implementations must provide an implementation of the
readable._read() method to fetch data from the underlying resource.
When readable._read() is called, if data is available from the resource,
the implementation should begin pushing that data into the read queue using the
this.push(dataChunk) method. _read() will be called again
after each call to this.push(dataChunk) once the stream is
ready to accept more data. _read() may continue reading from the resource and
pushing data until readable.push() returns false. Only when _read() is
called again after it has stopped should it resume pushing additional data into
the queue.
Once the readable._read() method has been called, it will not be called
again until more data is pushed through the readable.push()
method. Empty data such as empty buffers and strings will not cause
readable._read() to be called.
The size argument is advisory. For implementations where a "read" is a
single operation that returns data can use the size argument to determine how
much data to fetch. Other implementations may ignore this argument and simply
provide data whenever it becomes available. There is no need to "wait" until
size bytes are available before calling stream.push(chunk).
The readable._read() method is prefixed with an underscore because it is
internal to the class that defines it, and should never be called directly by
user programs.
The _destroy() method is called by readable.destroy().
It can be overridden by child classes but it must not be called directly.
The chunk argument can now be a Uint8Array instance.
When chunk is a Buffer, Uint8Array, or string, the chunk of data will
be added to the internal queue for users of the stream to consume.
Passing chunk as null signals the end of the stream (EOF), after which no
more data can be written.
When the Readable is operating in paused mode, the data added with
readable.push() can be read out by calling the
readable.read() method when the 'readable' event is
emitted.
When the Readable is operating in flowing mode, the data added with
readable.push() will be delivered by emitting a 'data' event.
The readable.push() method is designed to be as flexible as possible. For
example, when wrapping a lower-level source that provides some form of
pause/resume mechanism, and a data callback, the low-level source can be wrapped
by the custom Readable instance:
The readable.push() method is used to push the content
into the internal buffer. It can be driven by the readable._read() method.
For streams not operating in object mode, if the chunk parameter of
readable.push() is undefined, it will be treated as empty string or
buffer. See readable.push('') for more information.
Errors occurring during processing of the readable._read() must be
propagated through the readable.destroy(err) method.
Throwing an Error from within readable._read() or manually emitting an
'error' event results in undefined behavior.
The following is a basic example of a Readable stream that emits the numerals
from 1 to 1,000,000 in ascending order, and then ends.
A Duplex stream is one that implements both Readable and
Writable, such as a TCP socket connection.
Because JavaScript does not have support for multiple inheritance, the
stream.Duplex class is extended to implement a Duplex stream (as opposed
to extending the stream.Readable and stream.Writable classes).
The stream.Duplex class prototypically inherits from stream.Readable and
parasitically from stream.Writable, but instanceof will work properly for
both base classes due to overriding Symbol.hasInstance on
stream.Writable.
Custom Duplex streams must call the new stream.Duplex([options])
constructor and implement both the readable._read() and
writable._write() methods.
The readableHighWaterMark and writableHighWaterMark options are supported now.
Or, when using pre-ES6 style constructors:
Or, using the simplified constructor approach:
When using pipeline:
The following illustrates a simple example of a Duplex stream that wraps a
hypothetical lower-level source object to which data can be written, and
from which data can be read, albeit using an API that is not compatible with
Node.js streams.
The following illustrates a simple example of a Duplex stream that buffers
incoming written data via the Writable interface that is read back out
via the Readable interface.
The most important aspect of a Duplex stream is that the Readable and
Writable sides operate independently of one another despite co-existing within
a single object instance.
For Duplex streams, objectMode can be set exclusively for either the
Readable or Writable side using the readableObjectMode and
writableObjectMode options respectively.
In the following example, for instance, a new Transform stream (which is a
type of Duplex stream) is created that has an object mode Writable side
that accepts JavaScript numbers that are converted to hexadecimal strings on
the Readable side.
A Transform stream is a Duplex stream where the output is computed
in some way from the input. Examples include zlib streams or crypto
streams that compress, encrypt, or decrypt data.
There is no requirement that the output be the same size as the input, the same
number of chunks, or arrive at the same time. For example, a Hash stream will
only ever have a single chunk of output which is provided when the input is
ended. A zlib stream will produce output that is either much smaller or much
larger than its input.
The stream.Transform class is extended to implement a Transform stream.
The stream.Transform class prototypically inherits from stream.Duplex and
implements its own versions of the writable._write() and
readable._read() methods. Custom Transform implementations must
implement the transform._transform() method and may
also implement the transform._flush() method.
Care must be taken when using Transform streams in that data written to the
stream can cause the Writable side of the stream to become paused if the
output on the Readable side is not consumed.
Or, when using pre-ES6 style constructors:
Or, using the simplified constructor approach:
The 'end' event is from the stream.Readable class. The 'end' event is
emitted after all data has been output, which occurs after the callback in
transform._flush() has been called. In the case of an error,
'end' should not be emitted.
The 'finish' event is from the stream.Writable class. The 'finish'
event is emitted after stream.end() is called and all chunks
have been processed by stream._transform(). In the case
of an error, 'finish' should not be emitted.
This function MUST NOT be called by application code directly. It should be
implemented by child classes, and called by the internal Readable class
methods only.
In some cases, a transform operation may need to emit an additional bit of
data at the end of the stream. For example, a zlib compression stream will
store an amount of internal state used to optimally compress the output. When
the stream ends, however, that additional data needs to be flushed so that the
compressed data will be complete.
Custom Transform implementations may implement the transform._flush()
method. This will be called when there is no more written data to be consumed,
but before the 'end' event is emitted signaling the end of the
Readable stream.
Within the transform._flush() implementation, the transform.push() method
may be called zero or more times, as appropriate. The callback function must
be called when the flush operation is complete.
The transform._flush() method is prefixed with an underscore because it is
internal to the class that defines it, and should never be called directly by
user programs.
This function MUST NOT be called by application code directly. It should be
implemented by child classes, and called by the internal Readable class
methods only.
All Transform stream implementations must provide a _transform()
method to accept input and produce output. The transform._transform()
implementation handles the bytes being written, computes an output, then passes
that output off to the readable portion using the transform.push() method.
The transform.push() method may be called zero or more times to generate
output from a single input chunk, depending on how much is to be output
as a result of the chunk.
It is possible that no output is generated from any given chunk of input data.
The callback function must be called only when the current chunk is completely
consumed. The first argument passed to the callback must be an Error object
if an error occurred while processing the input or null otherwise. If a second
argument is passed to the callback, it will be forwarded on to the
transform.push() method, but only if the first argument is falsy. In other
words, the following are equivalent:
The transform._transform() method is prefixed with an underscore because it
is internal to the class that defines it, and should never be called directly by
user programs.
transform._transform() is never called in parallel; streams implement a
queue mechanism, and to receive the next chunk, callback must be
called, either synchronously or asynchronously.
The stream.PassThrough class is a trivial implementation of a Transform
stream that simply passes the input bytes across to the output. Its purpose is
primarily for examples and testing, but there are some use cases where
stream.PassThrough is useful as a building block for novel sorts of streams.
With the support of async generators and iterators in JavaScript, async
generators are effectively a first-class language-level stream construct at
this point.
Some common interop cases of using Node.js streams with async generators
and async iterators are provided below.
Async iterators register a permanent error handler on the stream to prevent any
unhandled post-destroy errors.
A Node.js readable stream can be created from an asynchronous generator using
the Readable.from() utility method:
When writing to a writable stream from an async iterator, ensure correct
handling of backpressure and errors. stream.pipeline() abstracts away
the handling of backpressure and backpressure-related errors:
Prior to Node.js 0.10, the Readable stream interface was simpler, but also
less powerful and less useful.
In Node.js 0.10, the Readable class was added. For backward
compatibility with older Node.js programs, Readable streams switch into
"flowing mode" when a 'data' event handler is added, or when the
stream.resume() method is called. The effect is that, even
when not using the new stream.read() method and
'readable' event, it is no longer necessary to worry about losing
'data' chunks.
While most applications will continue to function normally, this introduces an
edge case in the following conditions:
For example, consider the following code:
Prior to Node.js 0.10, the incoming message data would be simply discarded.
However, in Node.js 0.10 and beyond, the socket remains paused forever.
The workaround in this situation is to call the
stream.resume() method to begin the flow of data:
In addition to new Readable streams switching into flowing mode,
pre-0.10 style streams can be wrapped in a Readable class using the
readable.wrap() method.
There are some cases where it is necessary to trigger a refresh of the
underlying readable stream mechanisms, without actually consuming any
data. In such cases, it is possible to call readable.read(0), which will
always return null.
If the internal read buffer is below the highWaterMark, and the
stream is not currently reading, then calling stream.read(0) will trigger
a low-level stream._read() call.
While most applications will almost never need to do this, there are
situations within Node.js where this is done, particularly in the
Readable stream class internals.
Use of readable.push('') is not recommended.
Pushing a zero-byte string, Buffer, or Uint8Array to a stream that is not in
object mode has an interesting side effect. Because it is a call to
readable.push(), the call will end the reading process.
However, because the argument is an empty string, no data is added to the
readable buffer so there is nothing for a user to consume.
The use of readable.setEncoding() will change the behavior of how the
highWaterMark operates in non-object mode.
Typically, the size of the current buffer is measured against the
highWaterMark in bytes. However, after setEncoding() is called, the
comparison function will begin to measure the buffer's size in characters.
This is not a problem in common cases with latin1 or ascii. But it is
advised to be mindful about this behavior when working with strings that could
contain multi-byte characters.


Source Code: lib/string_decoder.js
The node:string_decoder module provides an API for decoding Buffer objects
into strings in a manner that preserves encoded multi-byte UTF-8 and UTF-16
characters. It can be accessed using:
The following example shows the basic use of the StringDecoder class.
When a Buffer instance is written to the StringDecoder instance, an
internal buffer is used to ensure that the decoded string does not contain
any incomplete multibyte characters. These are held in the buffer until the
next call to stringDecoder.write() or until stringDecoder.end() is called.
In the following example, the three UTF-8 encoded bytes of the European Euro
symbol (‚Ç¨) are written over three separate operations:
Creates a new StringDecoder instance.
Returns any remaining input stored in the internal buffer as a string. Bytes
representing incomplete UTF-8 and UTF-16 characters will be replaced with
substitution characters appropriate for the character encoding.
If the buffer argument is provided, one final call to stringDecoder.write()
is performed before returning the remaining input.
After end() is called, the stringDecoder object can be reused for new input.
Each invalid character is now replaced by a single replacement character instead of one for each individual byte.
Added in: v0.1.99
Returns a decoded string, ensuring that any incomplete multibyte characters at
the end of the Buffer, or TypedArray, or DataView are omitted from the
returned string and stored in an internal buffer for the next call to
stringDecoder.write() or stringDecoder.end().
The test runner is now stable.
Added in: v18.0.0, v16.17.0


Source Code: lib/test.js
The node:test module facilitates the creation of JavaScript tests.
To access it:
This module is only available under the node: scheme. The following will not
work:
Tests created via the test module consist of a single function that is
processed in one of three ways:
The following example illustrates how tests are written using the
test module.
If any tests fail, the process exit code is set to 1.
The test context's test() method allows subtests to be created. This method
behaves identically to the top level test() function. The following example
demonstrates the creation of a top level test with two subtests.
In this example, await is used to ensure that both subtests have completed.
This is necessary because parent tests do not wait for their subtests to
complete. Any subtests that are still outstanding when their parent finishes
are cancelled and treated as failures. Any subtest failures cause the parent
test to fail.
Individual tests can be skipped by passing the skip option to the test, or by
calling the test context's skip() method as shown in the
following example.
Running tests can also be done using describe to declare a suite
and it to declare a test.
A suite is used to organize and group related tests together.
it is a shorthand for test().
describe and it are imported from the node:test module.
If Node.js is started with the --test-only command-line option, it is
possible to skip all top level tests except for a selected subset by passing
the only option to the tests that should be run. When a test with the only
option set is run, all subtests are also run. The test context's runOnly()
method can be used to implement the same behavior at the subtest level.
The --test-name-pattern command-line option can be used to only run tests
whose name matches the provided pattern. Test name patterns are interpreted as
JavaScript regular expressions. The --test-name-pattern option can be
specified multiple times in order to run nested tests. For each test that is
executed, any corresponding test hooks, such as beforeEach(), are also
run.
Given the following test file, starting Node.js with the
--test-name-pattern="test [1-3]" option would cause the test runner to execute
test 1, test 2, and test 3. If test 1 did not match the test name
pattern, then its subtests would not execute, despite matching the pattern. The
same set of tests could also be executed by passing --test-name-pattern
multiple times (e.g. --test-name-pattern="test 1",
--test-name-pattern="test 2", etc.).
Test name patterns can also be specified using regular expression literals. This
allows regular expression flags to be used. In the previous example, starting
Node.js with --test-name-pattern="/test [4-5]/i" would match Test 4 and
Test 5 because the pattern is case-insensitive.
Test name patterns do not change the set of files that the test runner executes.
Once a test function finishes executing, the results are reported as quickly
as possible while maintaining the order of the tests. However, it is possible
for the test function to generate asynchronous activity that outlives the test
itself. The test runner handles this type of activity, but does not delay the
reporting of test results in order to accommodate it.
In the following example, a test completes with two setImmediate()
operations still outstanding. The first setImmediate() attempts to create a
new subtest. Because the parent test has already finished and output its
results, the new subtest is immediately marked as failed, and reported later
to the <TestsStream>.
The second setImmediate() creates an uncaughtException event.
uncaughtException and unhandledRejection events originating from a completed
test are marked as failed by the test module and reported as diagnostic
warnings at the top level by the <TestsStream>.


The Node.js test runner supports running in watch mode by passing the --watch flag:
In watch mode, the test runner will watch for changes to test files and
their dependencies. When a change is detected, the test runner will
rerun the tests affected by the change.
The test runner will continue to run until the process is terminated.
The Node.js test runner can be invoked from the command line by passing the
--test flag:
By default, Node.js will recursively search the current directory for
JavaScript source files matching a specific naming convention. Matching files
are executed as test files. More information on the expected test file naming
convention and behavior can be found in the test runner execution model
section.
Alternatively, one or more paths can be provided as the final argument(s) to
the Node.js command, as shown below.
In this example, the test runner will execute the files test1.js and
test2.mjs. The test runner will also recursively search the
custom_test_dir/ directory for test files to execute.
When searching for test files to execute, the test runner behaves as follows:
Each matching test file is executed in a separate child process. The maximum
number of child processes running at any time is controlled by the
--test-concurrency flag. If the child process finishes with an exit code
of 0, the test is considered passing. Otherwise, the test is considered to be a
failure. Test files must be executable by Node.js, but are not required to use
the node:test module internally.
Each test file is executed as if it was a regular script. That is, if the test
file itself uses node:test to define tests, all of those tests will be
executed within a single application thread, regardless of the value of the
concurrency option of test().


When Node.js is started with the --experimental-test-coverage
command-line flag, code coverage is collected and statistics are reported once
all tests have completed. If the NODE_V8_COVERAGE environment variable is
used to specify a code coverage directory, the generated V8 coverage files are
written to that directory. Node.js core modules and files within
node_modules/ directories are not included in the coverage report. If
coverage is enabled, the coverage report is sent to any test reporters via
the 'test:coverage' event.
Coverage can be disabled on a series of lines using the following
comment syntax:
Coverage can also be disabled for a specified number of lines. After the
specified number of lines, coverage will be automatically reenabled. If the
number of lines is not explicitly provided, a single line is ignored.
The tap and spec reporters will print a summary of the coverage statistics.
There is also an lcov reporter that will generate an lcov file which can be
used as an in depth coverage report.
The test runner's code coverage functionality has the following limitations,
which will be addressed in a future Node.js release:
The node:test module supports mocking during testing via a top-level mock
object. The following example creates a spy on a function that adds two numbers
together. The spy is then used to assert that the function was called as
expected.
The same mocking functionality is also exposed on the TestContext object
of each test. The following example creates a spy on an object method using the
API exposed on the TestContext. The benefit of mocking via the test context is
that the test runner will automatically restore all mocked functionality once
the test finishes.
Mocking timers is a technique commonly used in software testing to simulate and
control the behavior of timers, such as setInterval and setTimeout,
without actually waiting for the specified time intervals.
Refer to the MockTimers class for a full list of methods and features.
This allows developers to write more reliable and
predictable tests for time-dependent functionality.
The example below shows how to mock setTimeout.
Using .enable({ apis: ['setTimeout'] });
it will mock the setTimeout functions in the node:timers and
node:timers/promises modules,
as well as from the Node.js global context.
Note: Destructuring functions such as
import { setTimeout } from 'node:timers'
is currently not supported by this API.
The same mocking functionality is also exposed in the mock property on the TestContext object
of each test. The benefit of mocking via the test context is
that the test runner will automatically restore all mocked timers
functionality once the test finishes.
The mock timers API also allows the mocking of the Date object. This is a
useful feature for testing time-dependent functionality, or to simulate
internal calendar functions such as Date.now().
The dates implementation is also part of the MockTimers class. Refer to it
for a full list of methods and features.
Note: Dates and timers are dependent when mocked together. This means that
if you have both the Date and setTimeout mocked, advancing the time will
also advance the mocked date as they simulate a single internal clock.
The example below show how to mock the Date object and obtain the current
Date.now() value.
If there is no initial epoch set, the initial date will be based on 0 in the
Unix epoch. This is January 1st, 1970, 00:00:00 UTC. You can set an initial date
by passing a now property to the .enable() method. This value will be used
as the initial date for the mocked Date object. It can either be a positive
integer, or another Date object.
You can use the .setTime() method to manually move the mocked date to another
time. This method only accepts a positive integer.
Note: This method will execute any mocked timers that are in the past
from the new time.
In the below example we are setting a new time for the mocked date.
If you have any timer that's set to run in the past, it will be executed as if
the .tick() method has been called. This is useful if you want to test
time-dependent functionality that's already in the past.
Using .runAll() will execute all timers that are currently in the queue. This
will also advance the mocked date to the time of the last timer that was
executed as if the time has passed.
Reporters are now exposed at node:test/reporters.
Added in: v19.6.0, v18.15.0
The node:test module supports passing --test-reporter
flags for the test runner to use a specific reporter.
The following built-reporters are supported:
tap
The tap reporter outputs the test results in the TAP format.
spec
The spec reporter outputs the test results in a human-readable format.
dot
The dot reporter outputs the test results in a compact format,
where each passing test is represented by a .,
and each failing test is represented by a X.
junit
The junit reporter outputs test results in a jUnit XML format
lcov
The lcov reporter outputs test coverage when used with the
--experimental-test-coverage flag.
When stdout is a TTY, the spec reporter is used by default.
Otherwise, the tap reporter is used by default.
The exact output of these reporters is subject to change between versions of
Node.js, and should not be relied on programmatically. If programmatic access
to the test runner's output is required, use the events emitted by the
<TestsStream>.
The reporters are available via the node:test/reporters module:
--test-reporter can be used to specify a path to custom reporter.
A custom reporter is a module that exports a value
accepted by stream.compose.
Reporters should transform events emitted by a <TestsStream>
Example of a custom reporter using <stream.Transform>:
Example of a custom reporter using a generator function:
The value provided to --test-reporter should be a string like one used in an
import() in JavaScript code, or a value provided for --import.
The --test-reporter flag can be specified multiple times to report test
results in several formats. In this situation
it is required to specify a destination for each reporter
using --test-reporter-destination.
Destination can be stdout, stderr, or a file path.
Reporters and destinations are paired according
to the order they were specified.
In the following example, the spec reporter will output to stdout,
and the dot reporter will output to file.txt:
When a single reporter is specified, the destination will default to stdout,
unless a destination is explicitly provided.
Add a testNamePatterns option.
Added in: v18.9.0, v16.19.0
Note: shard is used to horizontally parallelize test running across
machines or processes, ideal for large-scale executions across varied
environments. It's incompatible with watch mode, tailored for rapid
code iteration by automatically rerunning tests on file changes.
Added the skip, todo, and only shorthands.
Add a signal option.
Add a timeout option.
Added in: v18.0.0, v16.17.0
The test() function is the value imported from the test module. Each
invocation of this function results in reporting the test to the <TestsStream>.
The TestContext object passed to the fn argument can be used to perform
actions related to the current test. Examples include skipping the test, adding
additional diagnostic information, or creating subtests.
test() returns a Promise that fulfills once the test completes.
if test() is called within a describe() block, it fulfills immediately.
The return value can usually be discarded for top level tests.
However, the return value from subtests should be used to prevent the parent
test from finishing first and cancelling the subtest
as shown in the following example.
The timeout option can be used to fail the test if it takes longer than
timeout milliseconds to complete. However, it is not a reliable mechanism for
canceling tests because a running test might block the application thread and
thus prevent the scheduled cancellation.
Shorthand for skipping a test,
same as test([name], { skip: true }[, fn]).
Shorthand for marking a test as TODO,
same as test([name], { todo: true }[, fn]).
Shorthand for marking a test as only,
same as test([name], { only: true }[, fn]).
The describe() function imported from the node:test module. Each
invocation of this function results in the creation of a Subtest.
After invocation of top level describe functions,
all top level tests and suites will execute.
Shorthand for skipping a suite, same as describe([name], { skip: true }[, fn]).
Shorthand for marking a suite as TODO, same as
describe([name], { todo: true }[, fn]).
Shorthand for marking a suite as only, same as
describe([name], { only: true }[, fn]).
Calling it() is now equivalent to calling test().
Added in: v18.6.0, v16.17.0
Shorthand for test().
The it() function is imported from the node:test module.
Shorthand for skipping a test,
same as it([name], { skip: true }[, fn]).
Shorthand for marking a test as TODO,
same as it([name], { todo: true }[, fn]).
Shorthand for marking a test as only,
same as it([name], { only: true }[, fn]).
This function is used to create a hook running before running a suite.
This function is used to create a hook running after  running a suite.
This function is used to create a hook running
before each subtest of the current suite.
This function is used to create a hook running
after each subtest of the current test.
The MockFunctionContext class is used to inspect or manipulate the behavior of
mocks created via the MockTracker APIs.
A getter that returns a copy of the internal array used to track calls to the
mock. Each entry in the array is an object with the following properties.
This function returns the number of times that this mock has been invoked. This
function is more efficient than checking ctx.calls.length because ctx.calls
is a getter that creates a copy of the internal call tracking array.
This function is used to change the behavior of an existing mock.
The following example creates a mock function using t.mock.fn(), calls the
mock function, and then changes the mock implementation to a different function.
This function is used to change the behavior of an existing mock for a single
invocation. Once invocation onCall has occurred, the mock will revert to
whatever behavior it would have used had mockImplementationOnce() not been
called.
The following example creates a mock function using t.mock.fn(), calls the
mock function, changes the mock implementation to a different function for the
next invocation, and then resumes its previous behavior.
Resets the call history of the mock function.
Resets the implementation of the mock function to its original behavior. The
mock can still be used after calling this function.
The MockTracker class is used to manage mocking functionality. The test runner
module provides a top level mock export which is a MockTracker instance.
Each test also provides its own MockTracker instance via the test context's
mock property.
This function is used to create a mock function.
The following example creates a mock function that increments a counter by one
on each invocation. The times option is used to modify the mock behavior such
that the first two invocations add two to the counter instead of one.
This function is syntax sugar for MockTracker.method with options.getter
set to true.
This function is used to create a mock on an existing object method. The
following example demonstrates how a mock is created on an existing object
method.
This function restores the default behavior of all mocks that were previously
created by this MockTracker and disassociates the mocks from the
MockTracker instance. Once disassociated, the mocks can still be used, but the
MockTracker instance can no longer be used to reset their behavior or
otherwise interact with them.
After each test completes, this function is called on the test context's
MockTracker. If the global MockTracker is used extensively, calling this
function manually is recommended.
This function restores the default behavior of all mocks that were previously
created by this MockTracker. Unlike mock.reset(), mock.restoreAll() does
not disassociate the mocks from the MockTracker instance.
This function is syntax sugar for MockTracker.method with options.setter
set to true.


Mocking timers is a technique commonly used in software testing to simulate and
control the behavior of timers, such as setInterval and setTimeout,
without actually waiting for the specified time intervals.
MockTimers is also able to mock the Date object.
The MockTracker provides a top-level timers export
which is a MockTimers instance.
Updated parameters to be an option object with available APIs and the default initial epoch.
Added in: v20.4.0
Enables timer mocking for the specified timers.
Note: When you enable mocking for a specific timer, its associated
clear function will also be implicitly mocked.
Note: Mocking Date will affect the behavior of the mocked timers
as they use the same internal clock.
Example usage without setting initial time:
The above example enables mocking for the setInterval timer and
implicitly mocks the clearInterval function. Only the setInterval
and clearInterval functions from node:timers,
node:timers/promises, and
globalThis will be mocked.
Example usage with initial time set
Example usage with initial Date object as time set
Alternatively, if you call mock.timers.enable() without any parameters:
All timers ('setInterval', 'clearInterval', 'setTimeout', and 'clearTimeout')
will be mocked. The setInterval, clearInterval, setTimeout, and clearTimeout
functions from node:timers, node:timers/promises,
and globalThis will be mocked. As well as the global Date object.
This function restores the default behavior of all mocks that were previously
created by this  MockTimers instance and disassociates the mocks
from the  MockTracker instance.
Note: After each test completes, this function is called on
the test context's  MockTracker.
Calls timers.reset().
Advances time for all mocked timers.
Note: This diverges from how setTimeout in Node.js behaves and accepts
only positive numbers. In Node.js, setTimeout with negative numbers is
only supported for web compatibility reasons.
The following example mocks a setTimeout function and
by using .tick advances in
time triggering all pending timers.
Alternativelly, the .tick function can be called many times
Advancing time using .tick will also advance the time for any Date object
created after the mock was enabled (if Date was also set to be mocked).
As mentioned, all clear functions from timers (clearTimeout and clearInterval)
are implicity mocked. Take a look at this example using setTimeout:
Once you enable mocking timers, node:timers,
node:timers/promises modules,
and timers from the Node.js global context are enabled:
Note: Destructuring functions such as
import { setTimeout } from 'node:timers' is currently
not supported by this API.
In Node.js, setInterval from node:timers/promises
is an AsyncGenerator and is also supported by this API:
Triggers all pending mocked timers immediately. If the Date object is also
mocked, it will also advance the Date object to the furthest timer's time.
The example below triggers all pending timers immediately,
causing them to execute without any delay.
Note: The runAll() function is specifically designed for
triggering timers in the context of timer mocking.
It does not have any effect on real-time system
clocks or actual timers outside of the mocking environment.
Sets the current Unix timestamp that will be used as reference for any mocked
Date objects.
Dates and timer objects are dependent on each other. If you use setTime() to
pass the current time to the mocked Date object, the set timers with
setTimeout and setInterval will not be affected.
However, the tick method will advanced the mocked Date object.
added type to test:pass and test:fail events for when the test is a suite.
Added in: v18.9.0, v16.19.0
A successful call to run() method will return a new <TestsStream>
object, streaming a series of events representing the execution of the tests.
TestsStream will emit events, in the order of the tests definition
Emitted when code coverage is enabled and all tests have completed.
Emitted when a test is dequeued, right before it is executed.
Emitted when context.diagnostic is called.
Emitted when a test is enqueued for execution.
Emitted when a test fails.
Emitted when a test passes.
Emitted when all subtests have completed for a given test.
Emitted when a test starts reporting its own and its subtests status.
This event is guaranteed to be emitted in the same order as the tests are
defined.
Emitted when a running test writes to stderr.
This event is only emitted if --test flag is passed.
Emitted when a running test writes to stdout.
This event is only emitted if --test flag is passed.
Emitted when no more tests are queued for execution in watch mode.
The before function was added to TestContext.
Added in: v18.0.0, v16.17.0
An instance of TestContext is passed to each test function in order to
interact with the test runner. However, the TestContext constructor is not
exposed as part of the API.
This function is used to create a hook running before
subtest of the current test.
This function is used to create a hook running
before each subtest of the current test.
This function is used to create a hook that runs after the current test
finishes.
This function is used to create a hook running
after each subtest of the current test.
This function is used to write diagnostics to the output. Any diagnostic
information is included at the end of the test's results. This function does
not return a value.
The name of the test.
If shouldRunOnlyTests is truthy, the test context will only run tests that
have the only option set. Otherwise, all tests are run. If Node.js was not
started with the --test-only command-line option, this function is a
no-op.
This function causes the test's output to indicate the test as skipped. If
message is provided, it is included in the output. Calling skip() does
not terminate execution of the test function. This function does not return a
value.
This function adds a TODO directive to the test's output. If message is
provided, it is included in the output. Calling todo() does not terminate
execution of the test function. This function does not return a value.
Add a signal option.
Add a timeout option.
Added in: v18.0.0, v16.17.0
This function is used to create subtests under the current test. This function
behaves in the same fashion as the top level test() function.
An instance of SuiteContext is passed to each suite function in order to
interact with the test runner. However, the SuiteContext constructor is not
exposed as part of the API.
The name of the suite.


Source Code: lib/timers.js
The timer module exposes a global API for scheduling functions to
be called at some future period of time. Because the timer functions are
globals, there is no need to call require('node:timers') to use the API.
The timer functions within Node.js implement a similar API as the timers API
provided by Web Browsers but use a different internal implementation that is
built around the Node.js Event Loop.
This object is created internally and is returned from setImmediate(). It
can be passed to clearImmediate() in order to cancel the scheduled
actions.
By default, when an immediate is scheduled, the Node.js event loop will continue
running as long as the immediate is active. The Immediate object returned by
setImmediate() exports both immediate.ref() and immediate.unref()
functions that can be used to control this default behavior.
If true, the Immediate object will keep the Node.js event loop active.
When called, requests that the Node.js event loop not exit so long as the
Immediate is active. Calling immediate.ref() multiple times will have no
effect.
By default, all Immediate objects are "ref'ed", making it normally unnecessary
to call immediate.ref() unless immediate.unref() had been called previously.
When called, the active Immediate object will not require the Node.js event
loop to remain active. If there is no other activity keeping the event loop
running, the process may exit before the Immediate object's callback is
invoked. Calling immediate.unref() multiple times will have no effect.


Cancels the immediate. This is similar to calling clearImmediate().
This object is created internally and is returned from setTimeout() and
setInterval(). It can be passed to either clearTimeout() or
clearInterval() in order to cancel the scheduled actions.
By default, when a timer is scheduled using either setTimeout() or
setInterval(), the Node.js event loop will continue running as long as the
timer is active. Each of the Timeout objects returned by these functions
export both timeout.ref() and timeout.unref() functions that can be used to
control this default behavior.


Cancels the timeout.
If true, the Timeout object will keep the Node.js event loop active.
When called, requests that the Node.js event loop not exit so long as the
Timeout is active. Calling timeout.ref() multiple times will have no effect.
By default, all Timeout objects are "ref'ed", making it normally unnecessary
to call timeout.ref() unless timeout.unref() had been called previously.
Sets the timer's start time to the current time, and reschedules the timer to
call its callback at the previously specified duration adjusted to the current
time. This is useful for refreshing a timer without allocating a new
JavaScript object.
Using this on a timer that has already called its callback will reactivate the
timer.
When called, the active Timeout object will not require the Node.js event loop
to remain active. If there is no other activity keeping the event loop running,
the process may exit before the Timeout object's callback is invoked. Calling
timeout.unref() multiple times will have no effect.
Coerce a Timeout to a primitive. The primitive can be used to
clear the Timeout. The primitive can only be used in the
same thread where the timeout was created. Therefore, to use it
across worker_threads it must first be passed to the correct
thread. This allows enhanced compatibility with browser
setTimeout() and setInterval() implementations.


Cancels the timeout.
A timer in Node.js is an internal construct that calls a given function after
a certain period of time. When a timer's function is called varies depending on
which method was used to create the timer and what other work the Node.js
event loop is doing.
Passing an invalid callback to the callback argument now throws ERR_INVALID_ARG_TYPE instead of ERR_INVALID_CALLBACK.
Added in: v0.9.1
Schedules the "immediate" execution of the callback after I/O events'
callbacks.
When multiple calls to setImmediate() are made, the callback functions are
queued for execution in the order in which they are created. The entire callback
queue is processed every event loop iteration. If an immediate timer is queued
from inside an executing callback, that timer will not be triggered until the
next event loop iteration.
If callback is not a function, a TypeError will be thrown.
This method has a custom variant for promises that is available using
timersPromises.setImmediate().
Passing an invalid callback to the callback argument now throws ERR_INVALID_ARG_TYPE instead of ERR_INVALID_CALLBACK.
Added in: v0.0.1
Schedules repeated execution of callback every delay milliseconds.
When delay is larger than 2147483647 or less than 1, the delay will be
set to 1. Non-integer delays are truncated to an integer.
If callback is not a function, a TypeError will be thrown.
This method has a custom variant for promises that is available using
timersPromises.setInterval().
Passing an invalid callback to the callback argument now throws ERR_INVALID_ARG_TYPE instead of ERR_INVALID_CALLBACK.
Added in: v0.0.1
Schedules execution of a one-time callback after delay milliseconds.
The callback will likely not be invoked in precisely delay milliseconds.
Node.js makes no guarantees about the exact timing of when callbacks will fire,
nor of their ordering. The callback will be called as close as possible to the
time specified.
When delay is larger than 2147483647 or less than 1, the delay
will be set to 1. Non-integer delays are truncated to an integer.
If callback is not a function, a TypeError will be thrown.
This method has a custom variant for promises that is available using
timersPromises.setTimeout().
The setImmediate(), setInterval(), and setTimeout() methods
each return objects that represent the scheduled timers. These can be used to
cancel the timer and prevent it from triggering.
For the promisified variants of setImmediate() and setTimeout(),
an AbortController may be used to cancel the timer. When canceled, the
returned Promises will be rejected with an 'AbortError'.
For setImmediate():
For setTimeout():
Cancels an Immediate object created by setImmediate().
Cancels a Timeout object created by setInterval().
Cancels a Timeout object created by setTimeout().
Graduated from experimental.
Added in: v15.0.0
The timers/promises API provides an alternative set of timer functions
that return Promise objects. The API is accessible via
require('node:timers/promises').
Returns an async iterator that generates values in an interval of delay ms.
If ref is true, you need to call next() of async iterator explicitly
or implicitly to keep the event loop alive.


An experimental API defined by the Scheduling APIs draft specification
being developed as a standard Web Platform API.
Calling timersPromises.scheduler.wait(delay, options) is roughly equivalent
to calling timersPromises.setTimeout(delay, undefined, options) except that
the ref option is not supported.


An experimental API defined by the Scheduling APIs draft specification
being developed as a standard Web Platform API.
Calling timersPromises.scheduler.yield() is equivalent to calling
timersPromises.setImmediate() with no arguments.


Source Code: lib/tls.js
The node:tls module provides an implementation of the Transport Layer Security
(TLS) and Secure Socket Layer (SSL) protocols that is built on top of OpenSSL.
The module can be accessed using:
It is possible for Node.js to be built without including support for the
node:crypto module. In such cases, attempting to import from tls or
calling require('node:tls') will result in an error being thrown.
When using CommonJS, the error thrown can be caught using try/catch:
When using the lexical ESM import keyword, the error can only be
caught if a handler for process.on('uncaughtException') is registered
before any attempt to load the module is made (using, for instance,
a preload module).
When using ESM, if there is a chance that the code may be run on a build
of Node.js where crypto support is not enabled, consider using the
import() function instead of the lexical import keyword:
TLS/SSL is a set of protocols that rely on a public key infrastructure (PKI) to
enable secure communication between a client and a server. For most common
cases, each server must have a private key.
Private keys can be generated in multiple ways. The example below illustrates
use of the OpenSSL command-line interface to generate a 2048-bit RSA private
key:
With TLS/SSL, all servers (and some clients) must have a certificate.
Certificates are public keys that correspond to a private key, and that are
digitally signed either by a Certificate Authority or by the owner of the
private key (such certificates are referred to as "self-signed"). The first
step to obtaining a certificate is to create a Certificate Signing Request
(CSR) file.
The OpenSSL command-line interface can be used to generate a CSR for a private
key:
Once the CSR file is generated, it can either be sent to a Certificate
Authority for signing or used to generate a self-signed certificate.
Creating a self-signed certificate using the OpenSSL command-line interface
is illustrated in the example below:
Once the certificate is generated, it can be used to generate a .pfx or
.p12 file:
Where:
The term forward secrecy or perfect forward secrecy describes a feature
of key-agreement (i.e., key-exchange) methods. That is, the server and client
keys are used to negotiate new temporary keys that are used specifically and
only for the current communication session. Practically, this means that even
if the server's private key is compromised, communication can only be decrypted
by eavesdroppers if the attacker manages to obtain the key-pair specifically
generated for the session.
Perfect forward secrecy is achieved by randomly generating a key pair for
key-agreement on every TLS/SSL handshake (in contrast to using the same key for
all sessions). Methods implementing this technique are called "ephemeral".
Currently two methods are commonly used to achieve perfect forward secrecy (note
the character "E" appended to the traditional abbreviations):
Perfect forward secrecy using ECDHE is enabled by default. The ecdhCurve
option can be used when creating a TLS server to customize the list of supported
ECDH curves to use. See tls.createServer() for more info.
DHE is disabled by default but can be enabled alongside ECDHE by setting the
dhparam option to 'auto'. Custom DHE parameters are also supported but
discouraged in favor of automatically selected, well-known parameters.
Perfect forward secrecy was optional up to TLSv1.2. As of TLSv1.3, (EC)DHE is
always used (with the exception of PSK-only connections).
ALPN (Application-Layer Protocol Negotiation Extension) and
SNI (Server Name Indication) are TLS handshake extensions:
TLS-PSK support is available as an alternative to normal certificate-based
authentication. It uses a pre-shared key instead of certificates to
authenticate a TLS connection, providing mutual authentication.
TLS-PSK and public key infrastructure are not mutually exclusive. Clients and
servers can accommodate both, choosing either of them during the normal cipher
negotiation step.
TLS-PSK is only a good choice where means exist to securely share a
key with every connecting machine, so it does not replace the public key
infrastructure (PKI) for the majority of TLS uses.
The TLS-PSK implementation in OpenSSL has seen many security flaws in
recent years, mostly because it is used only by a minority of applications.
Please consider all alternative solutions before switching to PSK ciphers.
Upon generating PSK it is of critical importance to use sufficient entropy as
discussed in RFC 4086. Deriving a shared secret from a password or other
low-entropy sources is not secure.
PSK ciphers are disabled by default, and using TLS-PSK thus requires explicitly
specifying a cipher suite with the ciphers option. The list of available
ciphers can be retrieved via openssl ciphers -v 'PSK'. All TLS 1.3
ciphers are eligible for PSK and can be retrieved via
openssl ciphers -v -s -tls1_3 -psk.
According to the RFC 4279, PSK identities up to 128 bytes in length and
PSKs up to 64 bytes in length must be supported. As of OpenSSL 1.1.0
maximum identity size is 128 bytes, and maximum PSK length is 256 bytes.
The current implementation doesn't support asynchronous PSK callbacks due to the
limitations of the underlying OpenSSL API.
The TLS protocol allows clients to renegotiate certain aspects of the TLS
session. Unfortunately, session renegotiation requires a disproportionate amount
of server-side resources, making it a potential vector for denial-of-service
attacks.
To mitigate the risk, renegotiation is limited to three times every ten minutes.
An 'error' event is emitted on the tls.TLSSocket instance when this
threshold is exceeded. The limits are configurable:
The default renegotiation limits should not be modified without a full
understanding of the implications and risks.
TLSv1.3 does not support renegotiation.
Establishing a TLS session can be relatively slow. The process can be sped
up by saving and later reusing the session state. There are several mechanisms
to do so, discussed here from oldest to newest (and preferred).
Servers generate a unique ID for new connections and
send it to the client. Clients and servers save the session state. When
reconnecting, clients send the ID of their saved session state and if the server
also has the state for that ID, it can agree to use it. Otherwise, the server
will create a new session. See RFC 2246 for more information, page 23 and
30.
Resumption using session identifiers is supported by most web browsers when
making HTTPS requests.
For Node.js, clients wait for the 'session' event to get the session data,
and provide the data to the session option of a subsequent tls.connect()
to reuse the session. Servers must
implement handlers for the 'newSession' and 'resumeSession' events
to save and restore the session data using the session ID as the lookup key to
reuse sessions. To reuse sessions across load balancers or cluster workers,
servers must use a shared session cache (such as Redis) in their session
handlers.
The servers encrypt the entire session state and send it
to the client as a "ticket". When reconnecting, the state is sent to the server
in the initial connection. This mechanism avoids the need for a server-side
session cache. If the server doesn't use the ticket, for any reason (failure
to decrypt it, it's too old, etc.), it will create a new session and send a new
ticket. See RFC 5077 for more information.
Resumption using session tickets is becoming commonly supported by many web
browsers when making HTTPS requests.
For Node.js, clients use the same APIs for resumption with session identifiers
as for resumption with session tickets. For debugging, if
tls.TLSSocket.getTLSTicket() returns a value, the session data contains a
ticket, otherwise it contains client-side session state.
With TLSv1.3, be aware that multiple tickets may be sent by the server,
resulting in multiple 'session' events, see 'session' for more
information.
Single process servers need no specific implementation to use session tickets.
To use session tickets across server restarts or load balancers, servers must
all have the same ticket keys. There are three 16-byte keys internally, but the
tls API exposes them as a single 48-byte buffer for convenience.
It's possible to get the ticket keys by calling server.getTicketKeys() on
one server instance and then distribute them, but it is more reasonable to
securely generate 48 bytes of secure random data and set them with the
ticketKeys option of tls.createServer(). The keys should be regularly
regenerated and server's keys can be reset with
server.setTicketKeys().
Session ticket keys are cryptographic keys, and they must be stored
securely. With TLS 1.2 and below, if they are compromised all sessions that
used tickets encrypted with them can be decrypted. They should not be stored
on disk, and they should be regenerated regularly.
If clients advertise support for tickets, the server will send them. The
server can disable tickets by supplying
require('node:constants').SSL_OP_NO_TICKET in secureOptions.
Both session identifiers and session tickets timeout, causing the server to
create new sessions. The timeout can be configured with the sessionTimeout
option of tls.createServer().
For all the mechanisms, when resumption fails, servers will create new sessions.
Since failing to resume the session does not cause TLS/HTTPS connection
failures, it is easy to not notice unnecessarily poor TLS performance. The
OpenSSL CLI can be used to verify that servers are resuming sessions. Use the
-reconnect option to openssl s_client, for example:
Read through the debug output. The first connection should say "New", for
example:
Subsequent connections should say "Reused", for example:
Node.js is built with a default suite of enabled and disabled TLS ciphers. This
default cipher list can be configured when building Node.js to allow
distributions to provide their own default list.
The following command can be used to show the default cipher suite:
This default can be replaced entirely using the --tls-cipher-list
command-line switch (directly, or via the NODE_OPTIONS environment
variable). For instance, the following makes ECDHE-RSA-AES128-GCM-SHA256:!RC4
the default TLS cipher suite:
To verify, use the following command to show the set cipher list, note the
difference between defaultCoreCipherList and defaultCipherList:
i.e. the defaultCoreCipherList list is set at compilation time and the
defaultCipherList is set at runtime.
To modify the default cipher suites from within the runtime, modify the
tls.DEFAULT_CIPHERS variable, this must be performed before listening on any
sockets, it will not affect sockets already opened. For example:
The default can also be replaced on a per client or server basis using the
ciphers option from tls.createSecureContext(), which is also available
in tls.createServer(), tls.connect(), and when creating new
tls.TLSSockets.
The ciphers list can contain a mixture of TLSv1.3 cipher suite names, the ones
that start with 'TLS_', and specifications for TLSv1.2 and below cipher
suites. The TLSv1.2 ciphers support a legacy specification format, consult
the OpenSSL cipher list format documentation for details, but those
specifications do not apply to TLSv1.3 ciphers. The TLSv1.3 suites can only
be enabled by including their full name in the cipher list. They cannot, for
example, be enabled or disabled by using the legacy TLSv1.2 'EECDH' or
'!EECDH' specification.
Despite the relative order of TLSv1.3 and TLSv1.2 cipher suites, the TLSv1.3
protocol is significantly more secure than TLSv1.2, and will always be chosen
over TLSv1.2 if the handshake indicates it is supported, and if any TLSv1.3
cipher suites are enabled.
The default cipher suite included within Node.js has been carefully
selected to reflect current security best practices and risk mitigation.
Changing the default cipher suite can have a significant impact on the security
of an application. The --tls-cipher-list switch and ciphers option should by
used only if absolutely necessary.
The default cipher suite prefers GCM ciphers for Chrome's 'modern
cryptography' setting and also prefers ECDHE and DHE ciphers for perfect
forward secrecy, while offering some backward compatibility.
Old clients that rely on insecure and deprecated RC4 or DES-based ciphers
(like Internet Explorer 6) cannot complete the handshaking process with
the default configuration. If these clients must be supported, the
TLS recommendations may offer a compatible cipher suite. For more details
on the format, see the OpenSSL cipher list format documentation.
There are only five TLSv1.3 cipher suites:
The first three are enabled by default. The two CCM-based suites are supported
by TLSv1.3 because they may be more performant on constrained systems, but they
are not enabled by default since they offer less security.
Multiple functions can fail due to certificate errors that are reported by
OpenSSL. In such a case, the function provides an <Error> via its callback that
has the property code which can take one of the following values:


The tls.CryptoStream class represents a stream of encrypted data. This class
is deprecated and should no longer be used.
The cryptoStream.bytesWritten property returns the total number of bytes
written to the underlying socket including the bytes required for the
implementation of the TLS protocol.


Returned by tls.createSecurePair().
The 'secure' event is emitted by the SecurePair object once a secure
connection has been established.
As with checking for the server
'secureConnection'
event, pair.cleartext.authorized should be inspected to confirm whether the
certificate used is properly authorized.
Accepts encrypted connections using TLS or SSL.
This event is emitted when a new TCP stream is established, before the TLS
handshake begins. socket is typically an object of type net.Socket but
will not receive events unlike the socket created from the net.Server
'connection' event. Usually users will not want to access this event.
This event can also be explicitly emitted by users to inject connections
into the TLS server. In that case, any Duplex stream can be passed.
The keylog event is emitted when key material is generated or received by
a connection to this server (typically before handshake has completed, but not
necessarily). This keying material can be stored for debugging, as it allows
captured TLS traffic to be decrypted. It may be emitted multiple times for
each socket.
A typical use case is to append received lines to a common text file, which
is later used by software (such as Wireshark) to decrypt the traffic:
The callback argument is now supported.
Added in: v0.9.2
The 'newSession' event is emitted upon creation of a new TLS session. This may
be used to store sessions in external storage. The data should be provided to
the 'resumeSession' callback.
The listener callback is passed three arguments when called:
Listening for this event will have an effect only on connections established
after the addition of the event listener.
The 'OCSPRequest' event is emitted when the client sends a certificate status
request. The listener callback is passed three arguments when called:
The server's current certificate can be parsed to obtain the OCSP URL
and certificate ID; after obtaining an OCSP response, callback(null, resp) is
then invoked, where resp is a Buffer instance containing the OCSP response.
Both certificate and issuer are Buffer DER-representations of the
primary and issuer's certificates. These can be used to obtain the OCSP
certificate ID and OCSP endpoint URL.
Alternatively, callback(null, null) may be called, indicating that there was
no OCSP response.
Calling callback(err) will result in a socket.destroy(err) call.
The typical flow of an OCSP request is as follows:
The issuer can be null if the certificate is either self-signed or the
issuer is not in the root certificates list. (An issuer may be provided
via the ca option when establishing the TLS connection.)
Listening for this event will have an effect only on connections established
after the addition of the event listener.
An npm module like asn1.js may be used to parse the certificates.
The 'resumeSession' event is emitted when the client requests to resume a
previous TLS session. The listener callback is passed two arguments when
called:
The event listener should perform a lookup in external storage for the
sessionData saved by the 'newSession' event handler using the given
sessionId. If found, call callback(null, sessionData) to resume the session.
If not found, the session cannot be resumed. callback() must be called
without sessionData so that the handshake can continue and a new session can
be created. It is possible to call callback(err) to terminate the incoming
connection and destroy the socket.
Listening for this event will have an effect only on connections established
after the addition of the event listener.
The following illustrates resuming a TLS session:
The 'secureConnection' event is emitted after the handshaking process for a
new connection has successfully completed. The listener callback is passed a
single argument when called:
The tlsSocket.authorized property is a boolean indicating whether the
client has been verified by one of the supplied Certificate Authorities for the
server. If tlsSocket.authorized is false, then socket.authorizationError
is set to describe how authorization failed. Depending on the settings
of the TLS server, unauthorized connections may still be accepted.
The tlsSocket.alpnProtocol property is a string that contains the selected
ALPN protocol. When ALPN has no selected protocol because the client or the
server did not send an ALPN extension, tlsSocket.alpnProtocol equals false.
The tlsSocket.servername property is a string containing the server name
requested via SNI.
The 'tlsClientError' event is emitted when an error occurs before a secure
connection is established. The listener callback is passed two arguments when
called:
The server.addContext() method adds a secure context that will be used if
the client request's SNI name matches the supplied hostname (or wildcard).
When there are multiple matching contexts, the most recently added one is
used.
Returns the bound address, the address family name, and port of the
server as reported by the operating system. See net.Server.address() for
more information.
The server.close() method stops the server from accepting new connections.
This function operates asynchronously. The 'close' event will be emitted
when the server has no more open connections.
Returns the session ticket keys.
See Session Resumption for more information.
Starts the server listening for encrypted connections.
This method is identical to server.listen() from net.Server.
The server.setSecureContext() method replaces the secure context of an
existing server. Existing connections to the server are not interrupted.
Sets the session ticket keys.
Changes to the ticket keys are effective only for future server connections.
Existing or currently pending server connections will use the previous keys.
See Session Resumption for more information.
Performs transparent encryption of written data and all required TLS
negotiation.
Instances of tls.TLSSocket implement the duplex Stream interface.
Methods that return TLS connection metadata (e.g.
tls.TLSSocket.getPeerCertificate()) will only return data while the
connection is open.
The enableTrace option is now supported.
ALPN options are supported now.
Added in: v0.11.4
Construct a new tls.TLSSocket object from an existing TCP socket.
The keylog event is emitted on a tls.TLSSocket when key material
is generated or received by the socket. This keying material can be stored
for debugging, as it allows captured TLS traffic to be decrypted. It may
be emitted multiple times, before or after the handshake completes.
A typical use case is to append received lines to a common text file, which
is later used by software (such as Wireshark) to decrypt the traffic:
The 'OCSPResponse' event is emitted if the requestOCSP option was set
when the tls.TLSSocket was created and an OCSP response has been received.
The listener callback is passed a single argument when called:
Typically, the response is a digitally signed object from the server's CA that
contains information about server's certificate revocation status.
The 'secureConnect' event is emitted after the handshaking process for a new
connection has successfully completed. The listener callback will be called
regardless of whether or not the server's certificate has been authorized. It
is the client's responsibility to check the tlsSocket.authorized property to
determine if the server certificate was signed by one of the specified CAs. If
tlsSocket.authorized === false, then the error can be found by examining the
tlsSocket.authorizationError property. If ALPN was used, the
tlsSocket.alpnProtocol property can be checked to determine the negotiated
protocol.
The 'secureConnect' event is not emitted when a <tls.TLSSocket> is created
using the new tls.TLSSocket() constructor.
The 'session' event is emitted on a client tls.TLSSocket when a new session
or TLS ticket is available. This may or may not be before the handshake is
complete, depending on the TLS protocol version that was negotiated. The event
is not emitted on the server, or if a new session was not created, for example,
when the connection was resumed. For some TLS protocol versions the event may be
emitted multiple times, in which case all the sessions can be used for
resumption.
On the client, the session can be provided to the session option of
tls.connect() to resume the connection.
See Session Resumption for more information.
For TLSv1.2 and below, tls.TLSSocket.getSession() can be called once
the handshake is complete. For TLSv1.3, only ticket-based resumption is allowed
by the protocol, multiple tickets are sent, and the tickets aren't sent until
after the handshake completes. So it is necessary to wait for the
'session' event to get a resumable session. Applications
should use the 'session' event instead of getSession() to ensure
they will work for all TLS versions. Applications that only expect to
get or use one session should listen for this event only once:
The family property now returns a string instead of a number.
The family property now returns a number instead of a string.
Added in: v0.11.4
Returns the bound address, the address family name, and port of the
underlying socket as reported by the operating system:
{ port: 12346, family: 'IPv4', address: '127.0.0.1' }.
Returns the reason why the peer's certificate was not been verified. This
property is set only when tlsSocket.authorized === false.
This property is true if the peer certificate was signed by one of the CAs
specified when creating the tls.TLSSocket instance, otherwise false.
Disables TLS renegotiation for this TLSSocket instance. Once called, attempts
to renegotiate will trigger an 'error' event on the TLSSocket.
When enabled, TLS packet trace information is written to stderr. This can be
used to debug TLS connection problems.
The format of the output is identical to the output of
openssl s_client -trace or openssl s_server -trace. While it is produced by
OpenSSL's SSL_trace() function, the format is undocumented, can change
without notice, and should not be relied on.
Always returns true. This may be used to distinguish TLS sockets from regular
net.Socket instances.
length <number> number of bytes to retrieve from keying material
label <string> an application specific label, typically this will be a
value from the
IANA Exporter Label Registry.
context <Buffer> Optionally provide a context.
Returns: <Buffer> requested bytes of the keying material
Keying material is used for validations to prevent different kind of attacks in
network protocols, for example in the specifications of IEEE 802.1X.
Example
See the OpenSSL SSL_export_keying_material documentation for more
information.
Returns an object representing the local certificate. The returned object has
some properties corresponding to the fields of the certificate.
See tls.TLSSocket.getPeerCertificate() for an example of the certificate
structure.
If there is no local certificate, an empty object will be returned. If the
socket has been destroyed, null will be returned.
Return the IETF cipher name as standardName.
Return the minimum cipher version, instead of a fixed string ('TLSv1/SSLv3').
Added in: v0.11.4
Returns an object containing information on the negotiated cipher suite.
For example, a TLSv1.2 protocol with AES256-SHA cipher:
See
SSL_CIPHER_get_name
for more information.
Returns an object representing the type, name, and size of parameter of
an ephemeral key exchange in perfect forward secrecy on a client
connection. It returns an empty object when the key exchange is not
ephemeral. As this is only supported on a client socket; null is returned
if called on a server socket. The supported types are 'DH' and 'ECDH'. The
name property is available only when type is 'ECDH'.
For example: { type: 'ECDH', name: 'prime256v1', size: 256 }.
As the Finished messages are message digests of the complete handshake
(with a total of 192 bits for TLS 1.0 and more for SSL 3.0), they can
be used for external authentication procedures when the authentication
provided by SSL/TLS is not desired or is not enough.
Corresponds to the SSL_get_finished routine in OpenSSL and may be used
to implement the tls-unique channel binding from RFC 5929.
Returns an object representing the peer's certificate. If the peer does not
provide a certificate, an empty object will be returned. If the socket has been
destroyed, null will be returned.
If the full certificate chain was requested, each certificate will include an
issuerCertificate property containing an object representing its issuer's
certificate.
Add "ca" property.
Add fingerprint512.
Support Elliptic Curve public key info.
A certificate object has properties corresponding to the fields of the
certificate.
The certificate may contain information about the public key, depending on
the key type.
For RSA keys, the following properties may be defined:
For EC keys, the following properties may be defined:
Example certificate:
As the Finished messages are message digests of the complete handshake
(with a total of 192 bits for TLS 1.0 and more for SSL 3.0), they can
be used for external authentication procedures when the authentication
provided by SSL/TLS is not desired or is not enough.
Corresponds to the SSL_get_peer_finished routine in OpenSSL and may be used
to implement the tls-unique channel binding from RFC 5929.
Returns the peer certificate as an <X509Certificate> object.
If there is no peer certificate, or the socket has been destroyed,
undefined will be returned.
Returns a string containing the negotiated SSL/TLS protocol version of the
current connection. The value 'unknown' will be returned for connected
sockets that have not completed the handshaking process. The value null will
be returned for server sockets or disconnected client sockets.
Protocol versions are:
See the OpenSSL SSL_get_version documentation for more information.
Returns the TLS session data or undefined if no session was
negotiated. On the client, the data can be provided to the session option of
tls.connect() to resume the connection. On the server, it may be useful
for debugging.
See Session Resumption for more information.
Note: getSession() works only for TLSv1.2 and below. For TLSv1.3, applications
must use the 'session' event (it also works for TLSv1.2 and below).
See
SSL_get_shared_sigalgs
for more information.
For a client, returns the TLS session ticket if one is available, or
undefined. For a server, always returns undefined.
It may be useful for debugging.
See Session Resumption for more information.
Returns the local certificate as an <X509Certificate> object.
If there is no local certificate, or the socket has been destroyed,
undefined will be returned.
See Session Resumption for more information.
Returns the string representation of the local IP address.
Returns the numeric representation of the local port.
Returns the string representation of the remote IP address. For example,
'74.125.127.100' or '2001:4860:a005::68'.
Returns the string representation of the remote IP family. 'IPv4' or 'IPv6'.
Returns the numeric representation of the remote port. For example, 443.
Passing an invalid callback to the callback argument now throws ERR_INVALID_ARG_TYPE instead of ERR_INVALID_CALLBACK.
Added in: v0.11.8
options <Object>
callback <Function> If renegotiate() returned true, callback is
attached once to the 'secure' event. If renegotiate() returned false,
callback will be called in the next tick with an error, unless the
tlsSocket has been destroyed, in which case callback will not be called
at all.
Returns: <boolean> true if renegotiation was initiated, false otherwise.
The tlsSocket.renegotiate() method initiates a TLS renegotiation process.
Upon completion, the callback function will be passed a single argument
that is either an Error (if the request failed) or null.
This method can be used to request a peer's certificate after the secure
connection has been established.
When running as the server, the socket will be destroyed with an error after
handshakeTimeout timeout.
For TLSv1.3, renegotiation cannot be initiated, it is not supported by the
protocol.
The tlsSocket.setMaxSendFragment() method sets the maximum TLS fragment size.
Returns true if setting the limit succeeded; false otherwise.
Smaller fragment sizes decrease the buffering latency on the client: larger
fragments are buffered by the TLS layer until the entire fragment is received
and its integrity is verified; large fragments can span multiple roundtrips
and their processing can be delayed due to packet loss or reordering. However,
smaller fragments add extra TLS framing bytes and CPU overhead, which may
decrease overall server throughput.
Support for uniformResourceIdentifier subject alternative names has been disabled in response to CVE-2021-44531.
Added in: v0.8.4
Verifies the certificate cert is issued to hostname.
Returns <Error> object, populating it with reason, host, and cert on
failure. On success, returns <undefined>.
This function is intended to be used in combination with the
checkServerIdentity option that can be passed to tls.connect() and as
such operates on a certificate object. For other purposes, consider using
x509.checkHost() instead.
This function can be overwritten by providing an alternative function as the
options.checkServerIdentity option that is passed to tls.connect(). The
overwriting function can call tls.checkServerIdentity() of course, to augment
the checks done with additional verification.
This function is only called if the certificate passed all other checks, such as
being issued by trusted CA (options.ca).
Earlier versions of Node.js incorrectly accepted certificates for a given
hostname if a matching uniformResourceIdentifier subject alternative name
was present (see CVE-2021-44531). Applications that wish to accept
uniformResourceIdentifier subject alternative names can use a custom
options.checkServerIdentity function that implements the desired behavior.
Added onread option.
The highWaterMark option is accepted now.
The pskCallback option is now supported.
Support the allowHalfOpen option.
The hints option is now supported.
The enableTrace option is now supported.
The timeout option is supported now.
The lookup option is supported now.
The ALPNProtocols option can be a TypedArray or DataView now.
ALPN options are supported now.
The secureContext option is supported now.
Added in: v0.11.3
enableTrace: See tls.createServer()
host <string> Host the client should connect to. Default:
'localhost'.
port <number> Port the client should connect to.
path <string> Creates Unix socket connection to path. If this option is
specified, host and port are ignored.
socket <stream.Duplex> Establish secure connection on a given socket
rather than creating a new socket. Typically, this is an instance of
net.Socket, but any Duplex stream is allowed.
If this option is specified, path, host, and port are ignored,
except for certificate validation. Usually, a socket is already connected
when passed to tls.connect(), but it can be connected later.
Connection/disconnection/destruction of socket is the user's
responsibility; calling tls.connect() will not cause net.connect() to be
called.
allowHalfOpen <boolean> If set to false, then the socket will
automatically end the writable side when the readable side ends. If the
socket option is set, this option has no effect. See the allowHalfOpen
option of net.Socket for details. Default: false.
rejectUnauthorized <boolean> If not false, the server certificate is
verified against the list of supplied CAs. An 'error' event is emitted if
verification fails; err.code contains the OpenSSL error code. Default:
true.
pskCallback <Function>
When negotiating TLS-PSK (pre-shared keys), this function is called
with optional identity hint provided by the server or null
in case of TLS 1.3 where hint was removed.
It will be necessary to provide a custom tls.checkServerIdentity()
for the connection as the default one will try to check host name/IP
of the server against the certificate but that's not applicable for PSK
because there won't be a certificate present.
More information can be found in the RFC 4279.
ALPNProtocols: <string[]> | <Buffer[]> | <TypedArray[]> | <DataView[]> | <Buffer> | <TypedArray> | <DataView>
An array of strings, Buffers, TypedArrays, or DataViews, or a
single Buffer, TypedArray, or DataView containing the supported ALPN
protocols. Buffers should have the format [len][name][len][name]...
e.g. '\x08http/1.1\x08http/1.0', where the len byte is the length of the
next protocol name. Passing an array is usually much simpler, e.g.
['http/1.1', 'http/1.0']. Protocols earlier in the list have higher
preference than those later.
servername: <string> Server name for the SNI (Server Name Indication) TLS
extension. It is the name of the host being connected to, and must be a host
name, and not an IP address. It can be used by a multi-homed server to
choose the correct certificate to present to the client, see the
SNICallback option to tls.createServer().
checkServerIdentity(servername, cert) <Function> A callback function
to be used (instead of the builtin tls.checkServerIdentity() function)
when checking the server's host name (or the provided servername when
explicitly set) against the certificate. This should return an <Error> if
verification fails. The method should return undefined if the servername
and cert are verified.
session <Buffer> A Buffer instance, containing TLS session.
minDHSize <number> Minimum size of the DH parameter in bits to accept a
TLS connection. When a server offers a DH parameter with a size less
than minDHSize, the TLS connection is destroyed and an error is thrown.
Default: 1024.
highWaterMark: <number> Consistent with the readable stream highWaterMark parameter.
Default: 16 * 1024.
secureContext: TLS context object created with
tls.createSecureContext(). If a secureContext is not provided, one
will be created by passing the entire options object to
tls.createSecureContext().
onread <Object> If the socket option is missing, incoming data is
stored in a single buffer and passed to the supplied callback when
data arrives on the socket, otherwise the option is ignored. See the
onread option of net.Socket for details.
...: tls.createSecureContext() options that are used if the
secureContext option is missing, otherwise they are ignored.
...: Any socket.connect() option not already listed.
The callback function, if specified, will be added as a listener for the
'secureConnect' event.
tls.connect() returns a tls.TLSSocket object.
Unlike the https API, tls.connect() does not enable the
SNI (Server Name Indication) extension by default, which may cause some
servers to return an incorrect certificate or reject the connection
altogether. To enable SNI, set the servername option in addition
to host.
The following illustrates a client for the echo server example from
tls.createServer():
Same as tls.connect() except that path can be provided
as an argument instead of an option.
A path option, if specified, will take precedence over the path argument.
Same as tls.connect() except that port and host can be provided
as arguments instead of options.
A port or host option, if specified, will take precedence over any port or host
argument.
The dhparam option can now be set to 'auto' to enable DHE with appropriate well-known parameters.
Added privateKeyIdentifier and privateKeyEngine options to get private key from an OpenSSL engine.
Added sigalgs option to override supported signature algorithms.
TLSv1.3 support added.
The ca: option now supports BEGIN TRUSTED CERTIFICATE.
The minVersion and maxVersion can be used to restrict the allowed TLS protocol versions.
The ecdhCurve cannot be set to false anymore due to a change in OpenSSL.
The options parameter can now include clientCertEngine.
The ecdhCurve option can now be multiple ':' separated curve names or 'auto'.
If the key option is an array, individual entries do not need a passphrase property anymore. Array entries can also just be strings or Buffers now.
The ca option can now be a single string containing multiple CA certificates.
Added in: v0.11.13
tls.createServer() sets the default value of the honorCipherOrder option
to true, other APIs that create secure contexts leave it unset.
tls.createServer() uses a 128 bit truncated SHA1 hash value generated
from process.argv as the default value of the sessionIdContext option, other
APIs that create secure contexts have no default value.
The tls.createSecureContext() method creates a SecureContext object. It is
usable as an argument to several tls APIs, such as server.addContext(),
but has no public methods. The tls.Server constructor and the
tls.createServer() method do not support the secureContext option.
A key is required for ciphers that use certificates. Either key or
pfx can be used to provide it.
If the ca option is not given, then Node.js will default to using
Mozilla's publicly trusted list of CAs.
Custom DHE parameters are discouraged in favor of the new dhparam: 'auto'
option. When set to 'auto', well-known DHE parameters of sufficient strength
will be selected automatically. Otherwise, if necessary, openssl dhparam can
be used to create custom parameters. The key length must be greater than or
equal to 1024 bits or else an error will be thrown. Although 1024 bits is
permissible, use 2048 bits or larger for stronger security.
ALPN options are supported now.
Deprecated since: v0.11.3
Added in: v0.3.2


Creates a new secure pair object with two streams, one of which reads and writes
the encrypted data and the other of which reads and writes the cleartext data.
Generally, the encrypted stream is piped to/from an incoming encrypted data
stream and the cleartext one is used as a replacement for the initial encrypted
stream.
tls.createSecurePair() returns a tls.SecurePair object with cleartext and
encrypted stream properties.
Using cleartext has the same API as tls.TLSSocket.
The tls.createSecurePair() method is now deprecated in favor of
tls.TLSSocket(). For example, the code:
can be replaced by:
where secureSocket has the same API as pair.cleartext.
The options parameter can now include ALPNCallback.
If ALPNProtocols is set, incoming connections that send an ALPN extension with no supported protocols are terminated with a fatal no_application_protocol alert.
The options parameter now supports net.createServer() options.
The options parameter can now include clientCertEngine.
The ALPNProtocols option can be a TypedArray or DataView now.
ALPN options are supported now.
Added in: v0.3.2
ALPNProtocols: <string[]> | <Buffer[]> | <TypedArray[]> | <DataView[]> | <Buffer> | <TypedArray> | <DataView>
An array of strings, Buffers, TypedArrays, or DataViews, or a single
Buffer, TypedArray, or DataView containing the supported ALPN
protocols. Buffers should have the format [len][name][len][name]...
e.g. 0x05hello0x05world, where the first byte is the length of the next
protocol name. Passing an array is usually much simpler, e.g.
['hello', 'world']. (Protocols should be ordered by their priority.)
ALPNCallback: <Function> If set, this will be called when a
client opens a connection using the ALPN extension. One argument will
be passed to the callback: an object containing servername and
protocols fields, respectively containing the server name from
the SNI extension (if any) and an array of ALPN protocol name strings. The
callback must return either one of the strings listed in
protocols, which will be returned to the client as the selected
ALPN protocol, or undefined, to reject the connection with a fatal alert.
If a string is returned that does not match one of the client's ALPN
protocols, an error will be thrown. This option cannot be used with the
ALPNProtocols option, and setting both options will throw an error.
clientCertEngine <string> Name of an OpenSSL engine which can provide the
client certificate.
enableTrace <boolean> If true, tls.TLSSocket.enableTrace() will be
called on new connections. Tracing can be enabled after the secure
connection is established, but this option must be used to trace the secure
connection setup. Default: false.
handshakeTimeout <number> Abort the connection if the SSL/TLS handshake
does not finish in the specified number of milliseconds.
A 'tlsClientError' is emitted on the tls.Server object whenever
a handshake times out. Default: 120000 (120 seconds).
rejectUnauthorized <boolean> If not false the server will reject any
connection which is not authorized with the list of supplied CAs. This
option only has an effect if requestCert is true. Default: true.
requestCert <boolean> If true the server will request a certificate from
clients that connect and attempt to verify that certificate. Default:
false.
sessionTimeout <number> The number of seconds after which a TLS session
created by the server will no longer be resumable. See
Session Resumption for more information. Default: 300.
SNICallback(servername, callback) <Function> A function that will be
called if the client supports SNI TLS extension. Two arguments will be
passed when called: servername and callback. callback is an
error-first callback that takes two optional arguments: error and ctx.
ctx, if provided, is a SecureContext instance.
tls.createSecureContext() can be used to get a proper SecureContext.
If callback is called with a falsy ctx argument, the default secure
context of the server will be used. If SNICallback wasn't provided the
default callback with high-level API will be used (see below).
ticketKeys: <Buffer> 48-bytes of cryptographically strong pseudorandom
data. See Session Resumption for more information.
pskCallback <Function>
When negotiating TLS-PSK (pre-shared keys), this function is called
with the identity provided by the client.
If the return value is null the negotiation process will stop and an
"unknown_psk_identity" alert message will be sent to the other party.
If the server wishes to hide the fact that the PSK identity was not known,
the callback must provide some random data as psk to make the connection
fail with "decrypt_error" before negotiation is finished.
PSK ciphers are disabled by default, and using TLS-PSK thus
requires explicitly specifying a cipher suite with the ciphers option.
More information can be found in the RFC 4279.
pskIdentityHint <string> optional hint to send to a client to help
with selecting the identity during TLS-PSK negotiation. Will be ignored
in TLS 1.3. Upon failing to set pskIdentityHint 'tlsClientError' will be
emitted with 'ERR_TLS_PSK_SET_IDENTIY_HINT_FAILED' code.
...: Any tls.createSecureContext() option can be provided. For
servers, the identity options (pfx, key/cert, or pskCallback)
are usually required.
...: Any net.createServer() option can be provided.
Creates a new tls.Server. The secureConnectionListener, if provided, is
automatically set as a listener for the 'secureConnection' event.
The ticketKeys options is automatically shared between node:cluster module
workers.
The following illustrates a simple echo server:
The server can be tested by connecting to it using the example client from
tls.connect().
Returns an array with the names of the supported TLS ciphers. The names are
lower-case for historical reasons, but must be uppercased to be used in
the ciphers option of tls.createSecureContext().
Not all supported ciphers are enabled by default. See
Modifying the default TLS cipher suite.
Cipher names that start with 'tls_' are for TLSv1.3, all the others are for
TLSv1.2 and below.
An immutable array of strings representing the root certificates (in PEM format)
from the bundled Mozilla CA store as supplied by the current Node.js version.
The bundled CA store, as supplied by Node.js, is a snapshot of Mozilla CA store
that is fixed at release time. It is identical on all supported platforms.
Default value changed to 'auto'.
Added in: v0.11.13
The default curve name to use for ECDH key agreement in a tls server. The
default value is 'auto'. See tls.createSecureContext() for further
information.


Source Code: lib/trace_events.js
The node:trace_events module provides a mechanism to centralize tracing
information generated by V8, Node.js core, and userspace code.
Tracing can be enabled with the --trace-event-categories command-line flag
or by using the node:trace_events module. The --trace-event-categories flag
accepts a list of comma-separated category names.
The available categories are:
By default the node, node.async_hooks, and v8 categories are enabled.
Prior versions of Node.js required the use of the --trace-events-enabled
flag to enable trace events. This requirement has been removed. However, the
--trace-events-enabled flag may still be used and will enable the
node, node.async_hooks, and v8 trace event categories by default.
Alternatively, trace events may be enabled using the node:trace_events module:
Running Node.js with tracing enabled will produce log files that can be opened
in the chrome://tracing
tab of Chrome.
The logging file is by default called node_trace.${rotation}.log, where
${rotation} is an incrementing log-rotation id. The filepath pattern can
be specified with --trace-event-file-pattern that accepts a template
string that supports ${rotation} and ${pid}:
To guarantee that the log file is properly generated after signal events like
SIGINT, SIGTERM, or SIGBREAK, make sure to have the appropriate handlers
in your code, such as:
The tracing system uses the same time source
as the one used by process.hrtime().
However the trace-event timestamps are expressed in microseconds,
unlike process.hrtime() which returns nanoseconds.
The features from this module are not available in Worker threads.
The Tracing object is used to enable or disable tracing for sets of
categories. Instances are created using the trace_events.createTracing()
method.
When created, the Tracing object is disabled. Calling the
tracing.enable() method adds the categories to the set of enabled trace event
categories. Calling tracing.disable() will remove the categories from the
set of enabled trace event categories.
A comma-separated list of the trace event categories covered by this
Tracing object.
Disables this Tracing object.
Only trace event categories not covered by other enabled Tracing objects
and not specified by the --trace-event-categories flag will be disabled.
Enables this Tracing object for the set of categories covered by the
Tracing object.
Creates and returns a Tracing object for the given set of categories.
Returns a comma-separated list of all currently-enabled trace event
categories. The current set of enabled trace event categories is determined
by the union of all currently-enabled Tracing objects and any categories
enabled using the --trace-event-categories flag.
Given the file test.js below, the command
node --trace-event-categories node.perf test.js will print
'node.async_hooks,node.perf' to the console.


Source Code: lib/tty.js
The node:tty module provides the tty.ReadStream and tty.WriteStream
classes. In most cases, it will not be necessary or possible to use this module
directly. However, it can be accessed using:
When Node.js detects that it is being run with a text terminal ("TTY")
attached, process.stdin will, by default, be initialized as an instance of
tty.ReadStream and both process.stdout and process.stderr will, by
default, be instances of tty.WriteStream. The preferred method of determining
whether Node.js is being run within a TTY context is to check that the value of
the process.stdout.isTTY property is true:
In most cases, there should be little to no reason for an application to
manually create instances of the tty.ReadStream and tty.WriteStream
classes.
Represents the readable side of a TTY. In normal circumstances
process.stdin will be the only tty.ReadStream instance in a Node.js
process and there should be no reason to create additional instances.
A boolean that is true if the TTY is currently configured to operate as a
raw device.
This flag is always false when a process starts, even if the terminal is
operating in raw mode. Its value will change with subsequent calls to
setRawMode.
A boolean that is always true for tty.ReadStream instances.
Allows configuration of tty.ReadStream so that it operates as a raw device.
When in raw mode, input is always available character-by-character, not
including modifiers. Additionally, all special processing of characters by the
terminal is disabled, including echoing input
characters. Ctrl+C will no longer cause a SIGINT when
in this mode.
Represents the writable side of a TTY. In normal circumstances,
process.stdout and process.stderr will be the only
tty.WriteStream instances created for a Node.js process and there
should be no reason to create additional instances.
The 'resize' event is emitted whenever either of the writeStream.columns
or writeStream.rows properties have changed. No arguments are passed to the
listener callback when called.
The stream's write() callback and return value are exposed.
Added in: v0.7.7
writeStream.clearLine() clears the current line of this WriteStream in a
direction identified by dir.
The stream's write() callback and return value are exposed.
Added in: v0.7.7
writeStream.clearScreenDown() clears this WriteStream from the current
cursor down.
A number specifying the number of columns the TTY currently has. This property
is updated whenever the 'resize' event is emitted.
The stream's write() callback and return value are exposed.
Added in: v0.7.7
writeStream.cursorTo() moves this WriteStream's cursor to the specified
position.
Returns:
Use this to determine what colors the terminal supports. Due to the nature of
colors in terminals it is possible to either have false positives or false
negatives. It depends on process information and the environment variables that
may lie about what terminal is used.
It is possible to pass in an env object to simulate the usage of a specific
terminal. This can be useful to check how specific environment settings behave.
To enforce a specific color support, use one of the below environment settings.
Disabling color support is also possible by using the NO_COLOR and
NODE_DISABLE_COLORS environment variables.
writeStream.getWindowSize() returns the size of the TTY
corresponding to this WriteStream. The array is of the type
[numColumns, numRows] where numColumns and numRows represent the number
of columns and rows in the corresponding TTY.
Returns true if the writeStream supports at least as many colors as provided
in count. Minimum support is 2 (black and white).
This has the same false positives and negatives as described in
writeStream.getColorDepth().
A boolean that is always true.
The stream's write() callback and return value are exposed.
Added in: v0.7.7
writeStream.moveCursor() moves this WriteStream's cursor relative to its
current position.
A number specifying the number of rows the TTY currently has. This property
is updated whenever the 'resize' event is emitted.
The tty.isatty() method returns true if the given fd is associated with
a TTY and false if it is not, including whenever fd is not a non-negative
integer.


Source Code: lib/dgram.js
The node:dgram module provides an implementation of UDP datagram sockets.
Encapsulates the datagram functionality.
New instances of dgram.Socket are created using dgram.createSocket().
The new keyword is not to be used to create dgram.Socket instances.
The 'close' event is emitted after a socket is closed with close().
Once triggered, no new 'message' events will be emitted on this socket.
The 'connect' event is emitted after a socket is associated to a remote
address as a result of a successful connect() call.
The 'error' event is emitted whenever any error occurs. The event handler
function is passed a single Error object.
The 'listening' event is emitted once the dgram.Socket is addressable and
can receive data. This happens either explicitly with socket.bind() or
implicitly the first time data is sent using socket.send().
Until the dgram.Socket is listening, the underlying system resources do not
exist and calls such as socket.address() and socket.setTTL() will fail.
The family property now returns a string instead of a number.
The family property now returns a number instead of a string.
Added in: v0.1.99
The 'message' event is emitted when a new datagram is available on a socket.
The event handler function is passed two arguments: msg and rinfo.
If the source address of the incoming packet is an IPv6 link-local
address, the interface name is added to the address. For
example, a packet received on the en0 interface might have the
address field set to 'fe80::2618:1234:ab11:3b9c%en0', where '%en0'
is the interface name as a zone ID suffix.
Tells the kernel to join a multicast group at the given multicastAddress and
multicastInterface using the IP_ADD_MEMBERSHIP socket option. If the
multicastInterface argument is not specified, the operating system will choose
one interface and will add membership to it. To add membership to every
available interface, call addMembership multiple times, once per interface.
When called on an unbound socket, this method will implicitly bind to a random
port, listening on all interfaces.
When sharing a UDP socket across multiple cluster workers, the
socket.addMembership() function must be called only once or an
EADDRINUSE error will occur:
Tells the kernel to join a source-specific multicast channel at the given
sourceAddress and groupAddress, using the multicastInterface with the
IP_ADD_SOURCE_MEMBERSHIP socket option. If the multicastInterface argument
is not specified, the operating system will choose one interface and will add
membership to it. To add membership to every available interface, call
socket.addSourceSpecificMembership() multiple times, once per interface.
When called on an unbound socket, this method will implicitly bind to a random
port, listening on all interfaces.
Returns an object containing the address information for a socket.
For UDP sockets, this object will contain address, family, and port
properties.
This method throws EBADF if called on an unbound socket.
The method was changed to an asynchronous execution model. Legacy code would need to be changed to pass a callback function to the method call.
Added in: v0.1.99
For UDP sockets, causes the dgram.Socket to listen for datagram
messages on a named port and optional address. If port is not
specified or is 0, the operating system will attempt to bind to a
random port. If address is not specified, the operating system will
attempt to listen on all addresses. Once binding is complete, a
'listening' event is emitted and the optional callback function is
called.
Specifying both a 'listening' event listener and passing a
callback to the socket.bind() method is not harmful but not very
useful.
A bound datagram socket keeps the Node.js process running to receive
datagram messages.
If binding fails, an 'error' event is generated. In rare case (e.g.
attempting to bind with a closed socket), an Error may be thrown.
Example of a UDP server listening on port 41234:
For UDP sockets, causes the dgram.Socket to listen for datagram
messages on a named port and optional address that are passed as
properties of an options object passed as the first argument. If
port is not specified or is 0, the operating system will attempt
to bind to a random port. If address is not specified, the operating
system will attempt to listen on all addresses. Once binding is
complete, a 'listening' event is emitted and the optional callback
function is called.
The options object may contain a fd property. When a fd greater
than 0 is set, it will wrap around an existing socket with the given
file descriptor. In this case, the properties of port and address
will be ignored.
Specifying both a 'listening' event listener and passing a
callback to the socket.bind() method is not harmful but not very
useful.
The options object may contain an additional exclusive property that is
used when using dgram.Socket objects with the cluster module. When
exclusive is set to false (the default), cluster workers will use the same
underlying socket handle allowing connection handling duties to be shared.
When exclusive is true, however, the handle is not shared and attempted
port sharing results in an error.
A bound datagram socket keeps the Node.js process running to receive
datagram messages.
If binding fails, an 'error' event is generated. In rare case (e.g.
attempting to bind with a closed socket), an Error may be thrown.
An example socket listening on an exclusive port is shown below.
Close the underlying socket and stop listening for data on it. If a callback is
provided, it is added as a listener for the 'close' event.


Calls socket.close() and returns a promise that fulfills when the
socket has closed.
Associates the dgram.Socket to a remote address and port. Every
message sent by this handle is automatically sent to that destination. Also,
the socket will only receive messages from that remote peer.
Trying to call connect() on an already connected socket will result
in an ERR_SOCKET_DGRAM_IS_CONNECTED exception. If address is not
provided, '127.0.0.1' (for udp4 sockets) or '::1' (for udp6 sockets)
will be used by default. Once the connection is complete, a 'connect' event
is emitted and the optional callback function is called. In case of failure,
the callback is called or, failing this, an 'error' event is emitted.
A synchronous function that disassociates a connected dgram.Socket from
its remote address. Trying to call disconnect() on an unbound or already
disconnected socket will result in an ERR_SOCKET_DGRAM_NOT_CONNECTED
exception.
Instructs the kernel to leave a multicast group at multicastAddress using the
IP_DROP_MEMBERSHIP socket option. This method is automatically called by the
kernel when the socket is closed or the process terminates, so most apps will
never have reason to call this.
If multicastInterface is not specified, the operating system will attempt to
drop membership on all valid interfaces.
Instructs the kernel to leave a source-specific multicast channel at the given
sourceAddress and groupAddress using the IP_DROP_SOURCE_MEMBERSHIP
socket option. This method is automatically called by the kernel when the
socket is closed or the process terminates, so most apps will never have
reason to call this.
If multicastInterface is not specified, the operating system will attempt to
drop membership on all valid interfaces.
This method throws ERR_SOCKET_BUFFER_SIZE if called on an unbound socket.
This method throws ERR_SOCKET_BUFFER_SIZE if called on an unbound socket.
By default, binding a socket will cause it to block the Node.js process from
exiting as long as the socket is open. The socket.unref() method can be used
to exclude the socket from the reference counting that keeps the Node.js
process active. The socket.ref() method adds the socket back to the reference
counting and restores the default behavior.
Calling socket.ref() multiples times will have no additional effect.
The socket.ref() method returns a reference to the socket so calls can be
chained.
Returns an object containing the address, family, and port of the remote
endpoint. This method throws an ERR_SOCKET_DGRAM_NOT_CONNECTED exception
if the socket is not connected.
The address parameter now only accepts a string, null or undefined.
The msg parameter can now be any TypedArray or DataView.
Added support for sending data on connected sockets.
The msg parameter can be an Uint8Array now.
The address parameter is always optional now.
On success, callback will now be called with an error argument of null rather than 0.
The msg parameter can be an array now. Also, the offset and length parameters are optional now.
Added in: v0.1.99
Broadcasts a datagram on the socket.
For connectionless sockets, the destination port and address must be
specified. Connected sockets, on the other hand, will use their associated
remote endpoint, so the port and address arguments must not be set.
The msg argument contains the message to be sent.
Depending on its type, different behavior can apply. If msg is a Buffer,
any TypedArray or a DataView,
the offset and length specify the offset within the Buffer where the
message begins and the number of bytes in the message, respectively.
If msg is a String, then it is automatically converted to a Buffer
with 'utf8' encoding. With messages that
contain multi-byte characters, offset and length will be calculated with
respect to byte length and not the character position.
If msg is an array, offset and length must not be specified.
The address argument is a string. If the value of address is a host name,
DNS will be used to resolve the address of the host. If address is not
provided or otherwise nullish, '127.0.0.1' (for udp4 sockets) or '::1'
(for udp6 sockets) will be used by default.
If the socket has not been previously bound with a call to bind, the socket
is assigned a random port number and is bound to the "all interfaces" address
('0.0.0.0' for udp4 sockets, '::0' for udp6 sockets.)
An optional callback function may be specified to as a way of reporting
DNS errors or for determining when it is safe to reuse the buf object.
DNS lookups delay the time to send for at least one tick of the
Node.js event loop.
The only way to know for sure that the datagram has been sent is by using a
callback. If an error occurs and a callback is given, the error will be
passed as the first argument to the callback. If a callback is not given,
the error is emitted as an 'error' event on the socket object.
Offset and length are optional but both must be set if either are used.
They are supported only when the first argument is a Buffer, a TypedArray,
or a DataView.
This method throws ERR_SOCKET_BAD_PORT if called on an unbound socket.
Example of sending a UDP packet to a port on localhost;
Example of sending a UDP packet composed of multiple buffers to a port on
127.0.0.1;
Sending multiple buffers might be faster or slower depending on the
application and operating system. Run benchmarks to
determine the optimal strategy on a case-by-case basis. Generally speaking,
however, sending multiple buffers is faster.
Example of sending a UDP packet using a socket connected to a port on
localhost:
The maximum size of an IPv4/v6 datagram depends on the MTU
(Maximum Transmission Unit) and on the Payload Length field size.
The Payload Length field is 16 bits wide, which means that a normal
payload cannot exceed 64K octets including the internet header and data
(65,507 bytes = 65,535 ‚àí 8 bytes UDP header ‚àí 20 bytes IP header);
this is generally true for loopback interfaces, but such long datagram
messages are impractical for most hosts and networks.
The MTU is the largest size a given link layer technology can support for
datagram messages. For any link, IPv4 mandates a minimum MTU of 68
octets, while the recommended MTU for IPv4 is 576 (typically recommended
as the MTU for dial-up type applications), whether they arrive whole or in
fragments.
For IPv6, the minimum MTU is 1280 octets. However, the mandatory minimum
fragment reassembly buffer size is 1500 octets. The value of 68 octets is
very small, since most current link layer technologies, like Ethernet, have a
minimum MTU of 1500.
It is impossible to know in advance the MTU of each link through which
a packet might travel. Sending a datagram greater than the receiver MTU will
not work because the packet will get silently dropped without informing the
source that the data did not reach its intended recipient.
Sets or clears the SO_BROADCAST socket option. When set to true, UDP
packets may be sent to a local interface's broadcast address.
This method throws EBADF if called on an unbound socket.
All references to scope in this section are referring to
IPv6 Zone Indices, which are defined by RFC 4007. In string form, an IP
with a scope index is written as 'IP%scope' where scope is an interface name
or interface number.
Sets the default outgoing multicast interface of the socket to a chosen
interface or back to system interface selection. The multicastInterface must
be a valid string representation of an IP from the socket's family.
For IPv4 sockets, this should be the IP configured for the desired physical
interface. All packets sent to multicast on the socket will be sent on the
interface determined by the most recent successful use of this call.
For IPv6 sockets, multicastInterface should include a scope to indicate the
interface as in the examples that follow. In IPv6, individual send calls can
also use explicit scope in addresses, so only packets sent to a multicast
address without specifying an explicit scope are affected by the most recent
successful use of this call.
This method throws EBADF if called on an unbound socket.
On most systems, where scope format uses the interface name:
On Windows, where scope format uses an interface number:
All systems use an IP of the host on the desired physical interface:
A call on a socket that is not ready to send or no longer open may throw a Not
running Error.
If multicastInterface can not be parsed into an IP then an EINVAL
System Error is thrown.
On IPv4, if multicastInterface is a valid address but does not match any
interface, or if the address does not match the family then
a System Error such as EADDRNOTAVAIL or EPROTONOSUP is thrown.
On IPv6, most errors with specifying or omitting scope will result in the socket
continuing to use (or returning to) the system's default interface selection.
A socket's address family's ANY address (IPv4 '0.0.0.0' or IPv6 '::') can be
used to return control of the sockets default outgoing interface to the system
for future multicast packets.
Sets or clears the IP_MULTICAST_LOOP socket option. When set to true,
multicast packets will also be received on the local interface.
This method throws EBADF if called on an unbound socket.
Sets the IP_MULTICAST_TTL socket option. While TTL generally stands for
"Time to Live", in this context it specifies the number of IP hops that a
packet is allowed to travel through, specifically for multicast traffic. Each
router or gateway that forwards a packet decrements the TTL. If the TTL is
decremented to 0 by a router, it will not be forwarded.
The ttl argument may be between 0 and 255. The default on most systems is 1.
This method throws EBADF if called on an unbound socket.
Sets the SO_RCVBUF socket option. Sets the maximum socket receive buffer
in bytes.
This method throws ERR_SOCKET_BUFFER_SIZE if called on an unbound socket.
Sets the SO_SNDBUF socket option. Sets the maximum socket send buffer
in bytes.
This method throws ERR_SOCKET_BUFFER_SIZE if called on an unbound socket.
Sets the IP_TTL socket option. While TTL generally stands for "Time to Live",
in this context it specifies the number of IP hops that a packet is allowed to
travel through. Each router or gateway that forwards a packet decrements the
TTL. If the TTL is decremented to 0 by a router, it will not be forwarded.
Changing TTL values is typically done for network probes or when multicasting.
The ttl argument may be between 1 and 255. The default on most systems
is 64.
This method throws EBADF if called on an unbound socket.
By default, binding a socket will cause it to block the Node.js process from
exiting as long as the socket is open. The socket.unref() method can be used
to exclude the socket from the reference counting that keeps the Node.js
process active, allowing the process to exit even if the socket is still
listening.
Calling socket.unref() multiple times will have no additional effect.
The socket.unref() method returns a reference to the socket so calls can be
chained.
AbortSignal support was added.
The ipv6Only option is supported.
The recvBufferSize and sendBufferSize options are supported now.
The lookup option is supported.
Added in: v0.11.13
Creates a dgram.Socket object. Once the socket is created, calling
socket.bind() will instruct the socket to begin listening for datagram
messages. When address and port are not passed to socket.bind() the
method will bind the socket to the "all interfaces" address on a random port
(it does the right thing for both udp4 and udp6 sockets). The bound address
and port can be retrieved using socket.address().address and
socket.address().port.
If the signal option is enabled, calling .abort() on the corresponding
AbortController is similar to calling .close() on the socket:
Creates a dgram.Socket object of the specified type.
Once the socket is created, calling socket.bind() will instruct the
socket to begin listening for datagram messages. When address and port are
not passed to socket.bind() the method will bind the socket to the "all
interfaces" address on a random port (it does the right thing for both udp4
and udp6 sockets). The bound address and port can be retrieved using
socket.address().address and socket.address().port.


Source Code: lib/url.js
The node:url module provides utilities for URL resolution and parsing. It can
be accessed using:
A URL string is a structured string containing multiple meaningful components.
When parsed, a URL object is returned containing properties for each of these
components.
The node:url module provides two APIs for working with URLs: a legacy API that
is Node.js specific, and a newer API that implements the same
WHATWG URL Standard used by web browsers.
A comparison between the WHATWG and legacy APIs is provided below. Above the URL
'https://user:[email protected]:8080/p/a/t/h?query=string#hash', properties
of an object returned by the legacy url.parse() are shown. Below it are
properties of a WHATWG URL object.
WHATWG URL's origin property includes protocol and host, but not
username or password.
Parsing the URL string using the WHATWG API:
Parsing the URL string using the legacy API:
It is possible to construct a WHATWG URL from component parts using either the
property setters or a template literal string:
To get the constructed URL string, use the href property accessor:
The class is now available on the global object.
Added in: v7.0.0, v6.13.0
Browser-compatible URL class, implemented by following the WHATWG URL
Standard. Examples of parsed URLs may be found in the Standard itself.
The URL class is also available on the global object.
In accordance with browser conventions, all properties of URL objects
are implemented as getters and setters on the class prototype, rather than as
data properties on the object itself. Thus, unlike legacy urlObjects,
using the delete keyword on any properties of URL objects (e.g. delete myURL.protocol, delete myURL.pathname, etc) has no effect but will still
return true.
ICU requirement is removed.
Creates a new URL object by parsing the input relative to the base. If
base is passed as a string, it will be parsed equivalent to new URL(base).
The URL constructor is accessible as a property on the global object.
It can also be imported from the built-in url module:
A TypeError will be thrown if the input or base are not valid URLs. Note
that an effort will be made to coerce the given values into strings. For
instance:
Unicode characters appearing within the host name of input will be
automatically converted to ASCII using the Punycode algorithm.
In cases where it is not known in advance if input is an absolute URL
and a base is provided, it is advised to validate that the origin of
the URL object is what is expected.
Gets and sets the fragment portion of the URL.
Invalid URL characters included in the value assigned to the hash property
are percent-encoded. The selection of which characters to
percent-encode may vary somewhat from what the url.parse() and
url.format() methods would produce.
Gets and sets the host portion of the URL.
Invalid host values assigned to the host property are ignored.
Gets and sets the host name portion of the URL. The key difference between
url.host and url.hostname is that url.hostname does not include the
port.
Invalid host name values assigned to the hostname property are ignored.
Gets and sets the serialized URL.
Getting the value of the href property is equivalent to calling
url.toString().
Setting the value of this property to a new value is equivalent to creating a
new URL object using new URL(value). Each of the URL
object's properties will be modified.
If the value assigned to the href property is not a valid URL, a TypeError
will be thrown.
The scheme "gopher" is no longer special and url.origin now returns 'null' for it.
Gets the read-only serialization of the URL's origin.
Gets and sets the password portion of the URL.
Invalid URL characters included in the value assigned to the password property
are percent-encoded. The selection of which characters to
percent-encode may vary somewhat from what the url.parse() and
url.format() methods would produce.
Gets and sets the path portion of the URL.
Invalid URL characters included in the value assigned to the pathname
property are percent-encoded. The selection of which characters
to percent-encode may vary somewhat from what the url.parse() and
url.format() methods would produce.
The scheme "gopher" is no longer special.
Gets and sets the port portion of the URL.
The port value may be a number or a string containing a number in the range
0 to 65535 (inclusive). Setting the value to the default port of the
URL objects given protocol will result in the port value becoming
the empty string ('').
The port value can be an empty string in which case the port depends on
the protocol/scheme:
Upon assigning a value to the port, the value will first be converted to a
string using .toString().
If that string is invalid but it begins with a number, the leading number is
assigned to port.
If the number lies outside the range denoted above, it is ignored.
Numbers which contain a decimal point,
such as floating-point numbers or numbers in scientific notation,
are not an exception to this rule.
Leading numbers up to the decimal point will be set as the URL's port,
assuming they are valid:
Gets and sets the protocol portion of the URL.
Invalid URL protocol values assigned to the protocol property are ignored.
The scheme "gopher" is no longer special.
The WHATWG URL Standard considers a handful of URL protocol schemes to be
special in terms of how they are parsed and serialized. When a URL is
parsed using one of these special protocols, the url.protocol property
may be changed to another special protocol but cannot be changed to a
non-special protocol, and vice versa.
For instance, changing from http to https works:
However, changing from http to a hypothetical fish protocol does not
because the new protocol is not special.
Likewise, changing from a non-special protocol to a special protocol is also
not permitted:
According to the WHATWG URL Standard, special protocol schemes are ftp,
file, http, https, ws, and wss.
Gets and sets the serialized query portion of the URL.
Any invalid URL characters appearing in the value assigned the search
property will be percent-encoded. The selection of which
characters to percent-encode may vary somewhat from what the url.parse()
and url.format() methods would produce.
Gets the URLSearchParams object representing the query parameters of the
URL. This property is read-only but the URLSearchParams object it provides
can be used to mutate the URL instance; to replace the entirety of query
parameters of the URL, use the url.search setter. See
URLSearchParams documentation for details.
Use care when using .searchParams to modify the URL because,
per the WHATWG specification, the URLSearchParams object uses
different rules to determine which characters to percent-encode. For
instance, the URL object will not percent encode the ASCII tilde (~)
character, while URLSearchParams will always encode it:
Gets and sets the username portion of the URL.
Any invalid URL characters appearing in the value assigned the username
property will be percent-encoded. The selection of which
characters to percent-encode may vary somewhat from what the url.parse()
and url.format() methods would produce.
The toString() method on the URL object returns the serialized URL. The
value returned is equivalent to that of url.href and url.toJSON().
The toJSON() method on the URL object returns the serialized URL. The
value returned is equivalent to that of url.href and
url.toString().
This method is automatically called when an URL object is serialized
with JSON.stringify().


Creates a 'blob:nodedata:...' URL string that represents the given <Blob>
object and can be used to retrieve the Blob later.
The data stored by the registered <Blob> will be retained in memory until
URL.revokeObjectURL() is called to remove it.
Blob objects are registered within the current thread. If using Worker
Threads, Blob objects registered within one Worker will not be available
to other workers or the main thread.


Removes the stored <Blob> identified by the given ID. Attempting to revoke a
ID that isn't registered will silently fail.
Checks if an input relative to the base can be parsed to a URL.
The class is now available on the global object.
Added in: v7.5.0, v6.13.0
The URLSearchParams API provides read and write access to the query of a
URL. The URLSearchParams class can also be used standalone with one of the
four following constructors.
The URLSearchParams class is also available on the global object.
The WHATWG URLSearchParams interface and the querystring module have
similar purpose, but the purpose of the querystring module is more
general, as it allows the customization of delimiter characters (& and =).
On the other hand, this API is designed purely for URL query strings.
Instantiate a new empty URLSearchParams object.
Parse the string as a query string, and use it to instantiate a new
URLSearchParams object. A leading '?', if present, is ignored.
Instantiate a new URLSearchParams object with a query hash map. The key and
value of each property of obj are always coerced to strings.
Unlike querystring module, duplicate keys in the form of array values are
not allowed. Arrays are stringified using array.toString(), which simply
joins all array elements with commas.
Instantiate a new URLSearchParams object with an iterable map in a way that
is similar to Map's constructor. iterable can be an Array or any
iterable object. That means iterable can be another URLSearchParams, in
which case the constructor will simply create a clone of the provided
URLSearchParams. Elements of iterable are key-value pairs, and can
themselves be any iterable object.
Duplicate keys are allowed.
Append a new name-value pair to the query string.
Add support for optional value argument.
If value is provided, removes all name-value pairs
where name is name and value is value..
If value is not provided, removes all name-value pairs whose name is name.
Returns an ES6 Iterator over each of the name-value pairs in the query.
Each item of the iterator is a JavaScript Array. The first item of the Array
is the name, the second item of the Array is the value.
Alias for urlSearchParams[@@iterator]().
Passing an invalid callback to the fn argument now throws ERR_INVALID_ARG_TYPE instead of ERR_INVALID_CALLBACK.
Iterates over each name-value pair in the query and invokes the given function.
Returns the value of the first name-value pair whose name is name. If there
are no such pairs, null is returned.
Returns the values of all name-value pairs whose name is name. If there are
no such pairs, an empty array is returned.
Add support for optional value argument.
Checks if the URLSearchParams object contains key-value pair(s) based on
name and an optional value argument.
If value is provided, returns true when name-value pair with
same name and value exists.
If value is not provided, returns true if there is at least one name-value
pair whose name is name.
Returns an ES6 Iterator over the names of each name-value pair.
Sets the value in the URLSearchParams object associated with name to
value. If there are any pre-existing name-value pairs whose names are name,
set the first such pair's value to value and remove all others. If not,
append the name-value pair to the query string.
The total number of parameter entries.
Sort all existing name-value pairs in-place by their names. Sorting is done
with a stable sorting algorithm, so relative order between name-value pairs
with the same name is preserved.
This method can be used, in particular, to increase cache hits.
Returns the search parameters serialized as a string, with characters
percent-encoded where necessary.
Returns an ES6 Iterator over the values of each name-value pair.
Returns an ES6 Iterator over each of the name-value pairs in the query string.
Each item of the iterator is a JavaScript Array. The first item of the Array
is the name, the second item of the Array is the value.
Alias for urlSearchParams.entries().
ICU requirement is removed.
Added in: v7.4.0, v6.13.0
Returns the Punycode ASCII serialization of the domain. If domain is an
invalid domain, the empty string is returned.
It performs the inverse operation to url.domainToUnicode().
ICU requirement is removed.
Added in: v7.4.0, v6.13.0
Returns the Unicode serialization of the domain. If domain is an invalid
domain, the empty string is returned.
It performs the inverse operation to url.domainToASCII().
This function ensures the correct decodings of percent-encoded characters as
well as ensuring a cross-platform valid absolute path string.
Returns a customizable serialization of a URL String representation of a
WHATWG URL object.
The URL object has both a toString() method and href property that return
string serializations of the URL. These are not, however, customizable in
any way. The url.format(URL[, options]) method allows for basic customization
of the output.
This function ensures that path is resolved absolutely, and that the URL
control characters are correctly encoded when converting into a File URL.
The returned object will also contain all the own enumerable properties of the url argument.
Added in: v15.7.0, v14.18.0
This utility function converts a URL object into an ordinary options object as
expected by the http.request() and https.request() APIs.
Deprecation revoked. Status changed to "Legacy".
This API is deprecated.


Deprecation revoked. Status changed to "Legacy".
The Legacy URL API is deprecated. Use the WHATWG URL API.


The legacy urlObject (require('node:url').Url or
import { Url } from 'node:url') is
created and returned by the url.parse() function.
The auth property is the username and password portion of the URL, also
referred to as userinfo. This string subset follows the protocol and
double slashes (if present) and precedes the host component, delimited by @.
The string is either the username, or it is the username and password separated
by :.
For example: 'user:pass'.
The hash property is the fragment identifier portion of the URL including the
leading # character.
For example: '#hash'.
The host property is the full lower-cased host portion of the URL, including
the port if specified.
For example: 'sub.example.com:8080'.
The hostname property is the lower-cased host name portion of the host
component without the port included.
For example: 'sub.example.com'.
The href property is the full URL string that was parsed with both the
protocol and host components converted to lower-case.
For example: 'http://user:[email protected]:8080/p/a/t/h?query=string#hash'.
The path property is a concatenation of the pathname and search
components.
For example: '/p/a/t/h?query=string'.
No decoding of the path is performed.
The pathname property consists of the entire path section of the URL. This
is everything following the host (including the port) and before the start
of the query or hash components, delimited by either the ASCII question
mark (?) or hash (#) characters.
For example: '/p/a/t/h'.
No decoding of the path string is performed.
The port property is the numeric port portion of the host component.
For example: '8080'.
The protocol property identifies the URL's lower-cased protocol scheme.
For example: 'http:'.
The query property is either the query string without the leading ASCII
question mark (?), or an object returned by the querystring module's
parse() method. Whether the query property is a string or object is
determined by the parseQueryString argument passed to url.parse().
For example: 'query=string' or {'query': 'string'}.
If returned as a string, no decoding of the query string is performed. If
returned as an object, both keys and values are decoded.
The search property consists of the entire "query string" portion of the
URL, including the leading ASCII question mark (?) character.
For example: '?query=string'.
No decoding of the query string is performed.
The slashes property is a boolean with a value of true if two ASCII
forward-slash characters (/) are required following the colon in the
protocol.
Now throws an ERR_INVALID_URL exception when Punycode conversion of a hostname introduces changes that could cause the URL to be re-parsed differently.
Deprecation revoked. Status changed to "Legacy".
The Legacy URL API is deprecated. Use the WHATWG URL API.
URLs with a file: scheme will now always use the correct number of slashes regardless of slashes option. A falsy slashes option with no protocol is now also respected at all times.
Added in: v0.1.25


The url.format() method returns a formatted URL string derived from
urlObject.
If urlObject is not an object or a string, url.format() will throw a
TypeError.
The formatting process operates as follows:
Documentation-only deprecation.
Deprecation revoked. Status changed to "Legacy".
The pathname property on the returned URL object is now / when there is no path and the protocol scheme is ws: or wss:.
The Legacy URL API is deprecated. Use the WHATWG URL API.
The search property on the returned URL object is now null when no query string is present.
Added in: v0.1.25


The url.parse() method takes a URL string, parses it, and returns a URL
object.
A TypeError is thrown if urlString is not a string.
A URIError is thrown if the auth property is present but cannot be decoded.
url.parse() uses a lenient, non-standard algorithm for parsing URL
strings. It is prone to security issues such as host name spoofing
and incorrect handling of usernames and passwords. Do not use with untrusted
input. CVEs are not issued for url.parse() vulnerabilities. Use the
WHATWG URL API instead.
Deprecation revoked. Status changed to "Legacy".
The Legacy URL API is deprecated. Use the WHATWG URL API.
The auth fields are now kept intact when from and to refer to the same host.
The auth fields is cleared now the to parameter contains a hostname.
The port field is copied correctly now.
Added in: v0.1.25


The url.resolve() method resolves a target URL relative to a base URL in a
manner similar to that of a web browser resolving an anchor tag.
To achieve the same result using the WHATWG URL API:

URLs are permitted to only contain a certain range of characters. Any character
falling outside of that range must be encoded. How such characters are encoded,
and which characters to encode depends entirely on where the character is
located within the structure of the URL.
Within the Legacy API, spaces (' ') and the following characters will be
automatically escaped in the properties of URL objects:
For example, the ASCII space character (' ') is encoded as %20. The ASCII
forward slash (/) character is encoded as %3C.
The WHATWG URL Standard uses a more selective and fine grained approach to
selecting encoded characters than that used by the Legacy API.
The WHATWG algorithm defines four "percent-encode sets" that describe ranges
of characters that must be percent-encoded:
The C0 control percent-encode set includes code points in range U+0000 to
U+001F (inclusive) and all code points greater than U+007E (~).
The fragment percent-encode set includes the C0 control percent-encode set
and code points U+0020 SPACE, U+0022 ("), U+003C (<), U+003E (>),
and U+0060 (`).
The path percent-encode set includes the C0 control percent-encode set
and code points U+0020 SPACE, U+0022 ("), U+0023 (#), U+003C (<), U+003E (>),
U+003F (?), U+0060 (`), U+007B ({), and U+007D (}).
The userinfo encode set includes the path percent-encode set and code
points U+002F (/), U+003A (:), U+003B (;), U+003D (=), U+0040 (@),
U+005B ([) to U+005E(^), and U+007C (|).
The userinfo percent-encode set is used exclusively for username and
passwords encoded within the URL. The path percent-encode set is used for the
path of most URLs. The fragment percent-encode set is used for URL fragments.
The C0 control percent-encode set is used for host and path under certain
specific conditions, in addition to all other cases.
When non-ASCII characters appear within a host name, the host name is encoded
using the Punycode algorithm. Note, however, that a host name may contain
both Punycode encoded and percent-encoded characters:


Source Code: lib/util.js
The node:util module supports the needs of Node.js internal APIs. Many of the
utilities are useful for application and module developers as well. To access
it:
Takes an async function (or a function that returns a Promise) and returns a
function following the error-first callback style, i.e. taking
an (err, value) => ... callback as the last argument. In the callback, the
first argument will be the rejection reason (or null if the Promise
resolved), and the second argument will be the resolved value.
Will print:
The callback is executed asynchronously, and will have a limited stack trace.
If the callback throws, the process will emit an 'uncaughtException'
event, and if not handled will exit.
Since null has a special meaning as the first argument to a callback, if a
wrapped function rejects a Promise with a falsy value as a reason, the value
is wrapped in an Error with the original value stored in a field named
reason.
The util.debuglog() method is used to create a function that conditionally
writes debug messages to stderr based on the existence of the NODE_DEBUG
environment variable. If the section name appears within the value of that
environment variable, then the returned function operates similar to
console.error(). If not, then the returned function is a no-op.
If this program is run with NODE_DEBUG=foo in the environment, then
it will output something like:
where 3245 is the process id. If it is not run with that
environment variable set, then it will not print anything.
The section supports wildcard also:
if it is run with NODE_DEBUG=foo* in the environment, then it will output
something like:
Multiple comma-separated section names may be specified in the NODE_DEBUG
environment variable: NODE_DEBUG=fs,net,tls.
The optional callback argument can be used to replace the logging function
with a different function that doesn't have any initialization or
unnecessary wrapping.
The util.debuglog().enabled getter is used to create a test that can be used
in conditionals based on the existence of the NODE_DEBUG environment variable.
If the section name appears within the value of that environment variable,
then the returned value will be true. If not, then the returned value will be
false.
If this program is run with NODE_DEBUG=foo in the environment, then it will
output something like:
Alias for util.debuglog. Usage allows for readability of that doesn't imply
logging when only using util.debuglog().enabled.
Deprecation warnings are only emitted once for each code.
Added in: v0.8.0
The util.deprecate() method wraps fn (which may be a function or class) in
such a way that it is marked as deprecated.
When called, util.deprecate() will return a function that will emit a
DeprecationWarning using the 'warning' event. The warning will
be emitted and printed to stderr the first time the returned function is
called. After the warning is emitted, the wrapped function is called without
emitting a warning.
If the same optional code is supplied in multiple calls to util.deprecate(),
the warning will be emitted only once for that code.
If either the --no-deprecation or --no-warnings command-line flags are
used, or if the process.noDeprecation property is set to true prior to
the first deprecation warning, the util.deprecate() method does nothing.
If the --trace-deprecation or --trace-warnings command-line flags are set,
or the process.traceDeprecation property is set to true, a warning and a
stack trace are printed to stderr the first time the deprecated function is
called.
If the --throw-deprecation command-line flag is set, or the
process.throwDeprecation property is set to true, then an exception will be
thrown when the deprecated function is called.
The --throw-deprecation command-line flag and process.throwDeprecation
property take precedence over --trace-deprecation and
process.traceDeprecation.
The %c specifier is ignored now.
The format argument is now only taken as such if it actually contains format specifiers.
If the format argument is not a format string, the output string's formatting is no longer dependent on the type of the first argument. This change removes previously present quotes from strings that were being output when the first argument was not a string.
The %d, %f, and %i specifiers now support Symbols properly.
The %o specifier's depth has default depth of 4 again.
The %o specifier's depth option will now fall back to the default depth.
The %d and %i specifiers now support BigInt.
The %o and %O specifiers are supported now.
Added in: v0.5.3
The util.format() method returns a formatted string using the first argument
as a printf-like format string which can contain zero or more format
specifiers. Each specifier is replaced with the converted value from the
corresponding argument. Supported specifiers are:
If a specifier does not have a corresponding argument, it is not replaced:
Values that are not part of the format string are formatted using
util.inspect() if their type is not string.
If there are more arguments passed to the util.format() method than the
number of specifiers, the extra arguments are concatenated to the returned
string, separated by spaces:
If the first argument does not contain a valid format specifier, util.format()
returns a string that is the concatenation of all arguments separated by spaces:
If only one argument is passed to util.format(), it is returned as it is
without any formatting:
util.format() is a synchronous method that is intended as a debugging tool.
Some input values can have a significant performance overhead that can block the
event loop. Use this function with care and never in a hot code path.
This function is identical to util.format(), except in that it takes
an inspectOptions argument which specifies options that are passed along to
util.inspect().
Returns the string name for a numeric error code that comes from a Node.js API.
The mapping between error codes and error names is platform-dependent.
See Common System Errors for the names of common errors.
Returns a Map of all system error codes available from the Node.js API.
The mapping between error codes and error names is platform-dependent.
See Common System Errors for the names of common errors.
The constructor parameter can refer to an ES6 class now.
Added in: v0.3.0


Usage of util.inherits() is discouraged. Please use the ES6 class and
extends keywords to get language level inheritance support. Also note
that the two styles are semantically incompatible.
Inherit the prototype methods from one constructor into another. The
prototype of constructor will be set to a new object created from
superConstructor.
This mainly adds some input validation on top of
Object.setPrototypeOf(constructor.prototype, superConstructor.prototype).
As an additional convenience, superConstructor will be accessible
through the constructor.super_ property.
ES6 example using class and extends:
add support for maxArrayLength when inspecting Set and Map.
The numericSeparator option is supported now.
Circular references now include a marker to the reference.
If object is from a different vm.Context now, a custom inspection function on it will not receive context-specific arguments anymore.
The maxStringLength option is supported now.
User defined prototype properties are inspected in case showHidden is true.
The compact options default is changed to 3 and the breakLength options default is changed to 80.
Internal properties no longer appear in the context argument of a custom inspection function.
The compact option accepts numbers for a new output mode.
ArrayBuffers now also show their binary contents.
The getters option is supported now.
The depth default changed back to 2.
The depth default changed to 20.
The inspection output is now limited to about 128 MiB. Data above that size will not be fully inspected.
The sorted option is supported now.
Inspecting linked lists and similar objects is now possible up to the maximum call stack size.
The WeakMap and WeakSet entries can now be inspected as well.
The compact option is supported now.
Custom inspection functions can now return this.
The breakLength option is supported now.
The maxArrayLength option is supported now; in particular, long arrays are truncated by default.
The showProxy option is supported now.
Added in: v0.3.0
The util.inspect() method returns a string representation of object that is
intended for debugging. The output of util.inspect may change at any time
and should not be depended upon programmatically. Additional options may be
passed that alter the result.
util.inspect() will use the constructor's name and/or @@toStringTag to make
an identifiable tag for an inspected value.
Circular references point to their anchor by using a reference index:
The following example inspects all properties of the util object:
The following example highlights the effect of the compact option:
The showHidden option allows WeakMap and WeakSet entries to be
inspected. If there are more entries than maxArrayLength, there is no
guarantee which entries are displayed. That means retrieving the same
WeakSet entries twice may result in different output. Furthermore, entries
with no remaining strong references may be garbage collected at any time.
The sorted option ensures that an object's property insertion order does not
impact the result of util.inspect().
The numericSeparator option adds an underscore every three digits to all
numbers.
util.inspect() is a synchronous method intended for debugging. Its maximum
output length is approximately 128 MiB. Inputs that result in longer output will
be truncated.
Color output (if enabled) of util.inspect is customizable globally
via the util.inspect.styles and util.inspect.colors properties.
util.inspect.styles is a map associating a style name to a color from
util.inspect.colors.
The default styles and associated colors are:
Color styling uses ANSI control codes that may not be supported on all
terminals. To verify color support use tty.hasColors().
Predefined control codes are listed below (grouped as "Modifiers", "Foreground
colors", and "Background colors").
Modifier support varies throughout different terminals. They will mostly be
ignored, if not supported.
The inspect argument is added for more interoperability.
Added in: v0.1.97
Objects may also define their own
[util.inspect.custom](depth, opts, inspect) function,
which util.inspect() will invoke and use the result of when inspecting
the object.
Custom [util.inspect.custom](depth, opts, inspect) functions typically return
a string but may return a value of any type that will be formatted accordingly
by util.inspect().
This is now defined as a shared symbol.
Added in: v6.6.0
In addition to being accessible through util.inspect.custom, this
symbol is registered globally and can be
accessed in any environment as Symbol.for('nodejs.util.inspect.custom').
Using this allows code to be written in a portable fashion, so that the custom
inspect function is used in an Node.js environment and ignored in the browser.
The util.inspect() function itself is passed as third argument to the custom
inspect function to allow further portability.
See Custom inspection functions on Objects for more details.
The defaultOptions value allows customization of the default options used by
util.inspect. This is useful for functions like console.log or
util.format which implicitly call into util.inspect. It shall be set to an
object containing one or more valid util.inspect() options. Setting
option properties directly is also supported.
Returns true if there is deep strict equality between val1 and val2.
Otherwise, returns false.
See assert.deepStrictEqual() for more information about deep strict
equality.


An implementation of the MIMEType class.
In accordance with browser conventions, all properties of MIMEType objects
are implemented as getters and setters on the class prototype, rather than as
data properties on the object itself.
A MIME string is a structured string containing multiple meaningful
components. When parsed, a MIMEType object is returned containing
properties for each of these components.
Creates a new MIMEType object by parsing the input.
A TypeError will be thrown if the input is not a valid MIME. Note
that an effort will be made to coerce the given values into strings. For
instance:
Gets and sets the type portion of the MIME.
Gets and sets the subtype portion of the MIME.
Gets the essence of the MIME. This property is read only.
Use mime.type or mime.subtype to alter the MIME.
Gets the MIMEParams object representing the
parameters of the MIME. This property is read-only. See
MIMEParams documentation for details.
The toString() method on the MIMEType object returns the serialized MIME.
Because of the need for standard compliance, this method does not allow users
to customize the serialization process of the MIME.
Alias for mime.toString().
This method is automatically called when an MIMEType object is serialized
with JSON.stringify().
The MIMEParams API provides read and write access to the parameters of a
MIMEType.
Creates a new MIMEParams object by with empty parameters
Remove all name-value pairs whose name is name.
Returns an iterator over each of the name-value pairs in the parameters.
Each item of the iterator is a JavaScript Array. The first item of the array
is the name, the second item of the array is the value.
Returns the value of the first name-value pair whose name is name. If there
are no such pairs, null is returned.
Returns true if there is at least one name-value pair whose name is name.
Returns an iterator over the names of each name-value pair.
Sets the value in the MIMEParams object associated with name to
value. If there are any pre-existing name-value pairs whose names are name,
set the first such pair's value to value.
Returns an iterator over the values of each name-value pair.
Alias for mimeParams.entries().
The API is no longer experimental.
Add support for default values in input config.
add support for returning detailed parse information using tokens in input config and returned properties.
Added in: v18.3.0, v16.17.0
config <Object> Used to provide arguments for parsing and to configure
the parser. config supports the following properties:
Returns: <Object> The parsed command line arguments:
Provides a higher level API for command-line argument parsing than interacting
with process.argv directly. Takes a specification for the expected arguments
and returns a structured object with the parsed options and positionals.
Detailed parse information is available for adding custom behaviors by
specifying tokens: true in the configuration.
The returned tokens have properties describing:
The returned tokens are in the order encountered in the input args. Options
that appear more than once in args produce a token for each use. Short option
groups like -xy expand to a token for each option. So -xxx produces
three tokens.
For example to use the returned tokens to add support for a negated option
like --no-color, the tokens can be reprocessed to change the value stored
for the negated option.
Example usage showing negated options, and when an option is used
multiple ways then last one wins.
Calling promisify on a function that returns a Promise is deprecated.
Added in: v8.0.0
Takes a function following the common error-first callback style, i.e. taking
an (err, value) => ... callback as the last argument, and returns a version
that returns promises.
Or, equivalently using async functions:
If there is an original[util.promisify.custom] property present, promisify
will return its value, see Custom promisified functions.
promisify() assumes that original is a function taking a callback as its
final argument in all cases. If original is not a function, promisify()
will throw an error. If original is a function but its last argument is not
an error-first callback, it will still be passed an error-first
callback as its last argument.
Using promisify() on class methods or other methods that use this may not
work as expected unless handled specially:
Using the util.promisify.custom symbol one can override the return value of
util.promisify():
This can be useful for cases where the original function does not follow the
standard format of taking an error-first callback as the last argument.
For example, with a function that takes in
(foo, onSuccessCallback, onErrorCallback):
If promisify.custom is defined but is not a function, promisify() will
throw an error.
This is now defined as a shared symbol.
Added in: v8.0.0
In addition to being accessible through util.promisify.custom, this
symbol is registered globally and can be
accessed in any environment as Symbol.for('nodejs.util.promisify.custom').
For example, with a function that takes in
(foo, onSuccessCallback, onErrorCallback):
Returns str with any ANSI escape codes removed.
An implementation of the WHATWG Encoding Standard TextDecoder API.
Per the WHATWG Encoding Standard, the encodings supported by the
TextDecoder API are outlined in the tables below. For each encoding,
one or more aliases may be used.
Different Node.js build configurations support different sets of encodings.
(see Internationalization)
The 'iso-8859-16' encoding listed in the WHATWG Encoding Standard
is not supported.
The class is now available on the global object.
Added in: v8.3.0
Creates a new TextDecoder instance. The encoding may specify one of the
supported encodings or an alias.
The TextDecoder class is also available on the global object.
Decodes the input and returns a string. If options.stream is true, any
incomplete byte sequences occurring at the end of the input are buffered
internally and emitted after the next call to textDecoder.decode().
If textDecoder.fatal is true, decoding errors that occur will result in a
TypeError being thrown.
The encoding supported by the TextDecoder instance.
The value will be true if decoding errors result in a TypeError being
thrown.
The value will be true if the decoding result will include the byte order
mark.
The class is now available on the global object.
Added in: v8.3.0
An implementation of the WHATWG Encoding Standard TextEncoder API. All
instances of TextEncoder only support UTF-8 encoding.
The TextEncoder class is also available on the global object.
UTF-8 encodes the input string and returns a Uint8Array containing the
encoded bytes.
UTF-8 encodes the src string to the dest Uint8Array and returns an object
containing the read Unicode code units and written UTF-8 bytes.
The encoding supported by the TextEncoder instance. Always set to 'utf-8'.
Returns the string after replacing any surrogate code points
(or equivalently, any unpaired surrogate code units) with the
Unicode "replacement character" U+FFFD.


Creates and returns an <AbortController> instance whose <AbortSignal> is marked
as transferable and can be used with structuredClone() or postMessage().


Marks the given <AbortSignal> as transferable so that it can be used with
structuredClone() and postMessage().


Listens to abort event on the provided signal and
returns a promise that is fulfilled when the signal is
aborted. If the passed resource is garbage collected before the signal is
aborted, the returned promise shall remain pending indefinitely.
Exposed as require('util/types').
Added in: v10.0.0
util.types provides type checks for different kinds of built-in objects.
Unlike instanceof or Object.prototype.toString.call(value), these checks do
not inspect properties of the object that are accessible from JavaScript (like
their prototype), and usually have the overhead of calling into C++.
The result generally does not make any guarantees about what kinds of
properties or behavior a value exposes in JavaScript. They are primarily
useful for addon developers who prefer to do type checking in JavaScript.
The API is accessible via require('node:util').types or require('node:util/types').
Returns true if the value is a built-in ArrayBuffer or
SharedArrayBuffer instance.
See also util.types.isArrayBuffer() and
util.types.isSharedArrayBuffer().
Returns true if the value is an instance of one of the ArrayBuffer
views, such as typed array objects or DataView. Equivalent to
ArrayBuffer.isView().
Returns true if the value is an arguments object.
Returns true if the value is a built-in ArrayBuffer instance.
This does not include SharedArrayBuffer instances. Usually, it is
desirable to test for both; See util.types.isAnyArrayBuffer() for that.
Returns true if the value is an async function.
This only reports back what the JavaScript engine is seeing;
in particular, the return value may not match the original source code if
a transpilation tool was used.
Returns true if the value is a BigInt64Array instance.
Returns true if the value is a BigUint64Array instance.
Returns true if the value is a boolean object, e.g. created
by new Boolean().
Returns true if the value is any boxed primitive object, e.g. created
by new Boolean(), new String() or Object(Symbol()).
For example:
Returns true if value is a <CryptoKey>, false otherwise.
Returns true if the value is a built-in DataView instance.
See also ArrayBuffer.isView().
Returns true if the value is a built-in Date instance.
Returns true if the value is a native External value.
A native External value is a special type of object that contains a
raw C++ pointer (void*) for access from native code, and has no other
properties. Such objects are created either by Node.js internals or native
addons. In JavaScript, they are frozen objects with a
null prototype.
For further information on napi_create_external, refer to
napi_create_external().
Returns true if the value is a built-in Float32Array instance.
Returns true if the value is a built-in Float64Array instance.
Returns true if the value is a generator function.
This only reports back what the JavaScript engine is seeing;
in particular, the return value may not match the original source code if
a transpilation tool was used.
Returns true if the value is a generator object as returned from a
built-in generator function.
This only reports back what the JavaScript engine is seeing;
in particular, the return value may not match the original source code if
a transpilation tool was used.
Returns true if the value is a built-in Int8Array instance.
Returns true if the value is a built-in Int16Array instance.
Returns true if the value is a built-in Int32Array instance.
Returns true if value is a <KeyObject>, false otherwise.
Returns true if the value is a built-in Map instance.
Returns true if the value is an iterator returned for a built-in
Map instance.
Returns true if the value is an instance of a Module Namespace Object.
Returns true if the value was returned by the constructor of a
built-in Error type.
Subclasses of the native error types are also native errors:
A value being instanceof a native error class is not equivalent to isNativeError()
returning true for that value. isNativeError() returns true for errors
which come from a different realm while instanceof Error returns false
for these errors:
Conversely, isNativeError() returns false for all objects which were not
returned by the constructor of a native error. That includes values
which are instanceof native errors:
Returns true if the value is a number object, e.g. created
by new Number().
Returns true if the value is a built-in Promise.
Returns true if the value is a Proxy instance.
Returns true if the value is a regular expression object.
Returns true if the value is a built-in Set instance.
Returns true if the value is an iterator returned for a built-in
Set instance.
Returns true if the value is a built-in SharedArrayBuffer instance.
This does not include ArrayBuffer instances. Usually, it is
desirable to test for both; See util.types.isAnyArrayBuffer() for that.
Returns true if the value is a string object, e.g. created
by new String().
Returns true if the value is a symbol object, created
by calling Object() on a Symbol primitive.
Returns true if the value is a built-in TypedArray instance.
See also ArrayBuffer.isView().
Returns true if the value is a built-in Uint8Array instance.
Returns true if the value is a built-in Uint8ClampedArray instance.
Returns true if the value is a built-in Uint16Array instance.
Returns true if the value is a built-in Uint32Array instance.
Returns true if the value is a built-in WeakMap instance.
Returns true if the value is a built-in WeakSet instance.


Returns true if the value is a built-in WebAssembly.Module instance.
The following APIs are deprecated and should no longer be used. Existing
applications and modules should be updated to find alternative approaches.


The util._extend() method was never intended to be used outside of internal
Node.js modules. The community found and used it anyway.
It is deprecated and should not be used in new code. JavaScript comes with very
similar built-in functionality through Object.assign().


Alias for Array.isArray().
Returns true if the given object is an Array. Otherwise, returns false.


Returns true if the given object is a Boolean. Otherwise, returns false.


Returns true if the given object is a Buffer. Otherwise, returns false.


Returns true if the given object is a Date. Otherwise, returns false.


Returns true if the given object is an Error. Otherwise, returns
false.
This method relies on Object.prototype.toString() behavior. It is
possible to obtain an incorrect result when the object argument manipulates
@@toStringTag.


Returns true if the given object is a Function. Otherwise, returns
false.


Returns true if the given object is strictly null. Otherwise, returns
false.


Returns true if the given object is null or undefined. Otherwise,
returns false.


Returns true if the given object is a Number. Otherwise, returns false.


Returns true if the given object is strictly an Object and not a
Function (even though functions are objects in JavaScript).
Otherwise, returns false.


Returns true if the given object is a primitive type. Otherwise, returns
false.


Returns true if the given object is a RegExp. Otherwise, returns false.


Returns true if the given object is a string. Otherwise, returns false.


Returns true if the given object is a Symbol. Otherwise, returns false.


Returns true if the given object is undefined. Otherwise, returns false.


The util.log() method prints the given string to stdout with an included
timestamp.
Source Code: lib/v8.js
The node:v8 module exposes APIs that are specific to the version of V8
built into the Node.js binary. It can be accessed using:
Returns an integer representing a version tag derived from the V8 version,
command-line flags, and detected CPU features. This is useful for determining
whether a vm.Script cachedData buffer is compatible with this instance
of V8.
Get statistics about code and its metadata in the heap, see V8
GetHeapCodeAndMetadataStatistics API. Returns an object with the
following properties:
Support options to configure the heap snapshot.
Added in: v11.13.0
options <Object>
Returns: <stream.Readable> A Readable containing the V8 heap snapshot.
Generates a snapshot of the current V8 heap and returns a Readable
Stream that may be used to read the JSON serialized representation.
This JSON stream format is intended to be used with tools such as
Chrome DevTools. The JSON schema is undocumented and specific to the
V8 engine. Therefore, the schema may change from one version of V8 to the next.
Creating a heap snapshot requires memory about twice the size of the heap at
the time the snapshot is created. This results in the risk of OOM killers
terminating the process.
Generating a snapshot is a synchronous operation which blocks the event loop
for a duration depending on the heap size.
Support values exceeding the 32-bit unsigned integer range.
Added in: v6.0.0
Returns statistics about the V8 heap spaces, i.e. the segments which make up
the V8 heap. Neither the ordering of heap spaces, nor the availability of a
heap space can be guaranteed as the statistics are provided via the V8
GetHeapSpaceStatistics function and may change from one V8 version to the
next.
The value returned is an array of objects containing the following properties:
Support values exceeding the 32-bit unsigned integer range.
Added malloced_memory, peak_malloced_memory, and does_zap_garbage.
Added in: v1.0.0
Returns an object with the following properties:
does_zap_garbage is a 0/1 boolean, which signifies whether the
--zap_code_space option is enabled or not. This makes V8 overwrite heap
garbage with a bit pattern. The RSS footprint (resident set size) gets bigger
because it continuously touches all heap pages and that makes them less likely
to get swapped out by the operating system.
number_of_native_contexts The value of native_context is the number of the
top-level contexts currently active. Increase of this number over time indicates
a memory leak.
number_of_detached_contexts The value of detached_context is the number
of contexts that were detached and not yet garbage collected. This number
being non-zero indicates a potential memory leak.
total_global_handles_size The value of total_global_handles_size is the
total memory size of V8 global handles.
used_global_handles_size The value of used_global_handles_size is the
used memory size of V8 global handles.
external_memory The value of external_memory is the memory size of array
buffers and external strings.
The v8.setFlagsFromString() method can be used to programmatically set
V8 command-line flags. This method should be used with care. Changing settings
after the VM has started may result in unpredictable behavior, including
crashes and data loss; or it may simply do nothing.
The V8 options available for a version of Node.js may be determined by running
node --v8-options.
Usage:
The v8.stopCoverage() method allows the user to stop the coverage collection
started by NODE_V8_COVERAGE, so that V8 can release the execution count
records and optimize code. This can be used in conjunction with
v8.takeCoverage() if the user wants to collect the coverage on demand.
The v8.takeCoverage() method allows the user to write the coverage started by
NODE_V8_COVERAGE to disk on demand. This method can be invoked multiple
times during the lifetime of the process. Each time the execution counter will
be reset and a new coverage report will be written to the directory specified
by NODE_V8_COVERAGE.
When the process is about to exit, one last coverage will still be written to
disk unless v8.stopCoverage() is invoked before the process exits.
Support options to configure the heap snapshot.
An exception will now be thrown if the file could not be written.
Make the returned error codes consistent across all platforms.
Added in: v11.13.0
Generates a snapshot of the current V8 heap and writes it to a JSON
file. This file is intended to be used with tools such as Chrome
DevTools. The JSON schema is undocumented and specific to the V8
engine, and may change from one version of V8 to the next.
A heap snapshot is specific to a single V8 isolate. When using
worker threads, a heap snapshot generated from the main thread will
not contain any information about the workers, and vice versa.
Creating a heap snapshot requires memory about twice the size of the heap at
the time the snapshot is created. This results in the risk of OOM killers
terminating the process.
Generating a snapshot is a synchronous operation which blocks the event loop
for a duration depending on the heap size.


The API is a no-op if --heapsnapshot-near-heap-limit is already set from the
command line or the API is called more than once. limit must be a positive
integer. See --heapsnapshot-near-heap-limit for more information.
The serialization API provides means of serializing JavaScript values in a way
that is compatible with the HTML structured clone algorithm.
The format is backward-compatible (i.e. safe to store to disk).
Equal JavaScript values may result in different serialized output.
Uses a DefaultSerializer to serialize value into a buffer.
ERR_BUFFER_TOO_LARGE will be thrown when trying to
serialize a huge object which requires buffer
larger than buffer.constants.MAX_LENGTH.
Uses a DefaultDeserializer with default options to read a JS value
from a buffer.
Creates a new Serializer object.
Writes out a header, which includes the serialization format version.
Serializes a JavaScript value and adds the serialized representation to the
internal buffer.
This throws an error if value cannot be serialized.
Returns the stored internal buffer. This serializer should not be used once
the buffer is released. Calling this method results in undefined behavior
if a previous write has failed.
Marks an ArrayBuffer as having its contents transferred out of band.
Pass the corresponding ArrayBuffer in the deserializing context to
deserializer.transferArrayBuffer().
Write a raw 32-bit unsigned integer.
For use inside of a custom serializer._writeHostObject().
Write a raw 64-bit unsigned integer, split into high and low 32-bit parts.
For use inside of a custom serializer._writeHostObject().
Write a JS number value.
For use inside of a custom serializer._writeHostObject().
Write raw bytes into the serializer's internal buffer. The deserializer
will require a way to compute the length of the buffer.
For use inside of a custom serializer._writeHostObject().
This method is called to write some kind of host object, i.e. an object created
by native C++ bindings. If it is not possible to serialize object, a suitable
exception should be thrown.
This method is not present on the Serializer class itself but can be provided
by subclasses.
This method is called to generate error objects that will be thrown when an
object can not be cloned.
This method defaults to the Error constructor and can be overridden on
subclasses.
This method is called when the serializer is going to serialize a
SharedArrayBuffer object. It must return an unsigned 32-bit integer ID for
the object, using the same ID if this SharedArrayBuffer has already been
serialized. When deserializing, this ID will be passed to
deserializer.transferArrayBuffer().
If the object cannot be serialized, an exception should be thrown.
This method is not present on the Serializer class itself but can be provided
by subclasses.
Indicate whether to treat TypedArray and DataView objects as
host objects, i.e. pass them to serializer._writeHostObject().
Creates a new Deserializer object.
Reads and validates a header (including the format version).
May, for example, reject an invalid or unsupported wire format. In that case,
an Error is thrown.
Deserializes a JavaScript value from the buffer and returns it.
Marks an ArrayBuffer as having its contents transferred out of band.
Pass the corresponding ArrayBuffer in the serializing context to
serializer.transferArrayBuffer() (or return the id from
serializer._getSharedArrayBufferId() in the case of SharedArrayBuffers).
Reads the underlying wire format version. Likely mostly to be useful to
legacy code reading old wire format versions. May not be called before
.readHeader().
Read a raw 32-bit unsigned integer and return it.
For use inside of a custom deserializer._readHostObject().
Read a raw 64-bit unsigned integer and return it as an array [hi, lo]
with two 32-bit unsigned integer entries.
For use inside of a custom deserializer._readHostObject().
Read a JS number value.
For use inside of a custom deserializer._readHostObject().
Read raw bytes from the deserializer's internal buffer. The length parameter
must correspond to the length of the buffer that was passed to
serializer.writeRawBytes().
For use inside of a custom deserializer._readHostObject().
This method is called to read some kind of host object, i.e. an object that is
created by native C++ bindings. If it is not possible to deserialize the data,
a suitable exception should be thrown.
This method is not present on the Deserializer class itself but can be
provided by subclasses.
A subclass of Serializer that serializes TypedArray
(in particular Buffer) and DataView objects as host objects, and only
stores the part of their underlying ArrayBuffers that they are referring to.
A subclass of Deserializer corresponding to the format written by
DefaultSerializer.
The promiseHooks interface can be used to track promise lifecycle events.
To track all async activity, see async_hooks which internally uses this
module to produce promise lifecycle events in addition to events for other
async resources. For request context management, see AsyncLocalStorage.
The init hook must be a plain function. Providing an async function will
throw as it would produce an infinite microtask loop.
The settled hook must be a plain function. Providing an async function will
throw as it would produce an infinite microtask loop.
The before hook must be a plain function. Providing an async function will
throw as it would produce an infinite microtask loop.
The after hook must be a plain function. Providing an async function will
throw as it would produce an infinite microtask loop.
The hook callbacks must be plain functions. Providing async functions will
throw as it would produce an infinite microtask loop.
Registers functions to be called for different lifetime events of each promise.
The callbacks init()/before()/after()/settled() are called for the
respective events during a promise's lifetime.
All callbacks are optional. For example, if only promise creation needs to
be tracked, then only the init callback needs to be passed. The
specifics of all functions that can be passed to callbacks is in the
Hook Callbacks section.
Key events in the lifetime of a promise have been categorized into four areas:
creation of a promise, before/after a continuation handler is called or around
an await, and when the promise resolves or rejects.
While these hooks are similar to those of async_hooks they lack a
destroy hook. Other types of async resources typically represent sockets or
file descriptors which have a distinct "closed" state to express the destroy
lifecycle event while promises remain usable for as long as code can still
reach them. Garbage collection tracking is used to make promises fit into the
async_hooks event model, however this tracking is very expensive and they may
not necessarily ever even be garbage collected.
Because promises are asynchronous resources whose lifecycle is tracked
via the promise hooks mechanism, the init(), before(), after(), and
settled() callbacks must not be async functions as they create more
promises which would produce an infinite loop.
While this API is used to feed promise events into async_hooks, the
ordering between the two is undefined. Both APIs are multi-tenant
and therefore could produce events in any order relative to each other.
Called when a promise is constructed. This does not mean that corresponding
before/after events will occur, only that the possibility exists. This will
happen if a promise is created without ever getting a continuation.
Called before a promise continuation executes. This can be in the form of
then(), catch(), or finally() handlers or an await resuming.
The before callback will be called 0 to N times. The before callback
will typically be called 0 times if no continuation was ever made for the
promise. The before callback may be called many times in the case where
many continuations have been made from the same promise.
Called immediately after a promise continuation executes. This may be after a
then(), catch(), or finally() handler or before an await after another
await.
Called when the promise receives a resolution or rejection value. This may
occur synchronously in the case of Promise.resolve() or Promise.reject().


The v8.startupSnapshot interface can be used to add serialization and
deserialization hooks for custom startup snapshots.
In the example above, entry.js can use methods from the v8.startupSnapshot
interface to specify how to save information for custom objects in the snapshot
during serialization and how the information can be used to synchronize these
objects during deserialization of the snapshot. For example, if the entry.js
contains the following script:
The resulted binary will get print the data deserialized from the snapshot
during start up, using the refreshed process.env and process.argv of
the launched process:
Currently the application deserialized from a user-land snapshot cannot
be snapshotted again, so these APIs are only available to applications
that are not deserialized from a user-land snapshot.
Add a callback that will be called when the Node.js instance is about to
get serialized into a snapshot and exit. This can be used to release
resources that should not or cannot be serialized or to convert user data
into a form more suitable for serialization.
Add a callback that will be called when the Node.js instance is deserialized
from a snapshot. The callback and the data (if provided) will be
serialized into the snapshot, they can be used to re-initialize the state
of the application or to re-acquire resources that the application needs
when the application is restarted from the snapshot.
This sets the entry point of the Node.js application when it is deserialized
from a snapshot. This can be called only once in the snapshot building
script. If called, the deserialized application no longer needs an additional
entry point script to start up and will simply invoke the callback along with
the deserialized data (if provided), otherwise an entry point script still
needs to be provided to the deserialized application.
Returns true if the Node.js instance is run to build a snapshot.
This API collects GC data in current thread.
Create a new instance of the v8.GCProfiler class.
Start collecting GC data.
Stop collecting GC data and return an object.The content of object
is as follows.
Here's an example.


Source Code: lib/vm.js
The node:vm module enables compiling and running code within V8 Virtual
Machine contexts.
The node:vm module is not a security
mechanism. Do not use it to run untrusted code.
JavaScript code can be compiled and run immediately or
compiled, saved, and run later.
A common use case is to run the code in a different V8 Context. This means
invoked code has a different global object than the invoking code.
One can provide the context by contextifying an
object. The invoked code treats any property in the context like a
global variable. Any changes to global variables caused by the invoked
code are reflected in the context object.
Instances of the vm.Script class contain precompiled scripts that can be
executed in specific contexts.
Added support for import attributes to the importModuleDynamically parameter.
The produceCachedData is deprecated in favour of script.createCachedData().
The cachedData and produceCachedData options are supported now.
Added in: v0.3.1
If options is a string, then it specifies the filename.
Creating a new vm.Script object compiles code but does not run it. The
compiled vm.Script can be run later multiple times. The code is not bound to
any global object; rather, it is bound before each run, just for that run.
When cachedData is supplied to create the vm.Script, this value will be set
to either true or false depending on acceptance of the data by V8.
Otherwise the value is undefined.
Creates a code cache that can be used with the Script constructor's
cachedData option. Returns a Buffer. This method may be called at any
time and any number of times.
The code cache of the Script doesn't contain any JavaScript observable
states. The code cache is safe to be saved along side the script source and
used to construct new Script instances multiple times.
Functions in the Script source can be marked as lazily compiled and they are
not compiled at construction of the Script. These functions are going to be
compiled when they are invoked the first time. The code cache serializes the
metadata that V8 currently knows about the Script that it can use to speed up
future compilations.
The breakOnSigint option is supported now.
Added in: v0.3.1
Runs the compiled code contained by the vm.Script object within the given
contextifiedObject and returns the result. Running code does not have access
to local scope.
The following example compiles code that increments a global variable, sets
the value of another global variable, then execute the code multiple times.
The globals are contained in the context object.
Using the timeout or breakOnSigint options will result in new event loops
and corresponding threads being started, which have a non-zero performance
overhead.
The microtaskMode option is supported now.
The contextCodeGeneration option is supported now.
The breakOnSigint option is supported now.
Added in: v0.3.1
First contextifies the given contextObject, runs the compiled code contained
by the vm.Script object within the created context, and returns the result.
Running code does not have access to local scope.
The following example compiles code that sets a global variable, then executes
the code multiple times in different contexts. The globals are set on and
contained within each individual context.
The breakOnSigint option is supported now.
Added in: v0.3.1
Runs the compiled code contained by the vm.Script within the context of the
current global object. Running code does not have access to local scope, but
does have access to the current global object.
The following example compiles code that increments a global variable then
executes that code multiple times:
When the script is compiled from a source that contains a source map magic
comment, this property will be set to the URL of the source map.


This feature is only available with the --experimental-vm-modules command
flag enabled.
The vm.Module class provides a low-level interface for using
ECMAScript modules in VM contexts. It is the counterpart of the vm.Script
class that closely mirrors Module Records as defined in the ECMAScript
specification.
Unlike vm.Script however, every vm.Module object is bound to a context from
its creation. Operations on vm.Module objects are intrinsically asynchronous,
in contrast with the synchronous nature of vm.Script objects. The use of
'async' functions can help with manipulating vm.Module objects.
Using a vm.Module object requires three distinct steps: creation/parsing,
linking, and evaluation. These three steps are illustrated in the following
example.
This implementation lies at a lower level than the ECMAScript Module
loader. There is also no way to interact with the Loader yet, though
support is planned.
The specifiers of all dependencies of this module. The returned array is frozen
to disallow any changes to it.
Corresponds to the [[RequestedModules]] field of Cyclic Module Records in
the ECMAScript specification.
If the module.status is 'errored', this property contains the exception
thrown by the module during evaluation. If the status is anything else,
accessing this property will result in a thrown exception.
The value undefined cannot be used for cases where there is not a thrown
exception due to possible ambiguity with throw undefined;.
Corresponds to the [[EvaluationError]] field of Cyclic Module Records
in the ECMAScript specification.
Evaluate the module.
This must be called after the module has been linked; otherwise it will reject.
It could be called also when the module has already been evaluated, in which
case it will either do nothing if the initial evaluation ended in success
(module.status is 'evaluated') or it will re-throw the exception that the
initial evaluation resulted in (module.status is 'errored').
This method cannot be called while the module is being evaluated
(module.status is 'evaluating').
Corresponds to the Evaluate() concrete method field of Cyclic Module
Records in the ECMAScript specification.
The identifier of the current module, as set in the constructor.
The option extra.assert is renamed to extra.attributes. The former name is still provided for backward compatibility.
specifier <string> The specifier of the requested module:
referencingModule <vm.Module> The Module object link() is called on.
extra <Object>
Returns: <vm.Module> | <Promise>
Link module dependencies. This method must be called before evaluation, and
can only be called once per module.
The function is expected to return a Module object or a Promise that
eventually resolves to a Module object. The returned Module must satisfy the
following two invariants:
If the returned Module's status is 'unlinked', this method will be
recursively called on the returned Module with the same provided linker
function.
link() returns a Promise that will either get resolved when all linking
instances resolve to a valid Module, or rejected if the linker function either
throws an exception or returns an invalid Module.
The linker function roughly corresponds to the implementation-defined
HostResolveImportedModule abstract operation in the ECMAScript
specification, with a few key differences:
The actual HostResolveImportedModule implementation used during module
linking is one that returns the modules linked during linking. Since at
that point all modules would have been fully linked already, the
HostResolveImportedModule implementation is fully synchronous per
specification.
Corresponds to the Link() concrete method field of Cyclic Module
Records in the ECMAScript specification.
The namespace object of the module. This is only available after linking
(module.link()) has completed.
Corresponds to the GetModuleNamespace abstract operation in the ECMAScript
specification.
The current status of the module. Will be one of:
'unlinked': module.link() has not yet been called.
'linking': module.link() has been called, but not all Promises returned
by the linker function have been resolved yet.
'linked': The module has been linked successfully, and all of its
dependencies are linked, but module.evaluate() has not yet been called.
'evaluating': The module is being evaluated through a module.evaluate() on
itself or a parent module.
'evaluated': The module has been successfully evaluated.
'errored': The module has been evaluated, but an exception was thrown.
Other than 'errored', this status string corresponds to the specification's
Cyclic Module Record's [[Status]] field. 'errored' corresponds to
'evaluated' in the specification, but with [[EvaluationError]] set to a
value that is not undefined.


This feature is only available with the --experimental-vm-modules command
flag enabled.
The vm.SourceTextModule class provides the Source Text Module Record as
defined in the ECMAScript specification.
Added support for import attributes to the importModuleDynamically parameter.
Creates a new SourceTextModule instance.
Properties assigned to the import.meta object that are objects may
allow the module to access information outside the specified context. Use
vm.runInContext() to create objects in a specific context.
Creates a code cache that can be used with the SourceTextModule constructor's
cachedData option. Returns a Buffer. This method may be called any number
of times before the module has been evaluated.
The code cache of the SourceTextModule doesn't contain any JavaScript
observable states. The code cache is safe to be saved along side the script
source and used to construct new SourceTextModule instances multiple times.
Functions in the SourceTextModule source can be marked as lazily compiled
and they are not compiled at construction of the SourceTextModule. These
functions are going to be compiled when they are invoked the first time. The
code cache serializes the metadata that V8 currently knows about the
SourceTextModule that it can use to speed up future compilations.


This feature is only available with the --experimental-vm-modules command
flag enabled.
The vm.SyntheticModule class provides the Synthetic Module Record as
defined in the WebIDL specification. The purpose of synthetic modules is to
provide a generic interface for exposing non-JavaScript sources to ECMAScript
module graphs.
Creates a new SyntheticModule instance.
Objects assigned to the exports of this instance may allow importers of
the module to access information outside the specified context. Use
vm.runInContext() to create objects in a specific context.
This method is used after the module is linked to set the values of exports. If
it is called before the module is linked, an ERR_VM_MODULE_STATUS error
will be thrown.
The return value now includes cachedDataRejected with the same semantics as the vm.Script version if the cachedData option was passed.
Added support for import attributes to the importModuleDynamically parameter.
Added importModuleDynamically option again.
Removal of importModuleDynamically due to compatibility issues.
The importModuleDynamically option is now supported.
Added in: v10.10.0
Compiles the given code into the provided context (if no context is
supplied, the current context is used), and returns it wrapped inside a
function with the given params.
The importModuleDynamically option is supported now.
The microtaskMode option is supported now.
The first argument can no longer be a function.
The codeGeneration option is supported now.
Added in: v0.3.1
If given a contextObject, the vm.createContext() method will prepare
that object so that it can be used in calls to
vm.runInContext() or script.runInContext(). Inside such scripts,
the contextObject will be the global object, retaining all of its existing
properties but also having the built-in objects and functions any standard
global object has. Outside of scripts run by the vm module, global variables
will remain unchanged.
If contextObject is omitted (or passed explicitly as undefined), a new,
empty contextified object will be returned.
The vm.createContext() method is primarily useful for creating a single
context that can be used to run multiple scripts. For instance, if emulating a
web browser, the method can be used to create a single context representing a
window's global object, then run all <script> tags together within that
context.
The provided name and origin of the context are made visible through the
Inspector API.
Returns true if the given object object has been contextified using
vm.createContext().


Measure the memory known to V8 and used by all contexts known to the
current V8 isolate, or the main context.
The format of the object that the returned Promise may resolve with is
specific to the V8 engine and may change from one version of V8 to the next.
The returned result is different from the statistics returned by
v8.getHeapSpaceStatistics() in that vm.measureMemory() measure the
memory reachable by each V8 specific contexts in the current instance of
the V8 engine, while the result of v8.getHeapSpaceStatistics() measure
the memory occupied by each heap space in the current V8 instance.
Added support for import attributes to the importModuleDynamically parameter.
The breakOnSigint option is supported now.
Added in: v0.3.1
The vm.runInContext() method compiles code, runs it within the context of
the contextifiedObject, then returns the result. Running code does not have
access to the local scope. The contextifiedObject object must have been
previously contextified using the vm.createContext() method.
If options is a string, then it specifies the filename.
The following example compiles and executes different scripts using a single
contextified object:
Added support for import attributes to the importModuleDynamically parameter.
The microtaskMode option is supported now.
The contextCodeGeneration option is supported now.
The breakOnSigint option is supported now.
Added in: v0.3.1
The vm.runInNewContext() first contextifies the given contextObject (or
creates a new contextObject if passed as undefined), compiles the code,
runs it within the created context, then returns the result. Running code
does not have access to the local scope.
If options is a string, then it specifies the filename.
The following example compiles and executes code that increments a global
variable and sets a new one. These globals are contained in the contextObject.
Added support for import attributes to the importModuleDynamically parameter.
The breakOnSigint option is supported now.
Added in: v0.3.1
vm.runInThisContext() compiles code, runs it within the context of the
current global and returns the result. Running code does not have access to
local scope, but does have access to the current global object.
If options is a string, then it specifies the filename.
The following example illustrates using both vm.runInThisContext() and
the JavaScript eval() function to run the same code:
Because vm.runInThisContext() does not have access to the local scope,
localVar is unchanged. In contrast, eval() does have access to the
local scope, so the value localVar is changed. In this way
vm.runInThisContext() is much like an indirect eval() call, e.g.
(0,eval)('code').
When using either script.runInThisContext() or
vm.runInThisContext(), the code is executed within the current V8 global
context. The code passed to this VM context will have its own isolated scope.
In order to run a simple web server using the node:http module the code passed
to the context must either call require('node:http') on its own, or have a
reference to the node:http module passed to it. For instance:
The require() in the above case shares the state with the context it is
passed from. This may introduce risks when untrusted code is executed, e.g.
altering objects in the context in unwanted ways.
All JavaScript executed within Node.js runs within the scope of a "context".
According to the V8 Embedder's Guide:
In V8, a context is an execution environment that allows separate, unrelated,
JavaScript applications to run in a single instance of V8. You must explicitly
specify the context in which you want any JavaScript code to be run.
When the method vm.createContext() is called, the contextObject argument
(or a newly-created object if contextObject is undefined) is associated
internally with a new instance of a V8 Context. This V8 Context provides the
code run using the node:vm module's methods with an isolated global
environment within which it can operate. The process of creating the V8 Context
and associating it with the contextObject is what this document refers to as
"contextifying" the object.
Promises and async functions can schedule tasks run by the JavaScript
engine asynchronously. By default, these tasks are run after all JavaScript
functions on the current stack are done executing.
This allows escaping the functionality of the timeout and
breakOnSigint options.
For example, the following code executed by vm.runInNewContext() with a
timeout of 5 milliseconds schedules an infinite loop to run after a promise
resolves. The scheduled loop is never interrupted by the timeout:
This can be addressed by passing microtaskMode: 'afterEvaluate' to the code
that creates the Context:
In this case, the microtask scheduled through promise.then() will be run
before returning from vm.runInNewContext(), and will be interrupted
by the timeout functionality. This applies only to code running in a
vm.Context, so e.g. vm.runInThisContext() does not take this option.
Promise callbacks are entered into the microtask queue of the context in which
they were created. For example, if () => loop() is replaced with just loop
in the above example, then loop will be pushed into the global microtask
queue, because it is a function from the outer (main) context, and thus will
also be able to escape the timeout.
If asynchronous scheduling functions such as process.nextTick(),
queueMicrotask(), setTimeout(), setImmediate(), etc. are made available
inside a vm.Context, functions passed to them will be added to global queues,
which are shared by all contexts. Therefore, callbacks passed to those functions
are not controllable through the timeout either.


The node:wasi module does not currently provide the
comprehensive file system security properties provided by some WASI runtimes.
Full support for secure file system sandboxing may or may not be implemented in
future. In the mean time, do not rely on it to run untrusted code. 
Source Code: lib/wasi.js
The WASI API provides an implementation of the WebAssembly System Interface
specification. WASI gives WebAssembly applications access to the underlying
operating system via a collection of POSIX-like functions.
To run the above example, create a new WebAssembly text format file named
demo.wat:
Use wabt to compile .wat to .wasm
Clarify WASI security properties.
Added in: v20.11.0
WASI provides a capabilities-based model through which applications are provided
their own custom env, preopens, stdin, stdout, stderr, and exit
capabilities.
The current Node.js threat model does not provide secure sandboxing as is
present in some WASI runtimes.
While the capability features are supported, they do not form a security model
in Node.js. For example, the file system sandboxing can be escaped with various
techniques. The project is exploring whether these security guarantees could be
added in future.
The WASI class provides the WASI system call API and additional convenience
methods for working with WASI-based applications. Each WASI instance
represents a distinct environment.
default value of returnOnExit changed to true.
The version option is now required and has no default value.
version field added to options.
Added in: v13.3.0, v12.16.0
Return an import object that can be passed to WebAssembly.instantiate() if
no other WASM imports are needed beyond those provided by WASI.
If version unstable was passed into the constructor it will return:
If version preview1 was passed into the constructor or no version was
specified it will return:
Attempt to begin execution of instance as a WASI command by invoking its
_start() export. If instance does not contain a _start() export, or if
instance contains an _initialize() export, then an exception is thrown.
start() requires that instance exports a WebAssembly.Memory named
memory. If instance does not have a memory export an exception is thrown.
If start() is called more than once, an exception is thrown.
Attempt to initialize instance as a WASI reactor by invoking its
_initialize() export, if it is present. If instance contains a _start()
export, then an exception is thrown.
initialize() requires that instance exports a WebAssembly.Memory named
memory. If instance does not have a memory export an exception is thrown.
If initialize() is called more than once, an exception is thrown.
wasiImport is an object that implements the WASI system call API. This object
should be passed as the wasi_snapshot_preview1 import during the instantiation
of a WebAssembly.Instance.
Arguments are now coerced and validated as per their WebIDL definitions like in other Web Crypto API implementations.
No longer experimental except for the Ed25519, Ed448, X25519, and X448 algorithms.
Removed proprietary 'node.keyObject' import/export format.
Removed proprietary 'NODE-DSA', 'NODE-DH', and 'NODE-SCRYPT' algorithms.
Added 'Ed25519', 'Ed448', 'X25519', and 'X448' algorithms.
Removed proprietary 'NODE-ED25519' and 'NODE-ED448' algorithms.
Removed proprietary 'NODE-X25519' and 'NODE-X448' named curves from the 'ECDH' algorithm.


Node.js provides an implementation of the standard Web Crypto API.
Use globalThis.crypto or require('node:crypto').webcrypto to access this
module.
The <SubtleCrypto> class can be used to generate symmetric (secret) keys
or asymmetric key pairs (public key and private key).


The table details the algorithms supported by the Node.js Web Crypto API
implementation and the APIs supported for each:
globalThis.crypto is an instance of the Crypto
class. Crypto is a singleton that provides access to the remainder of the
crypto API.
Provides access to the SubtleCrypto API.
Generates cryptographically strong random values. The given typedArray is
filled with random values, and a reference to typedArray is returned.
The given typedArray must be an integer-based instance of <TypedArray>,
i.e. Float32Array and Float64Array are not accepted.
An error will be thrown if the given typedArray is larger than 65,536 bytes.
Generates a random RFC 4122 version 4 UUID. The UUID is generated using a
cryptographic pseudorandom number generator.
An object detailing the algorithm for which the key can be used along with
additional algorithm-specific parameters.
Read-only.
When true, the <CryptoKey> can be extracted using either
subtleCrypto.exportKey() or subtleCrypto.wrapKey().
Read-only.
A string identifying whether the key is a symmetric ('secret') or
asymmetric ('private' or 'public') key.
An array of strings identifying the operations for which the
key may be used.
The possible usages are:
Valid key usages depend on the key algorithm (identified by
cryptokey.algorithm.name).
The CryptoKeyPair is a simple dictionary object with publicKey and
privateKey properties, representing an asymmetric key pair.
Using the method and parameters specified in algorithm and the keying
material provided by key, subtle.decrypt() attempts to decipher the
provided data. If successful, the returned promise will be resolved with
an <ArrayBuffer> containing the plaintext result.
The algorithms currently supported include:
Added 'X25519', and 'X448' algorithms.
Added in: v15.0.0
Using the method and parameters specified in algorithm and the keying
material provided by baseKey, subtle.deriveBits() attempts to generate
length bits.
The Node.js implementation requires that when length is a
number it must be multiple of 8.
When length is null the maximum number of bits for a given algorithm is
generated. This is allowed for the 'ECDH', 'X25519', and 'X448'
algorithms.
If successful, the returned promise will be resolved with an <ArrayBuffer>
containing the generated data.
The algorithms currently supported include:
Added 'X25519', and 'X448' algorithms.
Added in: v15.0.0
Using the method and parameters specified in algorithm, and the keying
material provided by baseKey, subtle.deriveKey() attempts to generate
a new <CryptoKey> based on the method and parameters in derivedKeyAlgorithm.
Calling subtle.deriveKey() is equivalent to calling subtle.deriveBits() to
generate raw keying material, then passing the result into the
subtle.importKey() method using the deriveKeyAlgorithm, extractable, and
keyUsages parameters as input.
The algorithms currently supported include:
Using the method identified by algorithm, subtle.digest() attempts to
generate a digest of data. If successful, the returned promise is resolved
with an <ArrayBuffer> containing the computed digest.
If algorithm is provided as a <string>, it must be one of:
If algorithm is provided as an <Object>, it must have a name property
whose value is one of the above.
Using the method and parameters specified by algorithm and the keying
material provided by key, subtle.encrypt() attempts to encipher data.
If successful, the returned promise is resolved with an <ArrayBuffer>
containing the encrypted result.
The algorithms currently supported include:
Added 'Ed25519', 'Ed448', 'X25519', and 'X448' algorithms.
Removed 'NODE-DSA' JWK export.
Added in: v15.0.0
Exports the given key into the specified format, if supported.
If the <CryptoKey> is not extractable, the returned promise will reject.
When format is either 'pkcs8' or 'spki' and the export is successful,
the returned promise will be resolved with an <ArrayBuffer> containing the
exported key data.
When format is 'jwk' and the export is successful, the returned promise
will be resolved with a JavaScript object conforming to the JSON Web Key
specification.
Using the method and parameters provided in algorithm, subtle.generateKey()
attempts to generate new keying material. Depending the method used, the method
may generate either a single <CryptoKey> or a <CryptoKeyPair>.
The <CryptoKeyPair> (public and private key) generating algorithms supported
include:
The <CryptoKey> (secret key) generating algorithms supported include:
Added 'Ed25519', 'Ed448', 'X25519', and 'X448' algorithms.
Removed 'NODE-DSA' JWK import.
Added in: v15.0.0
The subtle.importKey() method attempts to interpret the provided keyData
as the given format to create a <CryptoKey> instance using the provided
algorithm, extractable, and keyUsages arguments. If the import is
successful, the returned promise will be resolved with the created <CryptoKey>.
If importing a 'PBKDF2' key, extractable must be false.
The algorithms currently supported include:
Added 'Ed25519', and 'Ed448' algorithms.
Added in: v15.0.0
Using the method and parameters given by algorithm and the keying material
provided by key, subtle.sign() attempts to generate a cryptographic
signature of data. If successful, the returned promise is resolved with
an <ArrayBuffer> containing the generated signature.
The algorithms currently supported include:
In cryptography, "wrapping a key" refers to exporting and then encrypting the
keying material. The subtle.unwrapKey() method attempts to decrypt a wrapped
key and create a <CryptoKey> instance. It is equivalent to calling
subtle.decrypt() first on the encrypted key data (using the wrappedKey,
unwrapAlgo, and unwrappingKey arguments as input) then passing the results
in to the subtle.importKey() method using the unwrappedKeyAlgo,
extractable, and keyUsages arguments as inputs. If successful, the returned
promise is resolved with a <CryptoKey> object.
The wrapping algorithms currently supported include:
The unwrapped key algorithms supported include:
Added 'Ed25519', and 'Ed448' algorithms.
Added in: v15.0.0
Using the method and parameters given in algorithm and the keying material
provided by key, subtle.verify() attempts to verify that signature is
a valid cryptographic signature of data. The returned promise is resolved
with either true or false.
The algorithms currently supported include:
In cryptography, "wrapping a key" refers to exporting and then encrypting the
keying material. The subtle.wrapKey() method exports the keying material into
the format identified by format, then encrypts it using the method and
parameters specified by wrapAlgo and the keying material provided by
wrappingKey. It is the equivalent to calling subtle.exportKey() using
format and key as the arguments, then passing the result to the
subtle.encrypt() method using wrappingKey and wrapAlgo as inputs. If
successful, the returned promise will be resolved with an <ArrayBuffer>
containing the encrypted key data.
The wrapping algorithms currently supported include:
The algorithm parameter objects define the methods and parameters used by
the various <SubtleCrypto> methods. While described here as "classes", they
are simple JavaScript dictionary objects.
Provides the initialization vector. It must be exactly 16-bytes in length
and should be unpredictable and cryptographically random.
The initial value of the counter block. This must be exactly 16 bytes long.
The AES-CTR method uses the rightmost length bits of the block as the
counter and the remaining bits as the nonce.
With the AES-GCM method, the additionalData is extra input that is not
encrypted but is included in the authentication of the data. The use of
additionalData is optional.
The initialization vector must be unique for every encryption operation using a
given key.
Ideally, this is a deterministic 12-byte value that is computed in such a way
that it is guaranteed to be unique across all invocations that use the same key.
Alternatively, the initialization vector may consist of at least 12
cryptographically random bytes. For more information on constructing
initialization vectors for AES-GCM, refer to Section 8 of NIST SP 800-38D.
The length of the AES key to be generated. This must be either 128, 192,
or 256.
ECDH key derivation operates by taking as input one parties private key and
another parties public key -- using both to generate a common shared secret.
The ecdhKeyDeriveParams.public property is set to the other parties public
key.
If represented as a <string>, the value must be one of:
If represented as an <Object>, the object must have a name property
whose value is one of the above listed values.
The context member represents the optional context data to associate with
the message.
The Node.js Web Crypto API implementation only supports zero-length context
which is equivalent to not providing context at all.
If represented as a <string>, the value must be one of:
If represented as an <Object>, the object must have a name property
whose value is one of the above listed values.
Provides application-specific contextual input to the HKDF algorithm.
This can be zero-length but must be provided.
The salt value significantly improves the strength of the HKDF algorithm.
It should be random or pseudorandom and should be the same length as the
output of the digest function (for instance, if using 'SHA-256' as the
digest, the salt should be 256-bits of random data).
If represented as a <string>, the value must be one of:
If represented as an <Object>, the object must have a name property
whose value is one of the above listed values.
The optional number of bits in the HMAC key. This is optional and should
be omitted for most cases.
If represented as a <string>, the value must be one of:
If represented as an <Object>, the object must have a name property
whose value is one of the above listed values.
The number of bits to generate for the HMAC key. If omitted,
the length will be determined by the hash algorithm used.
This is optional and should be omitted for most cases.
If represented as a <string>, the value must be one of:
If represented as an <Object>, the object must have a name property
whose value is one of the above listed values.
The number of iterations the PBKDF2 algorithm should make when deriving bits.
Should be at least 16 random or pseudorandom bytes.
If represented as a <string>, the value must be one of:
If represented as an <Object>, the object must have a name property
whose value is one of the above listed values.
If represented as a <string>, the value must be one of:
If represented as an <Object>, the object must have a name property
whose value is one of the above listed values.
The length in bits of the RSA modulus. As a best practice, this should be
at least 2048.
The RSA public exponent. This must be a <Uint8Array> containing a big-endian,
unsigned integer that must fit within 32-bits. The <Uint8Array> may contain an
arbitrary number of leading zero-bits. The value must be a prime number. Unless
there is reason to use a different value, use new Uint8Array([1, 0, 1])
(65537) as the public exponent.
An additional collection of bytes that will not be encrypted, but will be bound
to the generated ciphertext.
The rsaOaepParams.label parameter is optional.
The length (in bytes) of the random salt to use.
An experimental implementation of
Secure Curves in the Web Cryptography API as of 30 August 2023 ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 ‚Ü©7 ‚Ü©8 ‚Ü©9 ‚Ü©10 ‚Ü©11 ‚Ü©12 ‚Ü©13 ‚Ü©14 ‚Ü©15 ‚Ü©16 ‚Ü©17 ‚Ü©18 ‚Ü©19 ‚Ü©20 ‚Ü©21 ‚Ü©22 ‚Ü©23 ‚Ü©24 ‚Ü©25 ‚Ü©26 ‚Ü©27 ‚Ü©28 ‚Ü©29 ‚Ü©30
Use of this API no longer emit a runtime warning.
Added in: v16.5.0


An implementation of the WHATWG Streams Standard.
The WHATWG Streams Standard (or "web streams") defines an API for handling
streaming data. It is similar to the Node.js Streams API but emerged later
and has become the "standard" API for streaming data across many JavaScript
environments.
There are three primary types of objects:
This example creates a simple ReadableStream that pushes the current
performance.now() timestamp once every second forever. An async iterable
is used to read the data from the stream.
This class is now exposed on the global object.
Added in: v16.5.0
The readableStream.locked property is false by default, and is
switched to true while there is an active reader consuming the
stream's data.
Causes the readableStream.locked to be true.
Connects this <ReadableStream> to the pair of <ReadableStream> and
<WritableStream> provided in the transform argument such that the
data from this <ReadableStream> is written in to transform.writable,
possibly transformed, then pushed to transform.readable. Once the
pipeline is configured, transform.readable is returned.
Causes the readableStream.locked to be true while the pipe operation
is active.
Causes the readableStream.locked to be true while the pipe operation
is active.
Support teeing a readable byte stream.
Added in: v16.5.0
Returns a pair of new <ReadableStream> instances to which this
ReadableStream's data will be forwarded. Each will receive the
same data.
Causes the readableStream.locked to be true.
Creates and returns an async iterator usable for consuming this
ReadableStream's data.
Causes the readableStream.locked to be true while the async iterator
is active.
The <ReadableStream> object supports the async iterator protocol using
for await syntax.
The async iterator will consume the <ReadableStream> until it terminates.
By default, if the async iterator exits early (via either a break,
return, or a throw), the <ReadableStream> will be closed. To prevent
automatic closing of the <ReadableStream>, use the readableStream.values()
method to acquire the async iterator and set the preventCancel option to
true.
The <ReadableStream> must not be locked (that is, it must not have an existing
active reader). During the async iteration, the <ReadableStream> will be locked.
A <ReadableStream> instance can be transferred using a <MessagePort>.
A utility method that creates a new <ReadableStream> from an iterable.
This class is now exposed on the global object.
Added in: v16.5.0
By default, calling readableStream.getReader() with no arguments
will return an instance of ReadableStreamDefaultReader. The default
reader treats the chunks of data passed through the stream as opaque
values, which allows the <ReadableStream> to work with generally any
JavaScript value.
Creates a new <ReadableStreamDefaultReader> that is locked to the
given <ReadableStream>.
Cancels the <ReadableStream> and returns a promise that is fulfilled
when the underlying stream has been canceled.
Requests the next chunk of data from the underlying <ReadableStream>
and returns a promise that is fulfilled with the data once it is
available.
Releases this reader's lock on the underlying <ReadableStream>.
This class is now exposed on the global object.
Added in: v16.5.0
The ReadableStreamBYOBReader is an alternative consumer for
byte-oriented <ReadableStream>s (those that are created with
underlyingSource.type set equal to 'bytes' when the
ReadableStream was created).
The BYOB is short for "bring your own buffer". This is a
pattern that allows for more efficient reading of byte-oriented
data that avoids extraneous copying.
Creates a new ReadableStreamBYOBReader that is locked to the
given <ReadableStream>.
Cancels the <ReadableStream> and returns a promise that is fulfilled
when the underlying stream has been canceled.
Requests the next chunk of data from the underlying <ReadableStream>
and returns a promise that is fulfilled with the data once it is
available.
Do not pass a pooled <Buffer> object instance in to this method.
Pooled Buffer objects are created using Buffer.allocUnsafe(),
or Buffer.from(), or are often returned by various node:fs module
callbacks. These types of Buffers use a shared underlying
<ArrayBuffer> object that contains all of the data from all of
the pooled Buffer instances. When a Buffer, <TypedArray>,
or <DataView> is passed in to readableStreamBYOBReader.read(),
the view's underlying ArrayBuffer is detached, invalidating
all existing views that may exist on that ArrayBuffer. This
can have disastrous consequences for your application.
Releases this reader's lock on the underlying <ReadableStream>.
Every <ReadableStream> has a controller that is responsible for
the internal state and management of the stream's queue. The
ReadableStreamDefaultController is the default controller
implementation for ReadableStreams that are not byte-oriented.
Closes the <ReadableStream> to which this controller is associated.
Returns the amount of data remaining to fill the <ReadableStream>'s
queue.
Appends a new chunk of data to the <ReadableStream>'s queue.
Signals an error that causes the <ReadableStream> to error and close.
Support handling a BYOB pull request from a released reader.
Added in: v16.5.0
Every <ReadableStream> has a controller that is responsible for
the internal state and management of the stream's queue. The
ReadableByteStreamController is for byte-oriented ReadableStreams.
Closes the <ReadableStream> to which this controller is associated.
Returns the amount of data remaining to fill the <ReadableStream>'s
queue.
Appends a new chunk of data to the <ReadableStream>'s queue.
Signals an error that causes the <ReadableStream> to error and close.
This class is now exposed on the global object.
Added in: v16.5.0
When using ReadableByteStreamController in byte-oriented
streams, and when using the ReadableStreamBYOBReader,
the readableByteStreamController.byobRequest property
provides access to a ReadableStreamBYOBRequest instance
that represents the current read request. The object
is used to gain access to the ArrayBuffer/TypedArray
that has been provided for the read request to fill,
and provides methods for signaling that the data has
been provided.
Signals that a bytesWritten number of bytes have been written
to readableStreamBYOBRequest.view.
Signals that the request has been fulfilled with bytes written
to a new Buffer, TypedArray, or DataView.
This class is now exposed on the global object.
Added in: v16.5.0
The WritableStream is a destination to which stream data is sent.
Abruptly terminates the WritableStream. All queued writes will be
canceled with their associated promises rejected.
Closes the WritableStream when no additional writes are expected.
Creates and returns a new writer instance that can be used to write
data into the WritableStream.
The writableStream.locked property is false by default, and is
switched to true while there is an active writer attached to this
WritableStream.
A <WritableStream> instance can be transferred using a <MessagePort>.
This class is now exposed on the global object.
Added in: v16.5.0
Creates a new WritableStreamDefaultWriter that is locked to the given
WritableStream.
Abruptly terminates the WritableStream. All queued writes will be
canceled with their associated promises rejected.
Closes the WritableStream when no additional writes are expected.
The amount of data required to fill the <WritableStream>'s queue.
Releases this writer's lock on the underlying <ReadableStream>.
Appends a new chunk of data to the <WritableStream>'s queue.
This class is now exposed on the global object.
Added in: v16.5.0
The WritableStreamDefaultController manage's the <WritableStream>'s
internal state.
Called by user-code to signal that an error has occurred while processing
the WritableStream data. When called, the <WritableStream> will be aborted,
with currently pending writes canceled.
This class is now exposed on the global object.
Added in: v16.5.0
A TransformStream consists of a <ReadableStream> and a <WritableStream> that
are connected such that the data written to the WritableStream is received,
and potentially transformed, before being pushed into the ReadableStream's
queue.
A <TransformStream> instance can be transferred using a <MessagePort>.
This class is now exposed on the global object.
Added in: v16.5.0
The TransformStreamDefaultController manages the internal state
of the TransformStream.
The amount of data required to fill the readable side's queue.
Appends a chunk of data to the readable side's queue.
Signals to both the readable and writable side that an error has occurred
while processing the transform data, causing both sides to be abruptly
closed.
Closes the readable side of the transport and causes the writable side
to be abruptly closed with an error.
This class is now exposed on the global object.
Added in: v16.5.0
This class is now exposed on the global object.
Added in: v16.5.0
This class is now exposed on the global object.
Added in: v16.6.0
Creates a new TextEncoderStream instance.
The encoding supported by the TextEncoderStream instance.
This class is now exposed on the global object.
Added in: v16.6.0
Creates a new TextDecoderStream instance.
The encoding supported by the TextDecoderStream instance.
The value will be true if decoding errors result in a TypeError being
thrown.
The value will be true if the decoding result will include the byte order
mark.
This class is now exposed on the global object.
Added in: v17.0.0
This class is now exposed on the global object.
Added in: v17.0.0
The utility consumer functions provide common options for consuming
streams.
They are accessed using:


Source Code: lib/worker_threads.js
The node:worker_threads module enables the use of threads that execute
JavaScript in parallel. To access it:
Workers (threads) are useful for performing CPU-intensive JavaScript operations.
They do not help much with I/O-intensive work. The Node.js built-in
asynchronous I/O operations are more efficient than Workers can be.
Unlike child_process or cluster, worker_threads can share memory. They do
so by transferring ArrayBuffer instances or sharing SharedArrayBuffer
instances.
The above example spawns a Worker thread for each parseJSAsync() call. In
practice, use a pool of Workers for these kinds of tasks. Otherwise, the
overhead of creating Workers would likely exceed their benefit.
When implementing a worker pool, use the AsyncResource API to inform
diagnostic tools (e.g. to provide asynchronous stack traces) about the
correlation between tasks and their outcomes. See
"Using AsyncResource for a Worker thread pool"
in the async_hooks documentation for an example implementation.
Worker threads inherit non-process-specific options by default. Refer to
Worker constructor options to know how to customize worker thread options,
specifically argv and execArgv options.
No longer experimental.
Added in: v15.12.0, v14.18.0
Within a worker thread, worker.getEnvironmentData() returns a clone
of data passed to the spawning thread's worker.setEnvironmentData().
Every new Worker receives its own copy of the environment data
automatically.
Is true if this code is not running inside of a Worker thread.
Mark an object as not transferable. If object occurs in the transfer list of
a port.postMessage() call, it is ignored.
In particular, this makes sense for objects that can be cloned, rather than
transferred, and which are used by other objects on the sending side.
For example, Node.js marks the ArrayBuffers it uses for its
Buffer pool with this.
This operation cannot be undone.
There is no equivalent to this API in browsers.
port <MessagePort> The message port to transfer.
contextifiedSandbox <Object> A contextified object as returned by the
vm.createContext() method.
Returns: <MessagePort>
Transfer a MessagePort to a different vm Context. The original port
object is rendered unusable, and the returned MessagePort instance
takes its place.
The returned MessagePort is an object in the target context and
inherits from its global Object class. Objects passed to the
port.onmessage() listener are also created in the target context
and inherit from its global Object class.
However, the created MessagePort no longer inherits from
EventTarget, and only port.onmessage() can be used to receive
events using it.
If this thread is a Worker, this is a MessagePort
allowing communication with the parent thread. Messages sent using
parentPort.postMessage() are available in the parent thread
using worker.on('message'), and messages sent from the parent thread
using worker.postMessage() are available in this thread using
parentPort.on('message').
The port argument can also refer to a BroadcastChannel now.
Added in: v12.3.0
port <MessagePort> | <BroadcastChannel>
Returns: <Object> | <undefined>
Receive a single message from a given MessagePort. If no message is available,
undefined is returned, otherwise an object with a single message property
that contains the message payload, corresponding to the oldest message in the
MessagePort's queue.
When this function is used, no 'message' event is emitted and the
onmessage listener is not invoked.
Provides the set of JS engine resource constraints inside this Worker thread.
If the resourceLimits option was passed to the Worker constructor,
this matches its values.
If this is used in the main thread, its value is an empty object.
A special value that can be passed as the env option of the Worker
constructor, to indicate that the current thread and the Worker thread should
share read and write access to the same set of environment variables.
No longer experimental.
Added in: v15.12.0, v14.18.0
The worker.setEnvironmentData() API sets the content of
worker.getEnvironmentData() in the current thread and all new Worker
instances spawned from the current context.
An integer identifier for the current thread. On the corresponding worker object
(if there is any), it is available as worker.threadId.
This value is unique for each Worker instance inside a single process.
An arbitrary JavaScript value that contains a clone of the data passed
to this thread's Worker constructor.
The data is cloned as if using postMessage(),
according to the HTML structured clone algorithm.
No longer experimental.
Added in: v15.4.0
Instances of BroadcastChannel allow asynchronous one-to-many communication
with all other BroadcastChannel instances bound to the same channel name.
Closes the BroadcastChannel connection.
Opposite of unref(). Calling ref() on a previously unref()ed
BroadcastChannel does not let the program exit if it's the only active handle
left (the default behavior). If the port is ref()ed, calling ref() again
has no effect.
Calling unref() on a BroadcastChannel allows the thread to exit if this
is the only active handle in the event system. If the BroadcastChannel is
already unref()ed calling unref() again has no effect.
Instances of the worker.MessageChannel class represent an asynchronous,
two-way communications channel.
The MessageChannel has no methods of its own. new MessageChannel()
yields an object with port1 and port2 properties, which refer to linked
MessagePort instances.
This class now inherits from EventTarget rather than from EventEmitter.
Added in: v10.5.0
Instances of the worker.MessagePort class represent one end of an
asynchronous, two-way communications channel. It can be used to transfer
structured data, memory regions and other MessagePorts between different
Workers.
This implementation matches browser MessagePorts.
The 'close' event is emitted once either side of the channel has been
disconnected.
The 'message' event is emitted for any incoming message, containing the cloned
input of port.postMessage().
Listeners on this event receive a clone of the value parameter as passed
to postMessage() and no further arguments.
The 'messageerror' event is emitted when deserializing a message failed.
Currently, this event is emitted when there is an error occurring while
instantiating the posted JS object on the receiving end. Such situations
are rare, but can happen, for instance, when certain Node.js API objects
are received in a vm.Context (where Node.js APIs are currently
unavailable).
Disables further sending of messages on either side of the connection.
This method can be called when no further communication will happen over this
MessagePort.
The 'close' event is emitted on both MessagePort instances that
are part of the channel.
Added X509Certificate to the list of cloneable types.
Added CryptoKey to the list of cloneable types.
Add 'BlockList' to the list of cloneable types.
Add 'Histogram' types to the list of cloneable types.
Added KeyObject to the list of cloneable types.
Added FileHandle to the list of transferable types.
Added in: v10.5.0
Sends a JavaScript value to the receiving side of this channel.
value is transferred in a way which is compatible with
the HTML structured clone algorithm.
In particular, the significant differences to JSON are:
transferList may be a list of ArrayBuffer, MessagePort, and
FileHandle objects.
After transferring, they are not usable on the sending side of the channel
anymore (even if they are not contained in value). Unlike with
child processes, transferring handles such as network sockets is currently
not supported.
If value contains SharedArrayBuffer instances, those are accessible
from either thread. They cannot be listed in transferList.
value may still contain ArrayBuffer instances that are not in
transferList; in that case, the underlying memory is copied rather than moved.
The message object is cloned immediately, and can be modified after
posting without having side effects.
For more information on the serialization and deserialization mechanisms
behind this API, see the serialization API of the node:v8 module.
All TypedArray and Buffer instances are views over an underlying
ArrayBuffer. That is, it is the ArrayBuffer that actually stores
the raw data while the TypedArray and Buffer objects provide a
way of viewing and manipulating the data. It is possible and common
for multiple views to be created over the same ArrayBuffer instance.
Great care must be taken when using a transfer list to transfer an
ArrayBuffer as doing so causes all TypedArray and Buffer
instances that share that same ArrayBuffer to become unusable.
For Buffer instances, specifically, whether the underlying
ArrayBuffer can be transferred or cloned depends entirely on how
instances were created, which often cannot be reliably determined.
An ArrayBuffer can be marked with markAsUntransferable() to indicate
that it should always be cloned and never transferred.
Depending on how a Buffer instance was created, it may or may
not own its underlying ArrayBuffer. An ArrayBuffer must not
be transferred unless it is known that the Buffer instance
owns it. In particular, for Buffers created from the internal
Buffer pool (using, for instance Buffer.from() or Buffer.allocUnsafe()),
transferring them is not possible and they are always cloned,
which sends a copy of the entire Buffer pool.
This behavior may come with unintended higher memory
usage and possible security concerns.
See Buffer.allocUnsafe() for more details on Buffer pooling.
The ArrayBuffers for Buffer instances created using
Buffer.alloc() or Buffer.allocUnsafeSlow() can always be
transferred but doing so renders all other existing views of
those ArrayBuffers unusable.
Because object cloning uses the HTML structured clone algorithm,
non-enumerable properties, property accessors, and object prototypes are
not preserved. In particular, Buffer objects will be read as
plain Uint8Arrays on the receiving side, and instances of JavaScript
classes will be cloned as plain JavaScript objects.
This limitation extends to many built-in objects, such as the global URL
object:


If true, the MessagePort object will keep the Node.js event loop active.
Opposite of unref(). Calling ref() on a previously unref()ed port does
not let the program exit if it's the only active handle left (the default
behavior). If the port is ref()ed, calling ref() again has no effect.
If listeners are attached or removed using .on('message'), the port
is ref()ed and unref()ed automatically depending on whether
listeners for the event exist.
Starts receiving messages on this MessagePort. When using this port
as an event emitter, this is called automatically once 'message'
listeners are attached.
This method exists for parity with the Web MessagePort API. In Node.js,
it is only useful for ignoring messages when no event listener is present.
Node.js also diverges in its handling of .onmessage. Setting it
automatically calls .start(), but unsetting it lets messages queue up
until a new handler is set or the port is discarded.
Calling unref() on a port allows the thread to exit if this is the only
active handle in the event system. If the port is already unref()ed calling
unref() again has no effect.
If listeners are attached or removed using .on('message'), the port is
ref()ed and unref()ed automatically depending on whether
listeners for the event exist.
The Worker class represents an independent JavaScript execution thread.
Most Node.js APIs are available inside of it.
Notable differences inside a Worker environment are:
Creating Worker instances inside of other Workers is possible.
Like Web Workers and the node:cluster module, two-way communication
can be achieved through inter-thread message passing. Internally, a Worker has
a built-in pair of MessagePorts that are already associated with each
other when the Worker is created. While the MessagePort object on the parent
side is not directly exposed, its functionalities are exposed through
worker.postMessage() and the worker.on('message') event
on the Worker object for the parent thread.
To create custom messaging channels (which is encouraged over using the default
global channel because it facilitates separation of concerns), users can create
a MessageChannel object on either thread and pass one of the
MessagePorts on that MessageChannel to the other thread through a
pre-existing channel, such as the global one.
See port.postMessage() for more information on how messages are passed,
and what kind of JavaScript values can be successfully transported through
the thread barrier.
Added support for a name option, which allows adding a name to worker title for debugging.
The filename parameter can be a WHATWG URL object using data: protocol.
The trackUnmanagedFds option was set to true by default.
The trackUnmanagedFds option was introduced.
The transferList option was introduced.
The filename parameter can be a WHATWG URL object using file: protocol.
The argv option was introduced.
The resourceLimits option was introduced.
Added in: v10.5.0
The 'error' event is emitted if the worker thread throws an uncaught
exception. In that case, the worker is terminated.
The 'exit' event is emitted once the worker has stopped. If the worker
exited by calling process.exit(), the exitCode parameter is the
passed exit code. If the worker was terminated, the exitCode parameter is
1.
This is the final event emitted by any Worker instance.
The 'message' event is emitted when the worker thread has invoked
require('node:worker_threads').parentPort.postMessage().
See the port.on('message') event for more details.
All messages sent from the worker thread are emitted before the
'exit' event is emitted on the Worker object.
The 'messageerror' event is emitted when deserializing a message failed.
The 'online' event is emitted when the worker thread has started executing
JavaScript code.
Support options to configure the heap snapshot.
Added in: v13.9.0, v12.17.0
Returns a readable stream for a V8 snapshot of the current state of the Worker.
See v8.getHeapSnapshot() for more details.
If the Worker thread is no longer running, which may occur before the
'exit' event is emitted, the returned Promise is rejected
immediately with an ERR_WORKER_NOT_RUNNING error.
An object that can be used to query performance information from a worker
instance. Similar to perf_hooks.performance.
The same call as perf_hooks eventLoopUtilization(), except the values
of the worker instance are returned.
One difference is that, unlike the main thread, bootstrapping within a worker
is done within the event loop. So the event loop utilization is
immediately available once the worker's script begins execution.
An idle time that does not increase does not indicate that the worker is
stuck in bootstrap. The following examples shows how the worker's entire
lifetime never accumulates any idle time, but is still be able to process
messages.
The event loop utilization of a worker is available only after the 'online'
event emitted, and if called before this, or after the 'exit'
event, then all properties have the value of 0.
Send a message to the worker that is received via
require('node:worker_threads').parentPort.on('message').
See port.postMessage() for more details.
Opposite of unref(), calling ref() on a previously unref()ed worker does
not let the program exit if it's the only active handle left (the default
behavior). If the worker is ref()ed, calling ref() again has
no effect.
Provides the set of JS engine resource constraints for this Worker thread.
If the resourceLimits option was passed to the Worker constructor,
this matches its values.
If the worker has stopped, the return value is an empty object.
This is a readable stream which contains data written to process.stderr
inside the worker thread. If stderr: true was not passed to the
Worker constructor, then data is piped to the parent thread's
process.stderr stream.
If stdin: true was passed to the Worker constructor, this is a
writable stream. The data written to this stream will be made available in
the worker thread as process.stdin.
This is a readable stream which contains data written to process.stdout
inside the worker thread. If stdout: true was not passed to the
Worker constructor, then data is piped to the parent thread's
process.stdout stream.
This function now returns a Promise. Passing a callback is deprecated, and was useless up to this version, as the Worker was actually terminated synchronously. Terminating is now a fully asynchronous operation.
Added in: v10.5.0
Stop all JavaScript execution in the worker thread as soon as possible.
Returns a Promise for the exit code that is fulfilled when the
'exit' event is emitted.
An integer identifier for the referenced thread. Inside the worker thread,
it is available as require('node:worker_threads').threadId.
This value is unique for each Worker instance inside a single process.
Calling unref() on a worker allows the thread to exit if this is the only
active handle in the event system. If the worker is already unref()ed calling
unref() again has no effect.
Workers utilize message passing via <MessagePort> to implement interactions
with stdio. This means that stdio output originating from a Worker can
get blocked by synchronous code on the receiving end that is blocking the
Node.js event loop.
Take care when launching worker threads from preload scripts (scripts loaded
and run using the -r command line flag). Unless the execArgv option is
explicitly set, new Worker threads automatically inherit the command line flags
from the running process and will preload the same preload scripts as the main
thread. If the preload script unconditionally launches a worker thread, every
thread spawned will spawn another until the application crashes.


Source Code: lib/zlib.js
The node:zlib module provides compression functionality implemented using
Gzip, Deflate/Inflate, and Brotli.
To access it:
Compression and decompression are built around the Node.js Streams API.
Compressing or decompressing a stream (such as a file) can be accomplished by
piping the source stream through a zlib Transform stream into a destination
stream:
It is also possible to compress or decompress data in a single step:
All zlib APIs, except those that are explicitly synchronous, use the Node.js
internal threadpool. This can lead to surprising effects and performance
limitations in some applications.
Creating and using a large number of zlib objects simultaneously can cause
significant memory fragmentation.
In the preceding example, 30,000 deflate instances are created concurrently.
Because of how some operating systems handle memory allocation and
deallocation, this may lead to significant memory fragmentation.
It is strongly recommended that the results of compression
operations be cached to avoid duplication of effort.
The node:zlib module can be used to implement support for the gzip, deflate
and br content-encoding mechanisms defined by
HTTP.
The HTTP Accept-Encoding header is used within an HTTP request to identify
the compression encodings accepted by the client. The Content-Encoding
header is used to identify the compression encodings actually applied to a
message.
The examples given below are drastically simplified to show the basic concept.
Using zlib encoding can be expensive, and the results ought to be cached.
See Memory usage tuning for more information on the speed/memory/compression
tradeoffs involved in zlib usage.
By default, the zlib methods will throw an error when decompressing
truncated data. However, if it is known that the data is incomplete, or
the desire is to inspect only the beginning of a compressed file, it is
possible to suppress the default error handling by changing the flushing
method that is used to decompress the last chunk of input data:
This will not change the behavior in other error-throwing situations, e.g.
when the input data has an invalid format. Using this method, it will not be
possible to determine whether the input ended prematurely or lacks the
integrity checks, making it necessary to manually check that the
decompressed result is valid.
From zlib/zconf.h, modified for Node.js usage:
The memory requirements for deflate are (in bytes):
That is: 128K for windowBits = 15 + 128K for memLevel = 8
(default values) plus a few kilobytes for small objects.
For example, to reduce the default memory requirements from 256K to 128K, the
options should be set to:
This will, however, generally degrade compression.
The memory requirements for inflate are (in bytes) 1 << windowBits.
That is, 32K for windowBits = 15 (default value) plus a few kilobytes
for small objects.
This is in addition to a single internal output slab buffer of size
chunkSize, which defaults to 16K.
The speed of zlib compression is affected most dramatically by the
level setting. A higher level will result in better compression, but
will take longer to complete. A lower level will result in less
compression, but will be much faster.
In general, greater memory usage options will mean that Node.js has to make
fewer calls to zlib because it will be able to process more data on
each write operation. So, this is another factor that affects the
speed, at the cost of memory usage.
There are equivalents to the zlib options for Brotli-based streams, although
these options have different ranges than the zlib ones:
See below for more details on Brotli-specific options.
Calling .flush() on a compression stream will make zlib return as much
output as currently possible. This may come at the cost of degraded compression
quality, but can be useful when data needs to be available as soon as possible.
In the following example, flush() is used to write a compressed partial
HTTP response to the client:
All of the constants defined in zlib.h are also defined on
require('node:zlib').constants. In the normal course of operations, it will
not be necessary to use these constants. They are documented so that their
presence is not surprising. This section is taken almost directly from the
zlib documentation.
Previously, the constants were available directly from require('node:zlib'),
for instance zlib.Z_NO_FLUSH. Accessing the constants directly from the module
is currently still possible but is deprecated.
Allowed flush values.
Return codes for the compression/decompression functions. Negative
values are errors, positive values are used for special but normal
events.
Compression levels.
Compression strategy.
There are several options and other constants available for Brotli-based
streams:
The following values are valid flush operations for Brotli-based streams:
There are several options that can be set on Brotli encoders, affecting
compression efficiency and speed. Both the keys and the values can be accessed
as properties of the zlib.constants object.
The most important options are:
The following flags can be set for advanced control over the compression
algorithm and memory usage tuning:
These advanced options are available for controlling decompression:
The maxOutputLength option is supported now.
The dictionary option can be an ArrayBuffer.
The dictionary option can be an Uint8Array now.
The finishFlush option is supported now.
Added in: v0.11.1
Each zlib-based class takes an options object. No options are required.
Some options are only relevant when compressing and are
ignored by the decompression classes.
See the deflateInit2 and inflateInit2 documentation for more
information.
The maxOutputLength option is supported now.
Added in: v11.7.0
Each Brotli-based class takes an options object. All options are optional.
For example:
Compress data using the Brotli algorithm.
Decompress data using the Brotli algorithm.
Compress data using deflate.
Compress data using deflate, and do not append a zlib header.
Trailing garbage at the end of the input stream will now result in an 'error' event.
Multiple concatenated gzip file members are supported now.
A truncated input stream will now result in an 'error' event.
Added in: v0.5.8
Decompress a gzip stream.
Compress data using gzip.
A truncated input stream will now result in an 'error' event.
Added in: v0.5.8
Decompress a deflate stream.
Custom dictionaries are now supported by InflateRaw.
A truncated input stream will now result in an 'error' event.
Added in: v0.5.8
Decompress a raw deflate stream.
Decompress either a Gzip- or Deflate-compressed stream by auto-detecting
the header.
This class was renamed from Zlib to ZlibBase.
Added in: v0.5.8
Not exported by the node:zlib module. It is documented here because it is the
base class of the compressor/decompressor classes.
This class inherits from stream.Transform, allowing node:zlib objects to
be used in pipes and similar stream operations.


Deprecated alias for zlib.bytesWritten. This original name was chosen
because it also made sense to interpret the value as the number of bytes
read by the engine, but is inconsistent with other streams in Node.js that
expose values under these names.
The zlib.bytesWritten property specifies the number of bytes written to
the engine, before the bytes are processed (compressed or decompressed,
as appropriate for the derived class).
Close the underlying handle.
Flush pending data. Don't call this frivolously, premature flushes negatively
impact the effectiveness of the compression algorithm.
Calling this only flushes data from the internal zlib state, and does not
perform flushing of any kind on the streams level. Rather, it behaves like a
normal call to .write(), i.e. it will be queued up behind other pending
writes and will only produce output when data is being read from the stream.
This function is only available for zlib-based streams, i.e. not Brotli.
Dynamically update the compression level and compression strategy.
Only applicable to deflate algorithm.
Reset the compressor/decompressor to factory defaults. Only applicable to
the inflate and deflate algorithms.
Provides an object enumerating Zlib-related constants.
Creates and returns a new BrotliCompress object.
Creates and returns a new BrotliDecompress object.
Creates and returns a new Deflate object.
Creates and returns a new DeflateRaw object.
An upgrade of zlib from 1.2.8 to 1.2.11 changed behavior when windowBits
is set to 8 for raw deflate streams. zlib would automatically set windowBits
to 9 if was initially set to 8. Newer versions of zlib will throw an exception,
so Node.js restored the original behavior of upgrading a value of 8 to 9,
since passing windowBits = 9 to zlib actually results in a compressed stream
that effectively uses an 8-bit window only.
Creates and returns a new Gunzip object.
Creates and returns a new Gzip object.
See example.
Creates and returns a new Inflate object.
Creates and returns a new InflateRaw object.
Creates and returns a new Unzip object.
All of these take a Buffer, TypedArray, DataView,
ArrayBuffer or string as the first argument, an optional second argument
to supply options to the zlib classes and will call the supplied callback
with callback(error, result).
Every method has a *Sync counterpart, which accept the same arguments, but
without a callback.
Compress a chunk of data with BrotliCompress.
Decompress a chunk of data with BrotliDecompress.
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.6.0
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.11.12
Compress a chunk of data with Deflate.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.6.0
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.11.12
Compress a chunk of data with DeflateRaw.
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.6.0
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.11.12
Decompress a chunk of data with Gunzip.
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.6.0
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.11.12
Compress a chunk of data with Gzip.
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.6.0
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.11.12
Decompress a chunk of data with Inflate.
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.6.0
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.11.12
Decompress a chunk of data with InflateRaw.
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.6.0
The buffer parameter can be an ArrayBuffer.
The buffer parameter can be any TypedArray or DataView.
The buffer parameter can be an Uint8Array now.
Added in: v0.11.12
Decompress a chunk of data with Unzip.
<ipython-input-2-2da959541922>:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.
  soup = BeautifulSoup(response.content, 'html.parser')

 
Sign in with a passkey




            New to GitHub?
              Create an account

It does not necessarily need to match your github repository name.
So, node-foo and bar-js are bad names. foo or bar are better.
version: A semver-compatible version.
engines: Specify the versions of node (or whatever else) that your program runs on. The node API changes a lot, and there may be bugs or new functionality that you depend on. Be explicit.
author: Take some credit.
scripts: If you have a special compilation or installation script, then you should put it in the scripts object. You should definitely have at least a basic smoke-test command as the "scripts.test" field. See scripts.
main: If you have a single module that serves as the entry point to your program (like what the "foo" package gives you at require("foo")), then you need to specify that in the "main" field.
directories: This is an object mapping names to folders. The best ones to include are "lib" and "doc", but if you use "man" to specify a folder full of man pages, they'll get installed just like these ones.
You can use npm init in the root of your package in order to get you started with a pretty basic package.json file. See npm init for more info.
Use a .npmignore file to keep stuff out of your package. If there's no .npmignore file, but there is a .gitignore file, then npm will ignore the stuff matched by the .gitignore file. If you want to include something that is excluded by your .gitignore file, you can create an empty .npmignore file to override it. Like git, npm looks for .npmignore and .gitignore files in all subdirectories of your package, not only the root directory.
.npmignore files follow the same pattern rules as .gitignore files:
By default, the following paths and files are ignored, so there's no need to add them to .npmignore explicitly:
Additionally, everything in node_modules is ignored, except for bundled dependencies. npm automatically handles this for you, so don't bother adding node_modules to .npmignore.
The following paths and files are never ignored, so adding them to .npmignore is pointless:
If, given the structure of your project, you find .npmignore to be a maintenance headache, you might instead try populating the files property of package.json, which is an array of file or directory names that should be included in your package. Sometimes manually picking which items to allow is easier to manage than building a block list.
If you want to double check that your package will include only the files you intend it to when published, you can run the npm pack command locally which will generate a tarball in the working directory, the same way it does for publishing.
npm link is designed to install a development package and see the changes in real time without having to keep re-installing it. (You do need to either re-link or npm rebuild -g to update compiled packages, of course.)
More info at npm link.
This is important.
If you can not install it locally, you'll have problems trying to publish it. Or, worse yet, you'll be able to publish it, but you'll be publishing a broken or pointless package. So don't do that.
In the root of your package, do this:
That'll show you that it's working. If you'd rather just create a symlink package that points to your working directory, then do this:
Use npm ls -g to see if it's there.
To test a local install, go into some other folder, and then do:
to install it locally into the node_modules folder in that other place.
Then go into the node-repl, and try using require("my-thing") to bring in your module's main module.
Create a user with the adduser command. It works like this:
and then follow the prompts.
This is documented better in npm adduser.
This part's easy. In the root of your folder, do this:
You can give publish a url to a tarball, or a filename of a tarball, or a path to a folder.
Note that pretty much everything in that folder will be exposed by default. So, if you have secret stuff in there, use a .npmignore file to list out the globs to ignore, or publish from a fresh checkout.
Send emails, write blogs, blab in IRC.
Tell the world how easy it is to install your program!
Select CLI Version:
So sad to see you go.
Or, if that fails, get the npm source code, and do:
Usually, the above instructions are sufficient. That will remove npm, but leave behind anything you've installed.
If that doesn't work, or if you require more drastic measures, continue reading.
Note that this is only necessary for globally-installed packages. Local installs are completely contained within a project's node_modules folder. Delete that folder, and everything is gone unless a package's install script is particularly ill-behaved.
This assumes that you installed node and npm in the default place. If you configured node with a different --prefix, or installed npm with a different prefix setting, then adjust the paths accordingly, replacing /usr/local with your install prefix.
To remove everything npm-related manually:
If you installed things with npm, then your best bet is to uninstall them with npm first, and then install them again once you have a proper install. This can help find any symlinks that are lying around:
Prior to version 0.3, npm used shim files for executables and node modules. To track those down, you can do the following:
Select CLI Version:
We read every piece of feedback, and take your input very seriously.

            To see all available qualifiers, see our documentation.
          

        Documentation for the npm registry, website, and command-line interface.
      

        Documentation for the npm registry, website, and command-line interface.
      
These are the legal policies of npm, Inc.
These are updated from time to time. Their sources are stored in a git repository at https://github.com/npm/documentation/tree/main/content/policies.

